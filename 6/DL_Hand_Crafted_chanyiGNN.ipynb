{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pau5RAGu5JFd",
        "outputId": "e02aa444-b9b6-4fcf-aa95-24a5c2700e1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#Âú®ColabÁî®\n",
        "!pip install -Uq timm==0.9.12\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbxBw8HI5RzE",
        "outputId": "a7706a47-74ab-426c-e10e-02638892a633"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#ÊéàÊ¨ä\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9EKp_bKGhoXg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import torch.nn.functional as F\n",
        "import timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "58oc3dAUQQAB"
      },
      "outputs": [],
      "source": [
        "num_epochs = 50\n",
        "batch_size = 16\n",
        "early_stop_patience = 4\n",
        "patience_counter = 0\n",
        "loss_alpha=1.0\n",
        "loss_beta=0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bS_YldbqhoXg",
        "outputId": "80ea742f-7ad3-436f-cda8-6078fb08021f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "È°ûÂà•Êï∏ÈáèÔºö 22\n"
          ]
        }
      ],
      "source": [
        "base_dir = \"/content/drive/MyDrive/DLA_term_project_data/classified_data\"\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "imagenet_stats = [(0.485, 0.456, 0.406), (0.229, 0.224, 0.225)]\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(imagenet_stats[0], imagenet_stats[1])\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "validation_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(imagenet_stats[0], imagenet_stats[1])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(imagenet_stats[0], imagenet_stats[1])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(os.path.join(base_dir, 'train'), transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(os.path.join(base_dir, 'validation'), transform=validation_transform)\n",
        "test_data = datasets.ImageFolder(os.path.join(base_dir, \"test\"), transform=test_transform)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"È°ûÂà•Êï∏ÈáèÔºö\", len(train_dataset.classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "a1xpCePFPJef"
      },
      "outputs": [],
      "source": [
        "# customed loss function\n",
        "# ----- 1. Node-to-Node Distance Matrix -----\n",
        "node_distance_matrix = torch.tensor([\n",
        "    [0,1,2,2,1,2,2,2.5,3.5,4,5.5,5,4,2.5,3.5,4,5,3.5,4.5,3.5,2.5,2.5],\n",
        "    [1,0,1,3,2,3,1,1.5,2.5,3,4.5,4,3,1.5,4.5,5,6,4.5,5.5,4.5,3.5,3.5],\n",
        "    [2,1,0,2,3,4,2,2.5,3.5,4,5.5,3,2,0.5,3.5,4,5,3.5,4.5,5.5,4.5,2.5],\n",
        "    [2,3,2,0,1,2,4,4.5,5.5,6,7.5,5,4,2.5,1.5,2,3,1.5,2.5,3.5,2.5,0.5],\n",
        "    [1,2,3,1,0,1,3,3.5,4.5,5,6.5,6,5,3.5,2.5,3,4,2.5,3.5,2.5,1.5,1.5],\n",
        "    [2,3,4,2,1,0,2,2.5,3.5,4,5.5,7,6,4.5,3.5,4,5,3.5,2.5,1.5,0.5,2.5],\n",
        "    [2,1,2,4,3,2,0,0.5,1.5,2,3.5,5,4,2.5,5.5,6,7,5.5,4.5,3.5,2.5,4.5],\n",
        "    [2.5,1.5,2.5,4.5,3.5,2.5,0.5,0,1,1.5,3,3.5,4.5,3,6,6.5,7.5,6,5,4,3,5],\n",
        "    [3.5,2.5,3.5,5.5,4.5,3.5,1.5,1,0,0.5,2,2.5,3.5,4,7,7.5,8.5,7,6,5,4,6],\n",
        "    [4,3,4,6,5,4,2,1.5,0.5,0,1.5,2,3,4.5,7.5,8,9,7.5,6.5,5.5,4.5,6.5],\n",
        "    [5.5,4.5,5.5,7.5,6.5,5.5,3.5,3,2,1.5,0,0.5,1.5,3,7,7.5,8.5,7,8,7,6,6.5],\n",
        "    [5,4,3,5,6,7,5,3.5,2.5,2,0.5,0,1,2.5,6.5,7,8,6.5,7.5,7.5,6.5,5.5],\n",
        "    [4,3,2,4,5,6,4,4.5,3.5,3,1.5,1,0,1.5,5.5,6,7,5.5,6.5,7.5,7,4.5],\n",
        "    [2.5,1.5,0.5,2.5,3.5,4.5,2.5,3,4,4.5,3,2.5,1.5,0,4,4.5,5.5,4,5,6,5.5,3],\n",
        "    [3.5,4.5,3.5,1.5,2.5,3.5,5.5,6,7,7.5,7,6.5,5.5,4,0,0.5,1.5,0.5,1.5,2.5,3.5,1],\n",
        "    [4,5,4,2,3,4,6,6.5,7.5,8,7.5,7,6,4.5,0.5,0,1,1,2,3,4,1.5],\n",
        "    [5,6,5,3,4,5,7,7.5,8.5,9,8.5,8,7,5.5,1.5,1,0,1.5,2.5,3.5,4.5,2.5],\n",
        "    [3.5,4.5,3.5,1.5,2.5,3.5,5.5,6,7,7.5,7,6.5,5.5,4,0.5,1,1.5,0,1,2,3,1],\n",
        "    [4.5,5.5,4.5,2.5,3.5,2.5,4.5,5,6,6.5,8,7.5,6.5,5,1.5,2,2.5,1,0,1,2,2],\n",
        "    [3.5,4.5,5.5,3.5,2.5,1.5,3.5,4,5,5.5,7,7.5,7.5,6,2.5,3,3.5,2,1,0,1,3],\n",
        "    [2.5,3.5,4.5,2.5,1.5,0.5,2.5,3,4,4.5,6,6.5,7,5.5,3.5,4,4.5,3,2,1,0,3],\n",
        "    [2.5,3.5,2.5,0.5,1.5,2.5,4.5,5,6,6.5,6.5,5.5,4.5,3,1,1.5,2.5,1,2,3,3,0]\n",
        "], dtype=torch.float)\n",
        "\n",
        "# ----- 2. Area-to-Area Distance Matrix -----\n",
        "area_distance_matrix = torch.tensor([\n",
        "    [0, 1, 2, 2, 2, 2],\n",
        "    [1, 0, 1, 1, 1, 1],\n",
        "    [2, 1, 0, 1, 3, 3],\n",
        "    [2, 1, 1, 0, 3, 3],\n",
        "    [2, 1, 3, 3, 0, 1],\n",
        "    [2, 1, 3, 3, 1, 0]\n",
        "], dtype=torch.float)\n",
        "\n",
        "# ----- 3. Node-to-Area Mapping -----\n",
        "node_to_area = torch.tensor([\n",
        "    0, 0, 0, 0, 0, 0, 0,    # 1~7 ‚Üí area 0\n",
        "    1,                      # 8  ‚Üí area 1\n",
        "    2, 2,                   # 9~10 ‚Üí area 2\n",
        "    3, 3, 3,                # 11~13 ‚Üí area 3\n",
        "    1,                      # 14 ‚Üí area 1\n",
        "    4, 4, 4,                # 15~17 ‚Üí area 4\n",
        "    5, 5, 5,                # 18~20 ‚Üí area 5\n",
        "    1,                      # 21 ‚Üí area 1\n",
        "    1                       # 22 ‚Üí area 1\n",
        "], dtype=torch.long)\n",
        "\n",
        "# ----- 4. Custom Loss Function -----\n",
        "class DistancePenaltyLoss(nn.Module):\n",
        "    def __init__(self, node_distance_matrix, area_distance_matrix, node_to_area, alpha=1.0, beta=1.0):\n",
        "        super().__init__()\n",
        "        self.node_distance_matrix = node_distance_matrix\n",
        "        self.area_distance_matrix = area_distance_matrix\n",
        "        self.node_to_area = node_to_area\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.ce = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        \"\"\"\n",
        "        logits: (batch_size, 22)\n",
        "        targets: (batch_size,) - ground truth class indices\n",
        "        \"\"\"\n",
        "        ce_loss = self.ce(logits, targets)\n",
        "\n",
        "        probs = F.softmax(logits, dim=1)  # (B, 22)\n",
        "        batch_size = targets.size(0)\n",
        "\n",
        "        node_penalty = 0.0\n",
        "        area_penalty = 0.0\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            target = targets[i]\n",
        "            prob = probs[i]  # shape (22,)\n",
        "\n",
        "            # Node distance penalty\n",
        "            node_distances = self.node_distance_matrix[target]  # (22,)\n",
        "            node_penalty += (prob * node_distances).sum()\n",
        "\n",
        "            # Area distance penalty\n",
        "            target_area = self.node_to_area[target]\n",
        "            pred_areas = self.node_to_area  # shape (22,)\n",
        "            area_dists = self.area_distance_matrix[target_area][pred_areas]  # shape (22,)\n",
        "            area_penalty += (prob * area_dists).sum()\n",
        "\n",
        "        node_penalty /= batch_size\n",
        "        area_penalty /= batch_size\n",
        "\n",
        "        total_loss = ce_loss + self.alpha * node_penalty + self.beta * area_penalty\n",
        "        return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "k5mafh1TQ0pA"
      },
      "outputs": [],
      "source": [
        "# timm.list_models('convnext*')\n",
        "# timm.list_models('convnext*', pretrained=True)\n",
        "# timm.list_models('efficient*', pretrained=True)\n",
        "# timm.list_models('res*', pretrained=True)\n",
        "# timm.list_models('swin*', pretrained=True)\n",
        "\n",
        "# convnextv2_tiny.fcmae\n",
        "# efficientvit_b0.r224_in1k\n",
        "# efficientvit_b1.r224_in1k\n",
        "# efficientvit_b2.r224_in1k\n",
        "# efficientvit_b3.r224_in1k\n",
        "# efficientvit_m0.r224_in1k\n",
        "# efficientvit_m1.r224_in1k\n",
        "# efficientvit_m2.r224_in1k\n",
        "# efficientvit_m3.r224_in1k\n",
        "# efficientvit_m4.r224_in1k\n",
        "# efficientvit_m5.r224_in1k\n",
        "# efficientformerv2_s0.snap_dist_in1k\n",
        "# efficientformerv2_s1.snap_dist_in1k\n",
        "# efficientformerv2_s2.snap_dist_in1k\n",
        "# resnet50.a1_in1k\n",
        "# resnet50d.ra2_in1k\n",
        "# resnet50.fb_swsl_ig1b_ft_in1k\n",
        "# resnet50.tv_in1k\n",
        "# resnet50.ra_in1k\n",
        "# res2net50d.in1k\n",
        "# res2net50_14w_8s.in1k\n",
        "# mobilevitv2_100.cvnets_in1k\n",
        "# mobilevitv2_150.cvnets_in22k_ft_in1k\n",
        "# mobilenetv3_large_100.ra_in1k\n",
        "# mobileone_s1.apple_in1k\n",
        "# mobilenetv2_100.ra_in1k\n",
        "# swin_tiny_patch4_window7_224.ms_in1k\n",
        "# swinv2_tiny_window8_256.ms_in1k\n",
        "# swinv2_cr_tiny_ns_224.sw_in1k\n",
        "\n",
        "\n",
        "\n",
        "# ‚úÖ Overall Best Choices (Balanced: Accuracy, Speed, Size)\n",
        "# mobilevitv2_100.cvnets_in1k\t       Transformer-CNN hybrid, small but very accurate\n",
        "# efficientnet_b0\t             Lightweight, easy to train, great accuracy/size tradeoff\n",
        "# mobilenetv3_large_100.ra_in1k       Very fast, widely supported, good baseline for small datasets\n",
        "# res2net50d.in1k\t             Strong feature extractor, better than plain ResNet50\n",
        "# resnet50d.ra2_in1k             Modern ResNet variant with strong augmentations\n",
        "# swinv2_tiny_window8_256.ms_in1k\t     Lightweight transformer, good if you want to try Swin family\n",
        "\n",
        "# üß† If You Want Transfer Learning Strength\n",
        "# mobilevitv2_150.cvnets_in22k_ft_in1k\t  Pretrained on ImageNet-22K ‚Üí fine-tuned on IN1K\n",
        "# resnet50.fb_swsl_ig1b_ft_in1k\t       Trained on IG-1B dataset ‚Äî great features even for few-shot\n",
        "\n",
        "# üöÄ If You Want Ultra-Fast Training\n",
        "# mobileone_s1.apple_in1k          \tTiny, deployable, optimized for fast inference\n",
        "# mobilenetv2_100.ra_in1k         \tClassic, efficient, and still performs well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "w4G_OmJF-J1M"
      },
      "outputs": [],
      "source": [
        "# ---------- 100% ÊâãÂàª Hybrid CNN + GNNÔºöPatch-GNNNet ----------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PatchGNNNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Â∞áËº∏ÂÖ•ÂΩ±ÂÉèÂÖàÈÄèÈÅéÂ∞èÂûã CNN Stem ÊèêÂèñÂá∫‰∏ÄÂºµÁâπÂæµÂúñ (C, H, W)Ôºå\n",
        "    ÁÑ∂ÂæåÊääÁâπÂæµÂúñÂàáÊàê patch_size√ópatch_size ÁöÑÂ∞èÊ†ºÔºåÊØèÊ†ºÁï∂‰ΩúÂúñ‰∏äÁöÑ‰∏ÄÂÄã NodeÔºõ\n",
        "    Êé•ËëóÂÅö‰∏ÄÊ¨°Á¥îÊâãÂàª GraphConvÔºà‰ª• patch ÈñìÁöÑ 8 ÈÑ∞ÂüüÁÇ∫ÈÑ∞Êé•ÔºâÔºå\n",
        "    ÊúÄÂæåÂÅöÂÖ®ÂüüÂπ≥Âùá + MLP Head ‰æÜÈ†êÊ∏¨ 22 È°ûÁØÄÈªû„ÄÇ\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=22, in_ch=3, stem_channels=64, patch_size=4, hidden_dim=256, gc_hidden=256):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_classes: ÂàÜÈ°ûÁØÄÈªûÊï∏ (22)\n",
        "            in_ch: Ëº∏ÂÖ•ÂΩ±ÂÉèÈÄöÈÅì (RGB‚Üí3)\n",
        "            stem_channels: CNN Stem Á¨¨‰∏ÄÂ±§Ëº∏Âá∫ÈÄöÈÅìÊï∏\n",
        "            patch_size:  ÊääÁâπÂæµÂúñÂàáÊàê patch_size√ópatch_size ÁöÑÂ°ä\n",
        "            hidden_dim: CNN Stem Êî∂Â∞æÈÄöÈÅìÊï∏ (ÊúÄÂæåÊúÉËº∏Âá∫ (hidden_dim, H, W))\n",
        "            gc_hidden: GNN Èö±ËóèÂ±§Á∂≠Â∫¶\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # 1) CNN StemÔºö4 Â±§Â∞èÂûã Conv ‚Üí BatchNorm ‚Üí Mish\n",
        "        #    Âæû (3√ó224√ó224) ‰∏ãÊé°Ê®£Âà∞ (hidden_dim, 56, 56) ‰ª•‰∏ãÔºö\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, stem_channels, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(stem_channels),\n",
        "            nn.Mish(),\n",
        "            nn.MaxPool2d(3, 2, 1),  # -> (stem_channels, 56, 56)\n",
        "\n",
        "            nn.Conv2d(stem_channels, stem_channels*2, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(stem_channels*2),\n",
        "            nn.Mish(),              # -> (stem_channels*2, 28, 28)\n",
        "\n",
        "            nn.Conv2d(stem_channels*2, stem_channels*4, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(stem_channels*4),\n",
        "            nn.Mish(),              # -> (stem_channels*4, 14, 14)\n",
        "\n",
        "            nn.Conv2d(stem_channels*4, hidden_dim, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(hidden_dim),\n",
        "            nn.Mish()               # -> (hidden_dim, 14, 14)\n",
        "        )\n",
        "        # ‰πãÊâÄ‰ª•ËÆì Stem ÊúÄÁµÇÁ©∫ÈñìÂ§ßÂ∞è‰øùÊåÅ (14√ó14)ÔºåÊòØÂõ†ÁÇ∫ patch_size=4 ÊôÇ (14 % 4 == 2)\n",
        "        # ‰ΩÜÈÄôË£°ÊàëÂÄëË¶Å (hidden_dim, 56, 56) / 4 -> (hidden_dim, 14, 14)\n",
        "\n",
        "        # 2) GraphConv Â±§ÔºöÂñÆÂ±§ GCN-like Êõ¥Êñ∞\n",
        "        #    Â∞çÊØèÂÄã patch nodeÔºåÊääËá™Â∑±ËàáÈÑ∞Â±Ö (8 ÈÑ∞Âüü) feature ÂÅö‰∏ÄÊ¨°ËûçÂêà\n",
        "        self.gc_in = nn.Linear(hidden_dim, gc_hidden, bias=False)   # W_self\n",
        "        self.gc_nei = nn.Linear(hidden_dim, gc_hidden, bias=False)  # W_neigh\n",
        "        self.gc_act = nn.Mish()\n",
        "\n",
        "        # 3) MLP HeadÔºöÊääÂúñÂç∑Á©çÂæåÁöÑ patch features ÂÖ®ÂüüÂπ≥ÂùáÔºåÊé•‰∏ÄÂÄãÈö±ËóèÂ±§ + ÂàÜÈ°ûÂ±§\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.Linear(gc_hidden, gc_hidden),\n",
        "            nn.Mish(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(gc_hidden, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (B, 3, 224, 224)  ‚Üí Stem ‚Üí (B, hidden_dim, 14, 14)\n",
        "        ‚Üí Âàá Patch ‚Üí (B, N, hidden_dim)ÔºåÂÖ∂‰∏≠ N = (14/patch_size) * (14/patch_size)\n",
        "          (Ëã• patch_size=2ÔºåÂâá N=49ÔºõËã• patch_size=4ÔºåÂâá N=12.25(ÈùûÊï¥Êï∏)\n",
        "        ‚Üí GraphConv ‚Üí (B, N, gc_hidden)\n",
        "        ‚Üí ÂÖ®Âüü average ‚Üí (B, gc_hidden) ‚Üí MLP Head ‚Üí (B, num_classes)\n",
        "        \"\"\"\n",
        "        B = x.size(0)\n",
        "\n",
        "        # --- 1. CNN Stem ---\n",
        "        f = self.stem(x)  # (B, hidden_dim, 14, 14)\n",
        "\n",
        "        # --- 2. Âàá patch: Áï∂‰ΩúÂúñ‰∏äÁöÑ Node ---\n",
        "        C, H, W = f.shape[1], f.shape[2], f.shape[3]\n",
        "        p = self.patch_size\n",
        "        assert H % p == 0 and W % p == 0, \"patch_size ÂøÖÈ†àÊï¥Èô§ÁâπÂæµÂúñÂ§ßÂ∞è\"\n",
        "\n",
        "        # Êää f reshape Êàê [B, N, C, p, p]ÔºåÂÜçÂÅöÂπ≥Âùá: (p,p) ‚Üí 1\n",
        "        f = f.unfold(2, p, p).unfold(3, p, p)\n",
        "        # f: (B, C, H/p, p, W/p, p) ‚Üí transpose / reshape\n",
        "        f = f.contiguous().view(B, C, H//p, p, W//p, p)\n",
        "        # ÂÖàÊääÊúÄÂæåÂÖ©ÂÄã p√óp Á∂≠Â∫¶Â£ìÊéâÔºöpatch_avg = avg_pool  ‚Üí (B, C, H/p, W/p)\n",
        "        patch_avg = f.mean(dim=3).mean(dim=4)  # (B, C, H/p, W/p)\n",
        "\n",
        "        # ÂÜçÂ±ïÂπ≥Êàê (B, N, C)ÔºåÂÖ∂‰∏≠ N = (H/p)*(W/p)\n",
        "        N = (H // p) * (W // p)\n",
        "        patch_feat = patch_avg.view(B, C, N).permute(0, 2, 1)  # (B, N, C)\n",
        "\n",
        "        # --- 3. Á¥îÊâãÂàª GraphConv  ---\n",
        "        #   ÊßãÂª∫ patch ÈñìÁöÑ 8-neighbor adjacency index\n",
        "        #   ÂÖàÊää (B, N, C) ËΩâÊàêÊ†ºÈªû (B, C, H/p, W/p)ÔºåÂÜçÂ∞çÊØèÂÄã node ËíêÈõÜ 8 ÈÑ∞Âüü feature\n",
        "        h = H // p\n",
        "        w = W // p  # h = w = 14/p (p=2 ‚Üí h=w=7)\n",
        "\n",
        "        # reshape Âõû 2D Grid\n",
        "        grid = patch_feat.view(B, h, w, C)  # (B, h, w, C)\n",
        "\n",
        "        # ÂÖà pad ‰∏ÄÂúà (value=0)ÔºåÊñπ‰æøÂèñÈÑ∞Â±Ö\n",
        "        pad_grid = F.pad(grid, (0, 0, 1, 1, 1, 1))  # pad top/bottom/left/right ‚Üí (B, h+2, w+2, C)\n",
        "\n",
        "        # ËíêÈõÜ 8 ÈÑ∞ÂüüÔºöÊàëÂÄëÈÄêÊñπÂêëÂèñ slice\n",
        "        nei_sum = torch.zeros(B, h, w, C, device=x.device)\n",
        "        for dy in [-1, 0, 1]:\n",
        "            for dx in [-1, 0, 1]:\n",
        "                if dy == 0 and dx == 0:\n",
        "                    continue\n",
        "                nei = pad_grid[:, 1+dy : 1+dy+h, 1+dx : 1+dx+w, :]  # (B, h, w, C)\n",
        "                nei_sum += nei\n",
        "        # Âπ≥ÂùáÂåñÔºàÊØèÂÄã node ÊúÄÂ§öÊúâ 8 ÂÄãÈÑ∞Â±ÖÔºõÈÇäÁïåÂ∞ë‰∏ÄÈªûÔºåÈÄôË£°Áõ¥Êé•Èô§ 8 ‰øùÊåÅÁ∞°ÂñÆÔºâ\n",
        "        nei_mean = nei_sum / 8.0  # (B, h, w, C)\n",
        "\n",
        "        # ÊääËá™Â∑± & ÈÑ∞Â±Ö feature ÈÉΩÂÅö‰∏ÄÊ¨°Á∑öÊÄßÂ±§Ôºö\n",
        "        self_feat = self.gc_in(patch_feat)   # (B, N, gc_hidden)\n",
        "        nei_feat  = self.gc_nei(nei_mean.view(B, N, C))  # (B, N, gc_hidden)\n",
        "        gconv = self.gc_act(self_feat + nei_feat)  # (B, N, gc_hidden)\n",
        "\n",
        "        # --- 4. ÂÖ®ÁêÉÂπ≥Âùá + MLP Head ---\n",
        "        # ÂÖà (B, N, gc_hidden) ‚Üí (B, gc_hidden) by Âπ≥Âùá over N\n",
        "        graph_feat = gconv.mean(dim=1)  # (B, gc_hidden)\n",
        "\n",
        "        logits = self.mlp_head(graph_feat)  # (B, num_classes)\n",
        "        return logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoGafpvlhoXh",
        "outputId": "c820faf0-73a0-4a9a-f12b-fe74b795db50"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-1734f8dfd10f>:24: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# Step 5: ÂàùÂßãÂåñÊ®°ÂûãËàáÂÑ™ÂåñÂô®\n",
        "\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "\n",
        "model = PatchGNNNet(\n",
        "    num_classes=len(train_dataset.classes),  # 22\n",
        "    in_ch=3,                       # Ëº∏ÂÖ•‰∏âÈÄöÈÅì RGB\n",
        "    stem_channels=64,              # CNN Stem Á¨¨‰∏ÄÂ±§Ëº∏Âá∫ 64\n",
        "    patch_size=2,                  # ÊääÁâπÂæµÂúñ 14√ó14 ÂàáÊàê 2√ó2 ‚Üí 7√ó7=49 nodes\n",
        "    hidden_dim=128,                # CNN Stem ÊúÄÂæåËº∏Âá∫ÈÄöÈÅì 128 ‚Üí patch feature dim\n",
        "    gc_hidden=256                  # GNN Èö±ËóèÂ±§Á∂≠Â∫¶ 256\n",
        ").to(device)\n",
        "\n",
        "\n",
        "criterion = DistancePenaltyLoss(\n",
        "    node_distance_matrix.to(device),\n",
        "    area_distance_matrix.to(device),\n",
        "    node_to_area.to(device),\n",
        "    alpha=loss_alpha,\n",
        "    beta=loss_beta\n",
        ")\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
        "scaler = GradScaler()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EQtCaXoWhoXi"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch_amp(model, loader):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    for batch_idx, (images, labels) in enumerate(loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.autocast(device.type if device.type != \"mps\" else \"cpu\", enabled=(device.type != \"cpu\")):\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        # Print batch details\n",
        "        print(f\"Batch {batch_idx+1}/{len(loader)} | Loss: {loss.item():.4f} | Batch Acc: {(predicted == labels).float().mean().item():.4f}\")\n",
        "    acc = correct / total\n",
        "    print(f\"Epoch Train Loss: {total_loss:.4f}, Accuracy: {acc:.4f}\")\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    acc = correct / total\n",
        "    print(f\"Validation Accuracy: {acc:.4f}\")\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhmyI6EhhoXj",
        "outputId": "051426c4-cb66-414e-ea29-c54dbf8a8f0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/50\n",
            "Batch 1/72 | Loss: 7.1431 | Batch Acc: 0.0000\n",
            "Batch 2/72 | Loss: 7.3595 | Batch Acc: 0.0000\n",
            "Batch 3/72 | Loss: 7.5631 | Batch Acc: 0.0000\n",
            "Batch 4/72 | Loss: 7.3832 | Batch Acc: 0.0625\n",
            "Batch 5/72 | Loss: 7.5407 | Batch Acc: 0.0000\n",
            "Batch 6/72 | Loss: 7.5120 | Batch Acc: 0.0000\n",
            "Batch 7/72 | Loss: 7.5102 | Batch Acc: 0.0625\n",
            "Batch 8/72 | Loss: 7.2032 | Batch Acc: 0.1250\n",
            "Batch 9/72 | Loss: 7.5139 | Batch Acc: 0.0000\n",
            "Batch 10/72 | Loss: 7.4157 | Batch Acc: 0.0625\n",
            "Batch 11/72 | Loss: 7.4605 | Batch Acc: 0.0625\n",
            "Batch 12/72 | Loss: 7.2414 | Batch Acc: 0.0625\n",
            "Batch 13/72 | Loss: 7.5600 | Batch Acc: 0.0000\n",
            "Batch 14/72 | Loss: 7.0495 | Batch Acc: 0.1875\n",
            "Batch 15/72 | Loss: 7.7380 | Batch Acc: 0.0000\n",
            "Batch 16/72 | Loss: 7.6669 | Batch Acc: 0.0000\n",
            "Batch 17/72 | Loss: 7.5044 | Batch Acc: 0.0000\n",
            "Batch 18/72 | Loss: 7.3517 | Batch Acc: 0.0625\n",
            "Batch 19/72 | Loss: 7.1604 | Batch Acc: 0.2500\n",
            "Batch 20/72 | Loss: 7.4905 | Batch Acc: 0.0000\n",
            "Batch 21/72 | Loss: 7.3923 | Batch Acc: 0.1250\n",
            "Batch 22/72 | Loss: 7.6148 | Batch Acc: 0.0625\n",
            "Batch 23/72 | Loss: 7.3136 | Batch Acc: 0.0625\n",
            "Batch 24/72 | Loss: 7.5118 | Batch Acc: 0.1250\n",
            "Batch 25/72 | Loss: 7.3880 | Batch Acc: 0.2500\n",
            "Batch 26/72 | Loss: 7.4595 | Batch Acc: 0.1250\n",
            "Batch 27/72 | Loss: 7.0637 | Batch Acc: 0.0625\n",
            "Batch 28/72 | Loss: 7.5088 | Batch Acc: 0.1250\n",
            "Batch 29/72 | Loss: 7.8152 | Batch Acc: 0.0625\n",
            "Batch 30/72 | Loss: 7.3274 | Batch Acc: 0.0625\n",
            "Batch 31/72 | Loss: 7.3388 | Batch Acc: 0.0000\n",
            "Batch 32/72 | Loss: 7.1121 | Batch Acc: 0.1250\n",
            "Batch 33/72 | Loss: 6.8876 | Batch Acc: 0.1250\n",
            "Batch 34/72 | Loss: 7.2351 | Batch Acc: 0.3125\n",
            "Batch 35/72 | Loss: 7.1923 | Batch Acc: 0.0625\n",
            "Batch 36/72 | Loss: 7.4052 | Batch Acc: 0.0625\n",
            "Batch 37/72 | Loss: 7.4098 | Batch Acc: 0.1875\n",
            "Batch 38/72 | Loss: 6.8255 | Batch Acc: 0.3125\n",
            "Batch 39/72 | Loss: 7.3006 | Batch Acc: 0.0000\n",
            "Batch 40/72 | Loss: 7.4383 | Batch Acc: 0.1250\n",
            "Batch 41/72 | Loss: 7.2895 | Batch Acc: 0.0625\n",
            "Batch 42/72 | Loss: 7.4487 | Batch Acc: 0.0625\n",
            "Batch 43/72 | Loss: 7.3212 | Batch Acc: 0.0625\n",
            "Batch 44/72 | Loss: 7.0873 | Batch Acc: 0.1250\n",
            "Batch 45/72 | Loss: 7.4479 | Batch Acc: 0.0625\n",
            "Batch 46/72 | Loss: 7.2928 | Batch Acc: 0.1250\n",
            "Batch 47/72 | Loss: 7.6937 | Batch Acc: 0.0000\n",
            "Batch 48/72 | Loss: 6.9745 | Batch Acc: 0.0625\n",
            "Batch 49/72 | Loss: 7.3864 | Batch Acc: 0.0000\n",
            "Batch 50/72 | Loss: 7.2528 | Batch Acc: 0.0625\n",
            "Batch 51/72 | Loss: 7.1961 | Batch Acc: 0.1250\n",
            "Batch 52/72 | Loss: 7.4488 | Batch Acc: 0.0000\n",
            "Batch 53/72 | Loss: 7.3045 | Batch Acc: 0.0625\n",
            "Batch 54/72 | Loss: 7.1334 | Batch Acc: 0.1250\n",
            "Batch 55/72 | Loss: 6.9851 | Batch Acc: 0.1250\n",
            "Batch 56/72 | Loss: 7.1899 | Batch Acc: 0.0625\n",
            "Batch 57/72 | Loss: 6.9671 | Batch Acc: 0.2500\n",
            "Batch 58/72 | Loss: 7.2903 | Batch Acc: 0.1250\n",
            "Batch 59/72 | Loss: 6.9761 | Batch Acc: 0.1250\n",
            "Batch 60/72 | Loss: 7.0010 | Batch Acc: 0.0000\n",
            "Batch 61/72 | Loss: 6.9797 | Batch Acc: 0.0000\n",
            "Batch 62/72 | Loss: 7.1177 | Batch Acc: 0.2500\n",
            "Batch 63/72 | Loss: 6.7069 | Batch Acc: 0.1250\n",
            "Batch 64/72 | Loss: 7.4613 | Batch Acc: 0.0625\n",
            "Batch 65/72 | Loss: 7.1533 | Batch Acc: 0.0625\n",
            "Batch 66/72 | Loss: 7.0660 | Batch Acc: 0.1875\n",
            "Batch 67/72 | Loss: 6.7782 | Batch Acc: 0.1875\n",
            "Batch 68/72 | Loss: 6.6717 | Batch Acc: 0.1250\n",
            "Batch 69/72 | Loss: 6.6844 | Batch Acc: 0.1875\n",
            "Batch 70/72 | Loss: 6.7710 | Batch Acc: 0.0625\n",
            "Batch 71/72 | Loss: 7.0685 | Batch Acc: 0.0625\n",
            "Batch 72/72 | Loss: 6.5733 | Batch Acc: 0.0000\n",
            "Epoch Train Loss: 522.6273, Accuracy: 0.0870\n",
            "Validation Accuracy: 0.1420\n",
            "‚úÖ ÂÑ≤Â≠òÊúÄ‰Ω≥Ê®°ÂûãÔºàAcc: 0.1420Ôºâ\n",
            "\n",
            "Epoch 2/50\n",
            "Batch 1/72 | Loss: 7.0670 | Batch Acc: 0.1875\n",
            "Batch 2/72 | Loss: 7.5732 | Batch Acc: 0.0625\n",
            "Batch 3/72 | Loss: 7.0240 | Batch Acc: 0.1250\n",
            "Batch 4/72 | Loss: 6.9088 | Batch Acc: 0.2500\n",
            "Batch 5/72 | Loss: 7.3837 | Batch Acc: 0.0625\n",
            "Batch 6/72 | Loss: 7.0733 | Batch Acc: 0.0000\n",
            "Batch 7/72 | Loss: 6.6977 | Batch Acc: 0.0625\n",
            "Batch 8/72 | Loss: 6.4104 | Batch Acc: 0.3750\n",
            "Batch 9/72 | Loss: 6.2666 | Batch Acc: 0.3125\n",
            "Batch 10/72 | Loss: 6.7715 | Batch Acc: 0.1250\n",
            "Batch 11/72 | Loss: 6.8362 | Batch Acc: 0.2500\n",
            "Batch 12/72 | Loss: 6.7812 | Batch Acc: 0.1875\n",
            "Batch 13/72 | Loss: 7.4405 | Batch Acc: 0.0625\n",
            "Batch 14/72 | Loss: 6.6472 | Batch Acc: 0.1875\n",
            "Batch 15/72 | Loss: 6.7572 | Batch Acc: 0.2500\n",
            "Batch 16/72 | Loss: 7.3708 | Batch Acc: 0.0000\n",
            "Batch 17/72 | Loss: 6.4624 | Batch Acc: 0.1875\n",
            "Batch 18/72 | Loss: 6.5498 | Batch Acc: 0.0625\n",
            "Batch 19/72 | Loss: 6.8838 | Batch Acc: 0.1250\n",
            "Batch 20/72 | Loss: 6.7489 | Batch Acc: 0.1875\n",
            "Batch 21/72 | Loss: 6.8818 | Batch Acc: 0.0000\n",
            "Batch 22/72 | Loss: 7.4509 | Batch Acc: 0.1250\n",
            "Batch 23/72 | Loss: 6.8435 | Batch Acc: 0.0625\n",
            "Batch 24/72 | Loss: 6.9073 | Batch Acc: 0.0625\n",
            "Batch 25/72 | Loss: 6.9977 | Batch Acc: 0.1875\n",
            "Batch 26/72 | Loss: 6.1713 | Batch Acc: 0.1250\n",
            "Batch 27/72 | Loss: 7.7476 | Batch Acc: 0.0000\n",
            "Batch 28/72 | Loss: 7.4891 | Batch Acc: 0.0625\n",
            "Batch 29/72 | Loss: 6.9587 | Batch Acc: 0.0625\n",
            "Batch 30/72 | Loss: 7.1073 | Batch Acc: 0.1250\n",
            "Batch 31/72 | Loss: 6.7050 | Batch Acc: 0.1875\n",
            "Batch 32/72 | Loss: 6.9236 | Batch Acc: 0.1250\n",
            "Batch 33/72 | Loss: 6.2524 | Batch Acc: 0.2500\n",
            "Batch 34/72 | Loss: 7.4294 | Batch Acc: 0.0625\n",
            "Batch 35/72 | Loss: 6.3318 | Batch Acc: 0.0625\n",
            "Batch 36/72 | Loss: 7.4047 | Batch Acc: 0.0625\n",
            "Batch 37/72 | Loss: 7.3128 | Batch Acc: 0.1250\n",
            "Batch 38/72 | Loss: 6.7631 | Batch Acc: 0.0625\n",
            "Batch 39/72 | Loss: 6.3821 | Batch Acc: 0.1875\n",
            "Batch 40/72 | Loss: 6.8083 | Batch Acc: 0.0000\n",
            "Batch 41/72 | Loss: 6.3523 | Batch Acc: 0.1875\n",
            "Batch 42/72 | Loss: 6.3663 | Batch Acc: 0.1250\n",
            "Batch 43/72 | Loss: 5.9466 | Batch Acc: 0.3125\n",
            "Batch 44/72 | Loss: 6.0498 | Batch Acc: 0.0625\n",
            "Batch 45/72 | Loss: 6.7562 | Batch Acc: 0.1250\n",
            "Batch 46/72 | Loss: 7.3838 | Batch Acc: 0.0625\n",
            "Batch 47/72 | Loss: 6.0222 | Batch Acc: 0.1875\n",
            "Batch 48/72 | Loss: 6.9600 | Batch Acc: 0.0625\n",
            "Batch 49/72 | Loss: 7.0712 | Batch Acc: 0.0000\n",
            "Batch 50/72 | Loss: 6.0868 | Batch Acc: 0.1250\n",
            "Batch 51/72 | Loss: 5.9468 | Batch Acc: 0.1875\n",
            "Batch 52/72 | Loss: 5.8745 | Batch Acc: 0.1250\n",
            "Batch 53/72 | Loss: 6.0500 | Batch Acc: 0.1250\n",
            "Batch 54/72 | Loss: 6.1379 | Batch Acc: 0.0625\n",
            "Batch 55/72 | Loss: 7.5229 | Batch Acc: 0.1250\n",
            "Batch 56/72 | Loss: 6.0823 | Batch Acc: 0.2500\n",
            "Batch 57/72 | Loss: 5.5234 | Batch Acc: 0.1875\n",
            "Batch 58/72 | Loss: 6.7982 | Batch Acc: 0.1875\n",
            "Batch 59/72 | Loss: 5.6414 | Batch Acc: 0.1250\n",
            "Batch 60/72 | Loss: 6.8853 | Batch Acc: 0.1875\n",
            "Batch 61/72 | Loss: 7.1768 | Batch Acc: 0.0000\n",
            "Batch 62/72 | Loss: 6.4313 | Batch Acc: 0.0625\n",
            "Batch 63/72 | Loss: 6.5249 | Batch Acc: 0.1875\n",
            "Batch 64/72 | Loss: 7.4218 | Batch Acc: 0.0625\n",
            "Batch 65/72 | Loss: 6.9832 | Batch Acc: 0.0625\n",
            "Batch 66/72 | Loss: 6.8048 | Batch Acc: 0.0625\n",
            "Batch 67/72 | Loss: 7.0551 | Batch Acc: 0.0625\n",
            "Batch 68/72 | Loss: 6.4811 | Batch Acc: 0.1250\n",
            "Batch 69/72 | Loss: 6.4397 | Batch Acc: 0.1250\n",
            "Batch 70/72 | Loss: 5.8860 | Batch Acc: 0.1875\n",
            "Batch 71/72 | Loss: 5.8038 | Batch Acc: 0.1875\n",
            "Batch 72/72 | Loss: 6.1337 | Batch Acc: 0.1538\n",
            "Epoch Train Loss: 483.1227, Accuracy: 0.1262\n",
            "Validation Accuracy: 0.1307\n",
            "\n",
            "Epoch 3/50\n",
            "Batch 1/72 | Loss: 5.5345 | Batch Acc: 0.1250\n",
            "Batch 2/72 | Loss: 6.1901 | Batch Acc: 0.1250\n",
            "Batch 3/72 | Loss: 6.5172 | Batch Acc: 0.1250\n",
            "Batch 4/72 | Loss: 6.0462 | Batch Acc: 0.0625\n",
            "Batch 5/72 | Loss: 6.0108 | Batch Acc: 0.0625\n",
            "Batch 6/72 | Loss: 7.1853 | Batch Acc: 0.0625\n",
            "Batch 7/72 | Loss: 6.6871 | Batch Acc: 0.1250\n",
            "Batch 8/72 | Loss: 5.8478 | Batch Acc: 0.1875\n",
            "Batch 9/72 | Loss: 6.6072 | Batch Acc: 0.2500\n",
            "Batch 10/72 | Loss: 6.2355 | Batch Acc: 0.0625\n",
            "Batch 11/72 | Loss: 5.4172 | Batch Acc: 0.1875\n",
            "Batch 12/72 | Loss: 6.5003 | Batch Acc: 0.1875\n",
            "Batch 13/72 | Loss: 6.9001 | Batch Acc: 0.0625\n",
            "Batch 14/72 | Loss: 7.0381 | Batch Acc: 0.0000\n",
            "Batch 15/72 | Loss: 6.6584 | Batch Acc: 0.0000\n",
            "Batch 16/72 | Loss: 6.4953 | Batch Acc: 0.1250\n",
            "Batch 17/72 | Loss: 6.6976 | Batch Acc: 0.0000\n",
            "Batch 18/72 | Loss: 6.3798 | Batch Acc: 0.1250\n",
            "Batch 19/72 | Loss: 6.6964 | Batch Acc: 0.2500\n",
            "Batch 20/72 | Loss: 6.2526 | Batch Acc: 0.0625\n",
            "Batch 21/72 | Loss: 7.4511 | Batch Acc: 0.0625\n",
            "Batch 22/72 | Loss: 5.8527 | Batch Acc: 0.2500\n",
            "Batch 23/72 | Loss: 6.0993 | Batch Acc: 0.1250\n",
            "Batch 24/72 | Loss: 6.1700 | Batch Acc: 0.0625\n",
            "Batch 25/72 | Loss: 5.9081 | Batch Acc: 0.1250\n",
            "Batch 26/72 | Loss: 6.2101 | Batch Acc: 0.2500\n",
            "Batch 27/72 | Loss: 5.9250 | Batch Acc: 0.2500\n",
            "Batch 28/72 | Loss: 7.1146 | Batch Acc: 0.0625\n",
            "Batch 29/72 | Loss: 7.4591 | Batch Acc: 0.0000\n",
            "Batch 30/72 | Loss: 6.6681 | Batch Acc: 0.1875\n",
            "Batch 31/72 | Loss: 6.2118 | Batch Acc: 0.1875\n",
            "Batch 32/72 | Loss: 6.3803 | Batch Acc: 0.1250\n",
            "Batch 33/72 | Loss: 5.9500 | Batch Acc: 0.1875\n",
            "Batch 34/72 | Loss: 6.0574 | Batch Acc: 0.1875\n",
            "Batch 35/72 | Loss: 6.8877 | Batch Acc: 0.1250\n",
            "Batch 36/72 | Loss: 6.5153 | Batch Acc: 0.0625\n",
            "Batch 37/72 | Loss: 6.7189 | Batch Acc: 0.0625\n",
            "Batch 38/72 | Loss: 6.4858 | Batch Acc: 0.0625\n",
            "Batch 39/72 | Loss: 6.1520 | Batch Acc: 0.1250\n",
            "Batch 40/72 | Loss: 5.7826 | Batch Acc: 0.2500\n",
            "Batch 41/72 | Loss: 6.5479 | Batch Acc: 0.0625\n",
            "Batch 42/72 | Loss: 5.9577 | Batch Acc: 0.2500\n",
            "Batch 43/72 | Loss: 6.2886 | Batch Acc: 0.2500\n",
            "Batch 44/72 | Loss: 7.0025 | Batch Acc: 0.1250\n",
            "Batch 45/72 | Loss: 6.3036 | Batch Acc: 0.1875\n",
            "Batch 46/72 | Loss: 6.2021 | Batch Acc: 0.1250\n",
            "Batch 47/72 | Loss: 6.5575 | Batch Acc: 0.1250\n",
            "Batch 48/72 | Loss: 7.0864 | Batch Acc: 0.0000\n",
            "Batch 49/72 | Loss: 7.1259 | Batch Acc: 0.0625\n",
            "Batch 50/72 | Loss: 6.3399 | Batch Acc: 0.1250\n",
            "Batch 51/72 | Loss: 5.7181 | Batch Acc: 0.2500\n",
            "Batch 52/72 | Loss: 6.8960 | Batch Acc: 0.2500\n",
            "Batch 53/72 | Loss: 6.5468 | Batch Acc: 0.1250\n",
            "Batch 54/72 | Loss: 6.5480 | Batch Acc: 0.0000\n",
            "Batch 55/72 | Loss: 6.7286 | Batch Acc: 0.1250\n",
            "Batch 56/72 | Loss: 5.5122 | Batch Acc: 0.1250\n",
            "Batch 57/72 | Loss: 6.4822 | Batch Acc: 0.0625\n",
            "Batch 58/72 | Loss: 6.6761 | Batch Acc: 0.1875\n",
            "Batch 59/72 | Loss: 6.5001 | Batch Acc: 0.1250\n",
            "Batch 60/72 | Loss: 6.0909 | Batch Acc: 0.1875\n",
            "Batch 61/72 | Loss: 5.5509 | Batch Acc: 0.1250\n",
            "Batch 62/72 | Loss: 6.7648 | Batch Acc: 0.0625\n",
            "Batch 63/72 | Loss: 6.7222 | Batch Acc: 0.1875\n",
            "Batch 64/72 | Loss: 6.2979 | Batch Acc: 0.0625\n",
            "Batch 65/72 | Loss: 5.9109 | Batch Acc: 0.1250\n",
            "Batch 66/72 | Loss: 6.4366 | Batch Acc: 0.0000\n",
            "Batch 67/72 | Loss: 5.9842 | Batch Acc: 0.2500\n",
            "Batch 68/72 | Loss: 6.9976 | Batch Acc: 0.0625\n",
            "Batch 69/72 | Loss: 6.2521 | Batch Acc: 0.0625\n",
            "Batch 70/72 | Loss: 6.5779 | Batch Acc: 0.2500\n",
            "Batch 71/72 | Loss: 6.4180 | Batch Acc: 0.1875\n",
            "Batch 72/72 | Loss: 5.5360 | Batch Acc: 0.0769\n",
            "Epoch Train Loss: 460.4573, Accuracy: 0.1271\n",
            "Validation Accuracy: 0.0966\n",
            "\n",
            "Epoch 4/50\n",
            "Batch 1/72 | Loss: 7.8302 | Batch Acc: 0.0000\n",
            "Batch 2/72 | Loss: 5.7761 | Batch Acc: 0.2500\n",
            "Batch 3/72 | Loss: 6.3908 | Batch Acc: 0.0625\n",
            "Batch 4/72 | Loss: 6.1273 | Batch Acc: 0.0625\n",
            "Batch 5/72 | Loss: 4.9968 | Batch Acc: 0.3125\n",
            "Batch 6/72 | Loss: 5.8810 | Batch Acc: 0.2500\n",
            "Batch 7/72 | Loss: 6.0089 | Batch Acc: 0.2500\n",
            "Batch 8/72 | Loss: 6.9855 | Batch Acc: 0.0000\n",
            "Batch 9/72 | Loss: 5.8517 | Batch Acc: 0.0625\n",
            "Batch 10/72 | Loss: 6.6490 | Batch Acc: 0.0625\n",
            "Batch 11/72 | Loss: 5.4364 | Batch Acc: 0.2500\n",
            "Batch 12/72 | Loss: 5.7457 | Batch Acc: 0.1875\n",
            "Batch 13/72 | Loss: 6.5214 | Batch Acc: 0.1250\n",
            "Batch 14/72 | Loss: 8.0364 | Batch Acc: 0.0625\n",
            "Batch 15/72 | Loss: 6.3196 | Batch Acc: 0.0000\n",
            "Batch 16/72 | Loss: 6.6746 | Batch Acc: 0.0625\n",
            "Batch 17/72 | Loss: 5.8329 | Batch Acc: 0.1250\n",
            "Batch 18/72 | Loss: 5.1892 | Batch Acc: 0.3125\n",
            "Batch 19/72 | Loss: 6.3986 | Batch Acc: 0.1250\n",
            "Batch 20/72 | Loss: 5.9809 | Batch Acc: 0.1250\n",
            "Batch 21/72 | Loss: 5.8643 | Batch Acc: 0.1875\n",
            "Batch 22/72 | Loss: 6.4116 | Batch Acc: 0.0625\n",
            "Batch 23/72 | Loss: 5.4011 | Batch Acc: 0.3750\n",
            "Batch 24/72 | Loss: 6.1195 | Batch Acc: 0.1875\n",
            "Batch 25/72 | Loss: 5.1497 | Batch Acc: 0.1875\n",
            "Batch 26/72 | Loss: 6.2822 | Batch Acc: 0.1250\n",
            "Batch 27/72 | Loss: 6.4744 | Batch Acc: 0.1250\n",
            "Batch 28/72 | Loss: 5.7571 | Batch Acc: 0.0000\n",
            "Batch 29/72 | Loss: 5.2780 | Batch Acc: 0.2500\n",
            "Batch 30/72 | Loss: 6.0469 | Batch Acc: 0.1875\n",
            "Batch 31/72 | Loss: 7.3037 | Batch Acc: 0.1250\n",
            "Batch 32/72 | Loss: 6.5076 | Batch Acc: 0.0625\n",
            "Batch 33/72 | Loss: 5.8914 | Batch Acc: 0.1875\n",
            "Batch 34/72 | Loss: 5.0699 | Batch Acc: 0.2500\n",
            "Batch 35/72 | Loss: 6.7921 | Batch Acc: 0.1250\n",
            "Batch 36/72 | Loss: 6.6997 | Batch Acc: 0.0000\n",
            "Batch 37/72 | Loss: 6.3812 | Batch Acc: 0.2500\n",
            "Batch 38/72 | Loss: 7.2212 | Batch Acc: 0.1250\n",
            "Batch 39/72 | Loss: 6.2812 | Batch Acc: 0.2500\n",
            "Batch 40/72 | Loss: 6.1451 | Batch Acc: 0.2500\n",
            "Batch 41/72 | Loss: 6.9752 | Batch Acc: 0.1250\n",
            "Batch 42/72 | Loss: 6.5899 | Batch Acc: 0.1250\n",
            "Batch 43/72 | Loss: 5.8638 | Batch Acc: 0.1250\n",
            "Batch 44/72 | Loss: 6.0799 | Batch Acc: 0.1250\n",
            "Batch 45/72 | Loss: 6.7227 | Batch Acc: 0.1875\n",
            "Batch 46/72 | Loss: 5.7333 | Batch Acc: 0.1875\n",
            "Batch 47/72 | Loss: 6.3038 | Batch Acc: 0.0000\n",
            "Batch 48/72 | Loss: 5.4661 | Batch Acc: 0.1875\n",
            "Batch 49/72 | Loss: 6.3290 | Batch Acc: 0.1250\n",
            "Batch 50/72 | Loss: 5.6302 | Batch Acc: 0.1875\n",
            "Batch 51/72 | Loss: 6.4312 | Batch Acc: 0.0000\n",
            "Batch 52/72 | Loss: 6.3856 | Batch Acc: 0.1250\n",
            "Batch 53/72 | Loss: 6.9321 | Batch Acc: 0.0625\n",
            "Batch 54/72 | Loss: 6.0680 | Batch Acc: 0.1250\n",
            "Batch 55/72 | Loss: 7.1024 | Batch Acc: 0.2500\n",
            "Batch 56/72 | Loss: 6.8584 | Batch Acc: 0.0625\n",
            "Batch 57/72 | Loss: 6.6628 | Batch Acc: 0.1875\n",
            "Batch 58/72 | Loss: 5.9868 | Batch Acc: 0.1250\n",
            "Batch 59/72 | Loss: 6.0260 | Batch Acc: 0.0625\n",
            "Batch 60/72 | Loss: 6.3911 | Batch Acc: 0.1250\n",
            "Batch 61/72 | Loss: 6.2668 | Batch Acc: 0.1250\n",
            "Batch 62/72 | Loss: 6.6152 | Batch Acc: 0.0000\n",
            "Batch 63/72 | Loss: 5.6527 | Batch Acc: 0.2500\n",
            "Batch 64/72 | Loss: 6.2971 | Batch Acc: 0.2500\n",
            "Batch 65/72 | Loss: 5.8751 | Batch Acc: 0.1250\n",
            "Batch 66/72 | Loss: 6.5614 | Batch Acc: 0.1875\n",
            "Batch 67/72 | Loss: 6.1764 | Batch Acc: 0.0625\n",
            "Batch 68/72 | Loss: 6.1640 | Batch Acc: 0.0625\n",
            "Batch 69/72 | Loss: 6.3977 | Batch Acc: 0.1875\n",
            "Batch 70/72 | Loss: 5.7964 | Batch Acc: 0.1875\n",
            "Batch 71/72 | Loss: 5.8732 | Batch Acc: 0.1875\n",
            "Batch 72/72 | Loss: 5.8524 | Batch Acc: 0.1538\n",
            "Epoch Train Loss: 447.7775, Accuracy: 0.1427\n",
            "Validation Accuracy: 0.1761\n",
            "‚úÖ ÂÑ≤Â≠òÊúÄ‰Ω≥Ê®°ÂûãÔºàAcc: 0.1761Ôºâ\n",
            "\n",
            "Epoch 5/50\n",
            "Batch 1/72 | Loss: 6.6335 | Batch Acc: 0.0625\n",
            "Batch 2/72 | Loss: 5.8253 | Batch Acc: 0.0625\n",
            "Batch 3/72 | Loss: 5.3426 | Batch Acc: 0.3125\n",
            "Batch 4/72 | Loss: 6.2329 | Batch Acc: 0.0625\n",
            "Batch 5/72 | Loss: 6.4727 | Batch Acc: 0.1250\n",
            "Batch 6/72 | Loss: 5.8912 | Batch Acc: 0.3125\n",
            "Batch 7/72 | Loss: 6.1212 | Batch Acc: 0.0000\n",
            "Batch 8/72 | Loss: 4.9415 | Batch Acc: 0.2500\n",
            "Batch 9/72 | Loss: 6.8402 | Batch Acc: 0.1250\n",
            "Batch 10/72 | Loss: 6.5593 | Batch Acc: 0.0000\n",
            "Batch 11/72 | Loss: 5.0817 | Batch Acc: 0.1250\n",
            "Batch 12/72 | Loss: 6.1545 | Batch Acc: 0.0625\n",
            "Batch 13/72 | Loss: 5.7955 | Batch Acc: 0.0625\n",
            "Batch 14/72 | Loss: 6.3965 | Batch Acc: 0.1250\n",
            "Batch 15/72 | Loss: 5.9911 | Batch Acc: 0.0625\n",
            "Batch 16/72 | Loss: 6.6420 | Batch Acc: 0.0625\n",
            "Batch 17/72 | Loss: 7.0403 | Batch Acc: 0.1250\n",
            "Batch 18/72 | Loss: 6.0666 | Batch Acc: 0.0625\n",
            "Batch 19/72 | Loss: 6.4145 | Batch Acc: 0.1250\n",
            "Batch 20/72 | Loss: 6.1589 | Batch Acc: 0.1875\n",
            "Batch 21/72 | Loss: 6.6604 | Batch Acc: 0.1250\n",
            "Batch 22/72 | Loss: 6.0290 | Batch Acc: 0.1875\n",
            "Batch 23/72 | Loss: 6.5880 | Batch Acc: 0.0625\n",
            "Batch 24/72 | Loss: 5.3195 | Batch Acc: 0.1250\n",
            "Batch 25/72 | Loss: 6.2856 | Batch Acc: 0.2500\n",
            "Batch 26/72 | Loss: 5.7945 | Batch Acc: 0.2500\n",
            "Batch 27/72 | Loss: 5.7482 | Batch Acc: 0.2500\n",
            "Batch 28/72 | Loss: 5.5638 | Batch Acc: 0.3125\n",
            "Batch 29/72 | Loss: 6.3150 | Batch Acc: 0.1250\n",
            "Batch 30/72 | Loss: 5.5508 | Batch Acc: 0.3125\n",
            "Batch 31/72 | Loss: 7.4411 | Batch Acc: 0.0000\n",
            "Batch 32/72 | Loss: 6.6654 | Batch Acc: 0.1250\n",
            "Batch 33/72 | Loss: 7.3627 | Batch Acc: 0.1250\n",
            "Batch 34/72 | Loss: 6.3144 | Batch Acc: 0.1875\n",
            "Batch 35/72 | Loss: 5.1989 | Batch Acc: 0.3125\n",
            "Batch 36/72 | Loss: 6.0027 | Batch Acc: 0.3125\n",
            "Batch 37/72 | Loss: 5.8109 | Batch Acc: 0.2500\n",
            "Batch 38/72 | Loss: 6.2895 | Batch Acc: 0.2500\n",
            "Batch 39/72 | Loss: 6.1811 | Batch Acc: 0.1875\n",
            "Batch 40/72 | Loss: 5.8610 | Batch Acc: 0.1875\n",
            "Batch 41/72 | Loss: 5.9123 | Batch Acc: 0.0625\n",
            "Batch 42/72 | Loss: 5.9086 | Batch Acc: 0.1875\n",
            "Batch 43/72 | Loss: 5.9944 | Batch Acc: 0.1250\n",
            "Batch 44/72 | Loss: 5.5317 | Batch Acc: 0.1875\n",
            "Batch 45/72 | Loss: 5.8068 | Batch Acc: 0.1875\n",
            "Batch 46/72 | Loss: 6.1239 | Batch Acc: 0.1875\n",
            "Batch 47/72 | Loss: 6.1502 | Batch Acc: 0.1250\n",
            "Batch 48/72 | Loss: 6.2574 | Batch Acc: 0.1250\n",
            "Batch 49/72 | Loss: 5.7594 | Batch Acc: 0.1875\n",
            "Batch 50/72 | Loss: 5.6031 | Batch Acc: 0.1250\n",
            "Batch 51/72 | Loss: 5.9142 | Batch Acc: 0.1250\n",
            "Batch 52/72 | Loss: 6.0995 | Batch Acc: 0.1875\n",
            "Batch 53/72 | Loss: 7.2813 | Batch Acc: 0.0000\n",
            "Batch 54/72 | Loss: 5.2882 | Batch Acc: 0.1875\n",
            "Batch 55/72 | Loss: 7.6727 | Batch Acc: 0.1875\n",
            "Batch 56/72 | Loss: 7.1923 | Batch Acc: 0.1250\n",
            "Batch 57/72 | Loss: 6.2748 | Batch Acc: 0.0625\n",
            "Batch 58/72 | Loss: 6.3090 | Batch Acc: 0.1250\n",
            "Batch 59/72 | Loss: 6.6989 | Batch Acc: 0.0000\n",
            "Batch 60/72 | Loss: 6.5541 | Batch Acc: 0.1250\n",
            "Batch 61/72 | Loss: 6.2785 | Batch Acc: 0.0625\n",
            "Batch 62/72 | Loss: 5.5630 | Batch Acc: 0.1875\n",
            "Batch 63/72 | Loss: 5.8122 | Batch Acc: 0.3750\n",
            "Batch 64/72 | Loss: 5.2868 | Batch Acc: 0.2500\n",
            "Batch 65/72 | Loss: 6.4498 | Batch Acc: 0.1250\n",
            "Batch 66/72 | Loss: 6.1795 | Batch Acc: 0.1875\n",
            "Batch 67/72 | Loss: 5.7757 | Batch Acc: 0.0625\n",
            "Batch 68/72 | Loss: 5.1350 | Batch Acc: 0.1875\n",
            "Batch 69/72 | Loss: 5.5159 | Batch Acc: 0.0625\n",
            "Batch 70/72 | Loss: 6.2501 | Batch Acc: 0.1875\n",
            "Batch 71/72 | Loss: 6.1001 | Batch Acc: 0.2500\n",
            "Batch 72/72 | Loss: 5.2503 | Batch Acc: 0.1538\n",
            "Epoch Train Loss: 439.5141, Accuracy: 0.1523\n",
            "Validation Accuracy: 0.1705\n",
            "\n",
            "Epoch 6/50\n",
            "Batch 1/72 | Loss: 6.2848 | Batch Acc: 0.1250\n",
            "Batch 2/72 | Loss: 5.4236 | Batch Acc: 0.1875\n",
            "Batch 3/72 | Loss: 5.9690 | Batch Acc: 0.0625\n",
            "Batch 4/72 | Loss: 6.0131 | Batch Acc: 0.1250\n",
            "Batch 5/72 | Loss: 6.2655 | Batch Acc: 0.1875\n",
            "Batch 6/72 | Loss: 6.2012 | Batch Acc: 0.0625\n",
            "Batch 7/72 | Loss: 6.1260 | Batch Acc: 0.4375\n",
            "Batch 8/72 | Loss: 6.7068 | Batch Acc: 0.0625\n",
            "Batch 9/72 | Loss: 5.2962 | Batch Acc: 0.2500\n",
            "Batch 10/72 | Loss: 5.6922 | Batch Acc: 0.0625\n",
            "Batch 11/72 | Loss: 5.7646 | Batch Acc: 0.0625\n",
            "Batch 12/72 | Loss: 6.6761 | Batch Acc: 0.0625\n",
            "Batch 13/72 | Loss: 5.8884 | Batch Acc: 0.1250\n",
            "Batch 14/72 | Loss: 5.4762 | Batch Acc: 0.3125\n",
            "Batch 15/72 | Loss: 6.1043 | Batch Acc: 0.2500\n",
            "Batch 16/72 | Loss: 5.5031 | Batch Acc: 0.2500\n",
            "Batch 17/72 | Loss: 6.6645 | Batch Acc: 0.1875\n",
            "Batch 18/72 | Loss: 5.4673 | Batch Acc: 0.1875\n",
            "Batch 19/72 | Loss: 5.5567 | Batch Acc: 0.3750\n",
            "Batch 20/72 | Loss: 7.3435 | Batch Acc: 0.0000\n",
            "Batch 21/72 | Loss: 5.6867 | Batch Acc: 0.1250\n",
            "Batch 22/72 | Loss: 5.6483 | Batch Acc: 0.1875\n",
            "Batch 23/72 | Loss: 5.0748 | Batch Acc: 0.3125\n",
            "Batch 24/72 | Loss: 5.9687 | Batch Acc: 0.1250\n",
            "Batch 25/72 | Loss: 6.2773 | Batch Acc: 0.1250\n",
            "Batch 26/72 | Loss: 4.5215 | Batch Acc: 0.5000\n",
            "Batch 27/72 | Loss: 5.4358 | Batch Acc: 0.1250\n",
            "Batch 28/72 | Loss: 5.4648 | Batch Acc: 0.1875\n",
            "Batch 29/72 | Loss: 6.8484 | Batch Acc: 0.0625\n",
            "Batch 30/72 | Loss: 6.5039 | Batch Acc: 0.1250\n",
            "Batch 31/72 | Loss: 7.0745 | Batch Acc: 0.1250\n",
            "Batch 32/72 | Loss: 5.4217 | Batch Acc: 0.3750\n",
            "Batch 33/72 | Loss: 6.4442 | Batch Acc: 0.1250\n",
            "Batch 34/72 | Loss: 6.7260 | Batch Acc: 0.1875\n",
            "Batch 35/72 | Loss: 6.2138 | Batch Acc: 0.1250\n",
            "Batch 36/72 | Loss: 6.8424 | Batch Acc: 0.1250\n",
            "Batch 37/72 | Loss: 6.6342 | Batch Acc: 0.1250\n",
            "Batch 38/72 | Loss: 5.8795 | Batch Acc: 0.1250\n",
            "Batch 39/72 | Loss: 6.9597 | Batch Acc: 0.1250\n",
            "Batch 40/72 | Loss: 5.9082 | Batch Acc: 0.0625\n",
            "Batch 41/72 | Loss: 5.9116 | Batch Acc: 0.1250\n",
            "Batch 42/72 | Loss: 6.9391 | Batch Acc: 0.1250\n",
            "Batch 43/72 | Loss: 6.0596 | Batch Acc: 0.1250\n",
            "Batch 44/72 | Loss: 6.9161 | Batch Acc: 0.0625\n",
            "Batch 45/72 | Loss: 6.7135 | Batch Acc: 0.1250\n",
            "Batch 46/72 | Loss: 5.8986 | Batch Acc: 0.1875\n",
            "Batch 47/72 | Loss: 5.7151 | Batch Acc: 0.3125\n",
            "Batch 48/72 | Loss: 6.9744 | Batch Acc: 0.1250\n",
            "Batch 49/72 | Loss: 6.3325 | Batch Acc: 0.0625\n",
            "Batch 50/72 | Loss: 5.8070 | Batch Acc: 0.0625\n",
            "Batch 51/72 | Loss: 6.2418 | Batch Acc: 0.0625\n",
            "Batch 52/72 | Loss: 7.4433 | Batch Acc: 0.0625\n",
            "Batch 53/72 | Loss: 5.9541 | Batch Acc: 0.1875\n",
            "Batch 54/72 | Loss: 5.9297 | Batch Acc: 0.1875\n",
            "Batch 55/72 | Loss: 5.8516 | Batch Acc: 0.1875\n",
            "Batch 56/72 | Loss: 5.5640 | Batch Acc: 0.0000\n",
            "Batch 57/72 | Loss: 5.2612 | Batch Acc: 0.1875\n",
            "Batch 58/72 | Loss: 5.1780 | Batch Acc: 0.1250\n",
            "Batch 59/72 | Loss: 5.3138 | Batch Acc: 0.1250\n",
            "Batch 60/72 | Loss: 5.9890 | Batch Acc: 0.0625\n",
            "Batch 61/72 | Loss: 5.7122 | Batch Acc: 0.1250\n",
            "Batch 62/72 | Loss: 5.7995 | Batch Acc: 0.1250\n",
            "Batch 63/72 | Loss: 6.5298 | Batch Acc: 0.1250\n",
            "Batch 64/72 | Loss: 5.5160 | Batch Acc: 0.2500\n",
            "Batch 65/72 | Loss: 5.6288 | Batch Acc: 0.1250\n",
            "Batch 66/72 | Loss: 6.1858 | Batch Acc: 0.2500\n",
            "Batch 67/72 | Loss: 5.6299 | Batch Acc: 0.2500\n",
            "Batch 68/72 | Loss: 6.1430 | Batch Acc: 0.1875\n",
            "Batch 69/72 | Loss: 6.3556 | Batch Acc: 0.1875\n",
            "Batch 70/72 | Loss: 5.1475 | Batch Acc: 0.3750\n",
            "Batch 71/72 | Loss: 6.1499 | Batch Acc: 0.0625\n",
            "Batch 72/72 | Loss: 6.4298 | Batch Acc: 0.0769\n",
            "Epoch Train Loss: 435.1794, Accuracy: 0.1584\n",
            "Validation Accuracy: 0.1705\n",
            "\n",
            "Epoch 7/50\n",
            "Batch 1/72 | Loss: 5.1746 | Batch Acc: 0.1250\n",
            "Batch 2/72 | Loss: 6.6038 | Batch Acc: 0.0000\n",
            "Batch 3/72 | Loss: 6.1364 | Batch Acc: 0.0000\n",
            "Batch 4/72 | Loss: 6.4089 | Batch Acc: 0.1250\n",
            "Batch 5/72 | Loss: 4.9381 | Batch Acc: 0.3125\n",
            "Batch 6/72 | Loss: 6.1536 | Batch Acc: 0.3125\n",
            "Batch 7/72 | Loss: 5.4280 | Batch Acc: 0.1250\n",
            "Batch 8/72 | Loss: 5.3746 | Batch Acc: 0.1875\n",
            "Batch 9/72 | Loss: 6.0052 | Batch Acc: 0.2500\n",
            "Batch 10/72 | Loss: 5.6921 | Batch Acc: 0.1875\n",
            "Batch 11/72 | Loss: 6.1317 | Batch Acc: 0.0000\n",
            "Batch 12/72 | Loss: 5.3032 | Batch Acc: 0.3125\n",
            "Batch 13/72 | Loss: 6.2088 | Batch Acc: 0.1875\n",
            "Batch 14/72 | Loss: 5.1210 | Batch Acc: 0.1250\n",
            "Batch 15/72 | Loss: 6.6596 | Batch Acc: 0.1250\n",
            "Batch 16/72 | Loss: 6.0542 | Batch Acc: 0.2500\n",
            "Batch 17/72 | Loss: 5.9858 | Batch Acc: 0.1250\n",
            "Batch 18/72 | Loss: 5.7645 | Batch Acc: 0.2500\n",
            "Batch 19/72 | Loss: 5.8635 | Batch Acc: 0.3125\n",
            "Batch 20/72 | Loss: 6.5594 | Batch Acc: 0.0625\n",
            "Batch 21/72 | Loss: 6.0216 | Batch Acc: 0.1250\n",
            "Batch 22/72 | Loss: 7.2304 | Batch Acc: 0.0625\n",
            "Batch 23/72 | Loss: 6.2494 | Batch Acc: 0.1250\n",
            "Batch 24/72 | Loss: 5.3118 | Batch Acc: 0.1250\n",
            "Batch 25/72 | Loss: 5.8405 | Batch Acc: 0.1875\n",
            "Batch 26/72 | Loss: 5.9715 | Batch Acc: 0.0625\n",
            "Batch 27/72 | Loss: 6.1315 | Batch Acc: 0.1875\n",
            "Batch 28/72 | Loss: 6.3936 | Batch Acc: 0.2500\n",
            "Batch 29/72 | Loss: 5.5930 | Batch Acc: 0.1250\n",
            "Batch 30/72 | Loss: 5.4923 | Batch Acc: 0.3750\n",
            "Batch 31/72 | Loss: 5.0223 | Batch Acc: 0.3125\n",
            "Batch 32/72 | Loss: 5.7475 | Batch Acc: 0.0625\n",
            "Batch 33/72 | Loss: 5.9587 | Batch Acc: 0.0625\n",
            "Batch 34/72 | Loss: 5.7962 | Batch Acc: 0.3125\n",
            "Batch 35/72 | Loss: 6.1489 | Batch Acc: 0.1875\n",
            "Batch 36/72 | Loss: 6.2581 | Batch Acc: 0.0625\n",
            "Batch 37/72 | Loss: 6.8880 | Batch Acc: 0.0000\n",
            "Batch 38/72 | Loss: 5.6526 | Batch Acc: 0.1250\n",
            "Batch 39/72 | Loss: 6.4927 | Batch Acc: 0.1875\n",
            "Batch 40/72 | Loss: 5.6871 | Batch Acc: 0.2500\n",
            "Batch 41/72 | Loss: 5.3870 | Batch Acc: 0.1875\n",
            "Batch 42/72 | Loss: 5.8248 | Batch Acc: 0.3125\n",
            "Batch 43/72 | Loss: 6.3729 | Batch Acc: 0.1875\n",
            "Batch 44/72 | Loss: 5.9673 | Batch Acc: 0.2500\n",
            "Batch 45/72 | Loss: 6.5278 | Batch Acc: 0.2500\n",
            "Batch 46/72 | Loss: 5.8906 | Batch Acc: 0.1875\n",
            "Batch 47/72 | Loss: 5.8607 | Batch Acc: 0.3125\n",
            "Batch 48/72 | Loss: 5.9247 | Batch Acc: 0.0000\n",
            "Batch 49/72 | Loss: 5.1600 | Batch Acc: 0.3125\n",
            "Batch 50/72 | Loss: 5.4199 | Batch Acc: 0.1875\n",
            "Batch 51/72 | Loss: 5.6914 | Batch Acc: 0.1250\n",
            "Batch 52/72 | Loss: 6.2681 | Batch Acc: 0.0000\n",
            "Batch 53/72 | Loss: 5.7179 | Batch Acc: 0.3125\n",
            "Batch 54/72 | Loss: 5.3988 | Batch Acc: 0.3125\n",
            "Batch 55/72 | Loss: 5.2801 | Batch Acc: 0.1875\n",
            "Batch 56/72 | Loss: 5.5357 | Batch Acc: 0.1875\n",
            "Batch 57/72 | Loss: 6.0367 | Batch Acc: 0.0625\n",
            "Batch 58/72 | Loss: 5.5056 | Batch Acc: 0.1250\n",
            "Batch 59/72 | Loss: 5.7220 | Batch Acc: 0.1875\n",
            "Batch 60/72 | Loss: 6.0933 | Batch Acc: 0.0625\n",
            "Batch 61/72 | Loss: 5.4986 | Batch Acc: 0.1875\n",
            "Batch 62/72 | Loss: 5.9145 | Batch Acc: 0.1250\n",
            "Batch 63/72 | Loss: 5.8393 | Batch Acc: 0.1875\n",
            "Batch 64/72 | Loss: 6.5275 | Batch Acc: 0.0625\n",
            "Batch 65/72 | Loss: 4.9463 | Batch Acc: 0.1875\n",
            "Batch 66/72 | Loss: 5.3426 | Batch Acc: 0.3750\n",
            "Batch 67/72 | Loss: 6.6229 | Batch Acc: 0.1250\n",
            "Batch 68/72 | Loss: 5.5966 | Batch Acc: 0.1250\n",
            "Batch 69/72 | Loss: 6.4171 | Batch Acc: 0.0625\n",
            "Batch 70/72 | Loss: 5.3329 | Batch Acc: 0.1875\n",
            "Batch 71/72 | Loss: 6.0838 | Batch Acc: 0.0625\n",
            "Batch 72/72 | Loss: 4.9180 | Batch Acc: 0.2308\n",
            "Epoch Train Loss: 422.0803, Accuracy: 0.1697\n",
            "Validation Accuracy: 0.2159\n",
            "‚úÖ ÂÑ≤Â≠òÊúÄ‰Ω≥Ê®°ÂûãÔºàAcc: 0.2159Ôºâ\n",
            "\n",
            "Epoch 8/50\n",
            "Batch 1/72 | Loss: 6.5592 | Batch Acc: 0.1250\n",
            "Batch 2/72 | Loss: 6.5946 | Batch Acc: 0.1250\n",
            "Batch 3/72 | Loss: 5.5705 | Batch Acc: 0.2500\n",
            "Batch 4/72 | Loss: 6.6623 | Batch Acc: 0.1250\n",
            "Batch 5/72 | Loss: 5.2515 | Batch Acc: 0.0625\n",
            "Batch 6/72 | Loss: 5.2078 | Batch Acc: 0.0625\n",
            "Batch 7/72 | Loss: 4.9960 | Batch Acc: 0.3750\n",
            "Batch 8/72 | Loss: 6.5101 | Batch Acc: 0.1875\n",
            "Batch 9/72 | Loss: 6.0261 | Batch Acc: 0.1250\n",
            "Batch 10/72 | Loss: 5.4470 | Batch Acc: 0.1250\n",
            "Batch 11/72 | Loss: 6.1110 | Batch Acc: 0.1875\n",
            "Batch 12/72 | Loss: 6.9217 | Batch Acc: 0.0000\n",
            "Batch 13/72 | Loss: 5.0377 | Batch Acc: 0.1875\n",
            "Batch 14/72 | Loss: 5.6234 | Batch Acc: 0.2500\n",
            "Batch 15/72 | Loss: 5.0081 | Batch Acc: 0.2500\n",
            "Batch 16/72 | Loss: 5.6196 | Batch Acc: 0.0625\n",
            "Batch 17/72 | Loss: 5.8405 | Batch Acc: 0.0625\n",
            "Batch 18/72 | Loss: 6.6695 | Batch Acc: 0.1875\n",
            "Batch 19/72 | Loss: 6.1547 | Batch Acc: 0.0625\n",
            "Batch 20/72 | Loss: 5.6150 | Batch Acc: 0.1875\n",
            "Batch 21/72 | Loss: 5.7620 | Batch Acc: 0.1875\n",
            "Batch 22/72 | Loss: 6.5325 | Batch Acc: 0.0625\n",
            "Batch 23/72 | Loss: 6.6340 | Batch Acc: 0.1250\n",
            "Batch 24/72 | Loss: 5.9723 | Batch Acc: 0.2500\n",
            "Batch 25/72 | Loss: 5.8755 | Batch Acc: 0.3125\n",
            "Batch 26/72 | Loss: 6.2076 | Batch Acc: 0.1875\n",
            "Batch 27/72 | Loss: 5.6265 | Batch Acc: 0.2500\n",
            "Batch 28/72 | Loss: 6.5342 | Batch Acc: 0.0000\n",
            "Batch 29/72 | Loss: 6.3106 | Batch Acc: 0.1875\n",
            "Batch 30/72 | Loss: 5.2315 | Batch Acc: 0.2500\n",
            "Batch 31/72 | Loss: 6.0263 | Batch Acc: 0.3125\n",
            "Batch 32/72 | Loss: 5.3374 | Batch Acc: 0.4375\n",
            "Batch 33/72 | Loss: 5.7932 | Batch Acc: 0.1250\n",
            "Batch 34/72 | Loss: 6.5799 | Batch Acc: 0.0000\n",
            "Batch 35/72 | Loss: 5.6719 | Batch Acc: 0.1875\n",
            "Batch 36/72 | Loss: 7.6073 | Batch Acc: 0.0625\n",
            "Batch 37/72 | Loss: 5.8971 | Batch Acc: 0.1250\n",
            "Batch 38/72 | Loss: 5.4355 | Batch Acc: 0.1875\n",
            "Batch 39/72 | Loss: 5.1922 | Batch Acc: 0.3750\n",
            "Batch 40/72 | Loss: 6.6777 | Batch Acc: 0.0625\n",
            "Batch 41/72 | Loss: 6.0786 | Batch Acc: 0.0625\n",
            "Batch 42/72 | Loss: 6.0439 | Batch Acc: 0.0625\n",
            "Batch 43/72 | Loss: 6.1304 | Batch Acc: 0.1250\n",
            "Batch 44/72 | Loss: 5.7082 | Batch Acc: 0.2500\n",
            "Batch 45/72 | Loss: 5.1903 | Batch Acc: 0.2500\n",
            "Batch 46/72 | Loss: 6.0794 | Batch Acc: 0.3125\n",
            "Batch 47/72 | Loss: 6.3587 | Batch Acc: 0.3750\n",
            "Batch 48/72 | Loss: 6.3919 | Batch Acc: 0.0625\n",
            "Batch 49/72 | Loss: 5.4127 | Batch Acc: 0.1250\n",
            "Batch 50/72 | Loss: 5.4283 | Batch Acc: 0.1875\n",
            "Batch 51/72 | Loss: 5.0307 | Batch Acc: 0.3750\n",
            "Batch 52/72 | Loss: 5.6977 | Batch Acc: 0.2500\n",
            "Batch 53/72 | Loss: 6.4035 | Batch Acc: 0.0000\n",
            "Batch 54/72 | Loss: 6.3986 | Batch Acc: 0.3125\n",
            "Batch 55/72 | Loss: 4.7032 | Batch Acc: 0.3125\n",
            "Batch 56/72 | Loss: 6.2778 | Batch Acc: 0.1250\n",
            "Batch 57/72 | Loss: 5.5447 | Batch Acc: 0.1250\n",
            "Batch 58/72 | Loss: 5.2325 | Batch Acc: 0.2500\n",
            "Batch 59/72 | Loss: 5.0214 | Batch Acc: 0.3125\n",
            "Batch 60/72 | Loss: 5.8338 | Batch Acc: 0.1250\n",
            "Batch 61/72 | Loss: 6.1772 | Batch Acc: 0.0625\n",
            "Batch 62/72 | Loss: 5.2444 | Batch Acc: 0.3750\n",
            "Batch 63/72 | Loss: 5.2462 | Batch Acc: 0.2500\n",
            "Batch 64/72 | Loss: 5.4661 | Batch Acc: 0.1875\n",
            "Batch 65/72 | Loss: 6.7588 | Batch Acc: 0.1875\n",
            "Batch 66/72 | Loss: 6.8307 | Batch Acc: 0.1875\n",
            "Batch 67/72 | Loss: 5.7615 | Batch Acc: 0.0625\n",
            "Batch 68/72 | Loss: 5.9549 | Batch Acc: 0.1875\n",
            "Batch 69/72 | Loss: 5.7890 | Batch Acc: 0.1250\n",
            "Batch 70/72 | Loss: 5.0914 | Batch Acc: 0.3125\n",
            "Batch 71/72 | Loss: 6.1399 | Batch Acc: 0.1250\n",
            "Batch 72/72 | Loss: 7.7138 | Batch Acc: 0.0769\n",
            "Epoch Train Loss: 425.4716, Accuracy: 0.1784\n",
            "Validation Accuracy: 0.1705\n",
            "\n",
            "Epoch 9/50\n",
            "Batch 1/72 | Loss: 5.3708 | Batch Acc: 0.1250\n",
            "Batch 2/72 | Loss: 6.2824 | Batch Acc: 0.1875\n",
            "Batch 3/72 | Loss: 5.0609 | Batch Acc: 0.2500\n",
            "Batch 4/72 | Loss: 5.7696 | Batch Acc: 0.3125\n",
            "Batch 5/72 | Loss: 5.1319 | Batch Acc: 0.3125\n",
            "Batch 6/72 | Loss: 4.6865 | Batch Acc: 0.1250\n",
            "Batch 7/72 | Loss: 6.9146 | Batch Acc: 0.1250\n",
            "Batch 8/72 | Loss: 5.4591 | Batch Acc: 0.2500\n",
            "Batch 9/72 | Loss: 5.9022 | Batch Acc: 0.3125\n",
            "Batch 10/72 | Loss: 6.1377 | Batch Acc: 0.0625\n",
            "Batch 11/72 | Loss: 5.4141 | Batch Acc: 0.3125\n",
            "Batch 12/72 | Loss: 5.7611 | Batch Acc: 0.3125\n",
            "Batch 13/72 | Loss: 5.4793 | Batch Acc: 0.1250\n",
            "Batch 14/72 | Loss: 6.5311 | Batch Acc: 0.0625\n",
            "Batch 15/72 | Loss: 5.9325 | Batch Acc: 0.0625\n",
            "Batch 16/72 | Loss: 4.5608 | Batch Acc: 0.2500\n",
            "Batch 17/72 | Loss: 6.6843 | Batch Acc: 0.0000\n",
            "Batch 18/72 | Loss: 6.2828 | Batch Acc: 0.1250\n",
            "Batch 19/72 | Loss: 6.3049 | Batch Acc: 0.0625\n",
            "Batch 20/72 | Loss: 8.1831 | Batch Acc: 0.0000\n",
            "Batch 21/72 | Loss: 6.3079 | Batch Acc: 0.1250\n",
            "Batch 22/72 | Loss: 5.6479 | Batch Acc: 0.1250\n",
            "Batch 23/72 | Loss: 5.7069 | Batch Acc: 0.1875\n",
            "Batch 24/72 | Loss: 5.4342 | Batch Acc: 0.1875\n",
            "Batch 25/72 | Loss: 5.5143 | Batch Acc: 0.1875\n",
            "Batch 26/72 | Loss: 5.8796 | Batch Acc: 0.3125\n",
            "Batch 27/72 | Loss: 6.0583 | Batch Acc: 0.1875\n",
            "Batch 28/72 | Loss: 6.1633 | Batch Acc: 0.1875\n",
            "Batch 29/72 | Loss: 6.0359 | Batch Acc: 0.1875\n",
            "Batch 30/72 | Loss: 6.7161 | Batch Acc: 0.0000\n",
            "Batch 31/72 | Loss: 7.1960 | Batch Acc: 0.1250\n",
            "Batch 32/72 | Loss: 5.8490 | Batch Acc: 0.0000\n",
            "Batch 33/72 | Loss: 4.8114 | Batch Acc: 0.3750\n",
            "Batch 34/72 | Loss: 6.3554 | Batch Acc: 0.1875\n",
            "Batch 35/72 | Loss: 5.9048 | Batch Acc: 0.3750\n",
            "Batch 36/72 | Loss: 5.8366 | Batch Acc: 0.2500\n",
            "Batch 37/72 | Loss: 5.6578 | Batch Acc: 0.0625\n",
            "Batch 38/72 | Loss: 6.0985 | Batch Acc: 0.2500\n",
            "Batch 39/72 | Loss: 6.3479 | Batch Acc: 0.1875\n",
            "Batch 40/72 | Loss: 6.5811 | Batch Acc: 0.0625\n",
            "Batch 41/72 | Loss: 5.0707 | Batch Acc: 0.1250\n",
            "Batch 42/72 | Loss: 5.6811 | Batch Acc: 0.1250\n",
            "Batch 43/72 | Loss: 5.8926 | Batch Acc: 0.2500\n",
            "Batch 44/72 | Loss: 5.7136 | Batch Acc: 0.1250\n",
            "Batch 45/72 | Loss: 5.0628 | Batch Acc: 0.3125\n",
            "Batch 46/72 | Loss: 6.2107 | Batch Acc: 0.1875\n",
            "Batch 47/72 | Loss: 6.4877 | Batch Acc: 0.1875\n",
            "Batch 48/72 | Loss: 5.4232 | Batch Acc: 0.2500\n",
            "Batch 49/72 | Loss: 5.8995 | Batch Acc: 0.1250\n",
            "Batch 50/72 | Loss: 5.4448 | Batch Acc: 0.2500\n",
            "Batch 51/72 | Loss: 5.1944 | Batch Acc: 0.1875\n",
            "Batch 52/72 | Loss: 5.4348 | Batch Acc: 0.1875\n",
            "Batch 53/72 | Loss: 6.5395 | Batch Acc: 0.1250\n",
            "Batch 54/72 | Loss: 6.1920 | Batch Acc: 0.1250\n",
            "Batch 55/72 | Loss: 5.6628 | Batch Acc: 0.3125\n",
            "Batch 56/72 | Loss: 5.3092 | Batch Acc: 0.1875\n",
            "Batch 57/72 | Loss: 5.5068 | Batch Acc: 0.1250\n",
            "Batch 58/72 | Loss: 6.1140 | Batch Acc: 0.1250\n",
            "Batch 59/72 | Loss: 5.7379 | Batch Acc: 0.3125\n",
            "Batch 60/72 | Loss: 5.8697 | Batch Acc: 0.1250\n",
            "Batch 61/72 | Loss: 5.4720 | Batch Acc: 0.3750\n",
            "Batch 62/72 | Loss: 5.5938 | Batch Acc: 0.0625\n",
            "Batch 63/72 | Loss: 6.0586 | Batch Acc: 0.1875\n",
            "Batch 64/72 | Loss: 5.9300 | Batch Acc: 0.1250\n",
            "Batch 65/72 | Loss: 5.8556 | Batch Acc: 0.1250\n",
            "Batch 66/72 | Loss: 5.2414 | Batch Acc: 0.4375\n",
            "Batch 67/72 | Loss: 5.4669 | Batch Acc: 0.1250\n",
            "Batch 68/72 | Loss: 5.9159 | Batch Acc: 0.1250\n",
            "Batch 69/72 | Loss: 5.5780 | Batch Acc: 0.2500\n",
            "Batch 70/72 | Loss: 5.1611 | Batch Acc: 0.3750\n",
            "Batch 71/72 | Loss: 5.5809 | Batch Acc: 0.1250\n",
            "Batch 72/72 | Loss: 7.1148 | Batch Acc: 0.2308\n",
            "Epoch Train Loss: 421.3695, Accuracy: 0.1836\n",
            "Validation Accuracy: 0.2273\n",
            "‚úÖ ÂÑ≤Â≠òÊúÄ‰Ω≥Ê®°ÂûãÔºàAcc: 0.2273Ôºâ\n",
            "\n",
            "Epoch 10/50\n",
            "Batch 1/72 | Loss: 5.6718 | Batch Acc: 0.1875\n",
            "Batch 2/72 | Loss: 6.3277 | Batch Acc: 0.1250\n",
            "Batch 3/72 | Loss: 6.6690 | Batch Acc: 0.1250\n",
            "Batch 4/72 | Loss: 6.7015 | Batch Acc: 0.1250\n",
            "Batch 5/72 | Loss: 5.0412 | Batch Acc: 0.2500\n",
            "Batch 6/72 | Loss: 6.2335 | Batch Acc: 0.1250\n",
            "Batch 7/72 | Loss: 5.1203 | Batch Acc: 0.3750\n",
            "Batch 8/72 | Loss: 5.6619 | Batch Acc: 0.0625\n",
            "Batch 9/72 | Loss: 5.8985 | Batch Acc: 0.1250\n",
            "Batch 10/72 | Loss: 5.8926 | Batch Acc: 0.1250\n",
            "Batch 11/72 | Loss: 5.5267 | Batch Acc: 0.1250\n",
            "Batch 12/72 | Loss: 6.2039 | Batch Acc: 0.1250\n",
            "Batch 13/72 | Loss: 6.2206 | Batch Acc: 0.0000\n",
            "Batch 14/72 | Loss: 4.8483 | Batch Acc: 0.3125\n",
            "Batch 15/72 | Loss: 6.0428 | Batch Acc: 0.1875\n",
            "Batch 16/72 | Loss: 5.5712 | Batch Acc: 0.2500\n",
            "Batch 17/72 | Loss: 5.5791 | Batch Acc: 0.0625\n",
            "Batch 18/72 | Loss: 6.1398 | Batch Acc: 0.0000\n",
            "Batch 19/72 | Loss: 5.8308 | Batch Acc: 0.1875\n",
            "Batch 20/72 | Loss: 5.5441 | Batch Acc: 0.1875\n",
            "Batch 21/72 | Loss: 4.9091 | Batch Acc: 0.1875\n",
            "Batch 22/72 | Loss: 6.2781 | Batch Acc: 0.0000\n",
            "Batch 23/72 | Loss: 5.3872 | Batch Acc: 0.1875\n",
            "Batch 24/72 | Loss: 5.4700 | Batch Acc: 0.3125\n",
            "Batch 25/72 | Loss: 5.3829 | Batch Acc: 0.2500\n",
            "Batch 26/72 | Loss: 5.5707 | Batch Acc: 0.1250\n",
            "Batch 27/72 | Loss: 4.7240 | Batch Acc: 0.1875\n",
            "Batch 28/72 | Loss: 5.0188 | Batch Acc: 0.1250\n",
            "Batch 29/72 | Loss: 6.1831 | Batch Acc: 0.0625\n",
            "Batch 30/72 | Loss: 6.5422 | Batch Acc: 0.0625\n",
            "Batch 31/72 | Loss: 7.0687 | Batch Acc: 0.1250\n",
            "Batch 32/72 | Loss: 4.8748 | Batch Acc: 0.2500\n",
            "Batch 33/72 | Loss: 4.6911 | Batch Acc: 0.3750\n",
            "Batch 34/72 | Loss: 6.3234 | Batch Acc: 0.0625\n",
            "Batch 35/72 | Loss: 5.4125 | Batch Acc: 0.2500\n",
            "Batch 36/72 | Loss: 5.4004 | Batch Acc: 0.3125\n",
            "Batch 37/72 | Loss: 6.9146 | Batch Acc: 0.1875\n",
            "Batch 38/72 | Loss: 5.5133 | Batch Acc: 0.1250\n",
            "Batch 39/72 | Loss: 5.9210 | Batch Acc: 0.1250\n",
            "Batch 40/72 | Loss: 5.7014 | Batch Acc: 0.1875\n",
            "Batch 41/72 | Loss: 5.9433 | Batch Acc: 0.1875\n",
            "Batch 42/72 | Loss: 5.8807 | Batch Acc: 0.1875\n",
            "Batch 43/72 | Loss: 5.9747 | Batch Acc: 0.1250\n",
            "Batch 44/72 | Loss: 6.4267 | Batch Acc: 0.0625\n",
            "Batch 45/72 | Loss: 5.8187 | Batch Acc: 0.2500\n",
            "Batch 46/72 | Loss: 5.3672 | Batch Acc: 0.1250\n",
            "Batch 47/72 | Loss: 6.1772 | Batch Acc: 0.1875\n",
            "Batch 48/72 | Loss: 4.7656 | Batch Acc: 0.3125\n",
            "Batch 49/72 | Loss: 6.1143 | Batch Acc: 0.2500\n",
            "Batch 50/72 | Loss: 6.0706 | Batch Acc: 0.1875\n",
            "Batch 51/72 | Loss: 6.9722 | Batch Acc: 0.0625\n",
            "Batch 52/72 | Loss: 6.0142 | Batch Acc: 0.1875\n",
            "Batch 53/72 | Loss: 5.2838 | Batch Acc: 0.2500\n",
            "Batch 54/72 | Loss: 5.8747 | Batch Acc: 0.1250\n",
            "Batch 55/72 | Loss: 6.6862 | Batch Acc: 0.0000\n",
            "Batch 56/72 | Loss: 5.5119 | Batch Acc: 0.1875\n",
            "Batch 57/72 | Loss: 6.2972 | Batch Acc: 0.0000\n",
            "Batch 58/72 | Loss: 4.7004 | Batch Acc: 0.1875\n",
            "Batch 59/72 | Loss: 5.5014 | Batch Acc: 0.2500\n",
            "Batch 60/72 | Loss: 5.0703 | Batch Acc: 0.3125\n",
            "Batch 61/72 | Loss: 4.4784 | Batch Acc: 0.1875\n",
            "Batch 62/72 | Loss: 6.4592 | Batch Acc: 0.1250\n",
            "Batch 63/72 | Loss: 6.7499 | Batch Acc: 0.0625\n",
            "Batch 64/72 | Loss: 5.6123 | Batch Acc: 0.1250\n",
            "Batch 65/72 | Loss: 6.3770 | Batch Acc: 0.1250\n",
            "Batch 66/72 | Loss: 6.7370 | Batch Acc: 0.1250\n",
            "Batch 67/72 | Loss: 5.8292 | Batch Acc: 0.1250\n",
            "Batch 68/72 | Loss: 5.8692 | Batch Acc: 0.1875\n",
            "Batch 69/72 | Loss: 5.0114 | Batch Acc: 0.2500\n",
            "Batch 70/72 | Loss: 4.8908 | Batch Acc: 0.3750\n",
            "Batch 71/72 | Loss: 6.1040 | Batch Acc: 0.3750\n",
            "Batch 72/72 | Loss: 5.5567 | Batch Acc: 0.2308\n",
            "Epoch Train Loss: 416.1390, Accuracy: 0.1706\n",
            "Validation Accuracy: 0.1818\n",
            "\n",
            "Epoch 11/50\n",
            "Batch 1/72 | Loss: 7.1086 | Batch Acc: 0.2500\n",
            "Batch 2/72 | Loss: 6.0048 | Batch Acc: 0.1250\n",
            "Batch 3/72 | Loss: 5.1927 | Batch Acc: 0.3125\n",
            "Batch 4/72 | Loss: 5.9451 | Batch Acc: 0.2500\n",
            "Batch 5/72 | Loss: 6.4507 | Batch Acc: 0.0625\n",
            "Batch 6/72 | Loss: 6.0347 | Batch Acc: 0.1875\n",
            "Batch 7/72 | Loss: 6.2309 | Batch Acc: 0.1875\n",
            "Batch 8/72 | Loss: 5.6402 | Batch Acc: 0.0625\n",
            "Batch 9/72 | Loss: 6.5713 | Batch Acc: 0.1875\n",
            "Batch 10/72 | Loss: 5.7600 | Batch Acc: 0.3125\n",
            "Batch 11/72 | Loss: 6.6923 | Batch Acc: 0.0625\n",
            "Batch 12/72 | Loss: 4.9623 | Batch Acc: 0.2500\n",
            "Batch 13/72 | Loss: 5.7080 | Batch Acc: 0.2500\n",
            "Batch 14/72 | Loss: 5.6436 | Batch Acc: 0.3750\n",
            "Batch 15/72 | Loss: 6.0020 | Batch Acc: 0.2500\n",
            "Batch 16/72 | Loss: 6.1492 | Batch Acc: 0.0625\n",
            "Batch 17/72 | Loss: 5.7562 | Batch Acc: 0.2500\n",
            "Batch 18/72 | Loss: 5.3723 | Batch Acc: 0.1875\n",
            "Batch 19/72 | Loss: 5.5459 | Batch Acc: 0.3125\n",
            "Batch 20/72 | Loss: 5.2161 | Batch Acc: 0.2500\n",
            "Batch 21/72 | Loss: 6.1177 | Batch Acc: 0.0000\n",
            "Batch 22/72 | Loss: 6.1065 | Batch Acc: 0.1250\n",
            "Batch 23/72 | Loss: 5.2164 | Batch Acc: 0.3750\n",
            "Batch 24/72 | Loss: 6.7189 | Batch Acc: 0.1250\n",
            "Batch 25/72 | Loss: 5.8661 | Batch Acc: 0.1250\n",
            "Batch 26/72 | Loss: 5.5880 | Batch Acc: 0.3125\n",
            "Batch 27/72 | Loss: 6.6608 | Batch Acc: 0.1250\n",
            "Batch 28/72 | Loss: 5.1221 | Batch Acc: 0.2500\n",
            "Batch 29/72 | Loss: 5.4850 | Batch Acc: 0.2500\n",
            "Batch 30/72 | Loss: 6.7992 | Batch Acc: 0.0625\n",
            "Batch 31/72 | Loss: 5.8533 | Batch Acc: 0.0625\n",
            "Batch 32/72 | Loss: 5.6359 | Batch Acc: 0.3125\n",
            "Batch 33/72 | Loss: 4.5893 | Batch Acc: 0.1875\n",
            "Batch 34/72 | Loss: 6.6910 | Batch Acc: 0.1250\n",
            "Batch 35/72 | Loss: 5.7273 | Batch Acc: 0.0625\n",
            "Batch 36/72 | Loss: 6.3057 | Batch Acc: 0.1875\n",
            "Batch 37/72 | Loss: 6.4180 | Batch Acc: 0.1250\n",
            "Batch 38/72 | Loss: 5.0726 | Batch Acc: 0.1875\n",
            "Batch 39/72 | Loss: 6.2114 | Batch Acc: 0.0625\n",
            "Batch 40/72 | Loss: 6.4588 | Batch Acc: 0.2500\n",
            "Batch 41/72 | Loss: 5.2805 | Batch Acc: 0.1250\n",
            "Batch 42/72 | Loss: 5.1711 | Batch Acc: 0.2500\n",
            "Batch 43/72 | Loss: 5.1487 | Batch Acc: 0.1875\n",
            "Batch 44/72 | Loss: 5.6734 | Batch Acc: 0.0000\n",
            "Batch 45/72 | Loss: 6.7178 | Batch Acc: 0.1250\n",
            "Batch 46/72 | Loss: 5.0871 | Batch Acc: 0.2500\n",
            "Batch 47/72 | Loss: 7.9056 | Batch Acc: 0.1250\n",
            "Batch 48/72 | Loss: 6.4008 | Batch Acc: 0.0625\n",
            "Batch 49/72 | Loss: 6.6628 | Batch Acc: 0.0625\n",
            "Batch 50/72 | Loss: 5.2474 | Batch Acc: 0.1875\n",
            "Batch 51/72 | Loss: 6.0646 | Batch Acc: 0.1875\n",
            "Batch 52/72 | Loss: 5.2210 | Batch Acc: 0.2500\n",
            "Batch 53/72 | Loss: 5.3136 | Batch Acc: 0.1250\n",
            "Batch 54/72 | Loss: 5.0984 | Batch Acc: 0.2500\n",
            "Batch 55/72 | Loss: 5.6865 | Batch Acc: 0.1875\n",
            "Batch 56/72 | Loss: 5.1048 | Batch Acc: 0.3750\n",
            "Batch 57/72 | Loss: 4.8082 | Batch Acc: 0.1875\n",
            "Batch 58/72 | Loss: 5.8917 | Batch Acc: 0.1875\n",
            "Batch 59/72 | Loss: 6.0178 | Batch Acc: 0.1875\n",
            "Batch 60/72 | Loss: 5.8158 | Batch Acc: 0.1875\n",
            "Batch 61/72 | Loss: 6.5750 | Batch Acc: 0.1875\n",
            "Batch 62/72 | Loss: 6.1657 | Batch Acc: 0.1875\n",
            "Batch 63/72 | Loss: 4.8648 | Batch Acc: 0.3125\n",
            "Batch 64/72 | Loss: 5.7317 | Batch Acc: 0.2500\n",
            "Batch 65/72 | Loss: 5.3019 | Batch Acc: 0.1250\n",
            "Batch 66/72 | Loss: 6.1884 | Batch Acc: 0.1250\n",
            "Batch 67/72 | Loss: 4.8196 | Batch Acc: 0.2500\n",
            "Batch 68/72 | Loss: 5.9872 | Batch Acc: 0.1250\n",
            "Batch 69/72 | Loss: 5.3225 | Batch Acc: 0.3125\n",
            "Batch 70/72 | Loss: 6.1203 | Batch Acc: 0.1250\n",
            "Batch 71/72 | Loss: 5.9516 | Batch Acc: 0.1875\n",
            "Batch 72/72 | Loss: 6.9567 | Batch Acc: 0.1538\n",
            "Epoch Train Loss: 420.9338, Accuracy: 0.1854\n",
            "Validation Accuracy: 0.2386\n",
            "‚úÖ ÂÑ≤Â≠òÊúÄ‰Ω≥Ê®°ÂûãÔºàAcc: 0.2386Ôºâ\n",
            "\n",
            "Epoch 12/50\n",
            "Batch 1/72 | Loss: 5.4269 | Batch Acc: 0.2500\n",
            "Batch 2/72 | Loss: 6.6066 | Batch Acc: 0.3125\n",
            "Batch 3/72 | Loss: 6.5462 | Batch Acc: 0.0625\n",
            "Batch 4/72 | Loss: 6.4262 | Batch Acc: 0.1875\n",
            "Batch 5/72 | Loss: 5.7464 | Batch Acc: 0.0625\n",
            "Batch 6/72 | Loss: 6.4860 | Batch Acc: 0.2500\n",
            "Batch 7/72 | Loss: 4.9904 | Batch Acc: 0.1875\n",
            "Batch 8/72 | Loss: 4.9966 | Batch Acc: 0.2500\n",
            "Batch 9/72 | Loss: 6.0323 | Batch Acc: 0.1875\n",
            "Batch 10/72 | Loss: 5.7545 | Batch Acc: 0.1875\n",
            "Batch 11/72 | Loss: 5.7503 | Batch Acc: 0.1250\n",
            "Batch 12/72 | Loss: 5.2535 | Batch Acc: 0.3125\n",
            "Batch 13/72 | Loss: 5.7771 | Batch Acc: 0.1875\n",
            "Batch 14/72 | Loss: 5.7864 | Batch Acc: 0.1250\n",
            "Batch 15/72 | Loss: 5.4104 | Batch Acc: 0.1875\n",
            "Batch 16/72 | Loss: 6.0313 | Batch Acc: 0.1250\n",
            "Batch 17/72 | Loss: 7.0164 | Batch Acc: 0.0000\n",
            "Batch 18/72 | Loss: 5.1054 | Batch Acc: 0.3750\n",
            "Batch 19/72 | Loss: 6.4657 | Batch Acc: 0.1875\n",
            "Batch 20/72 | Loss: 5.2693 | Batch Acc: 0.0625\n",
            "Batch 21/72 | Loss: 5.4712 | Batch Acc: 0.2500\n",
            "Batch 22/72 | Loss: 5.9033 | Batch Acc: 0.3125\n",
            "Batch 23/72 | Loss: 6.7452 | Batch Acc: 0.1250\n",
            "Batch 24/72 | Loss: 5.9491 | Batch Acc: 0.2500\n",
            "Batch 25/72 | Loss: 6.3678 | Batch Acc: 0.1250\n",
            "Batch 26/72 | Loss: 5.7989 | Batch Acc: 0.3125\n",
            "Batch 27/72 | Loss: 5.8052 | Batch Acc: 0.1875\n",
            "Batch 28/72 | Loss: 5.3414 | Batch Acc: 0.1875\n",
            "Batch 29/72 | Loss: 5.7656 | Batch Acc: 0.1875\n",
            "Batch 30/72 | Loss: 4.8767 | Batch Acc: 0.2500\n",
            "Batch 31/72 | Loss: 6.1298 | Batch Acc: 0.1875\n",
            "Batch 32/72 | Loss: 6.2689 | Batch Acc: 0.0625\n",
            "Batch 33/72 | Loss: 5.5654 | Batch Acc: 0.1250\n",
            "Batch 34/72 | Loss: 5.6146 | Batch Acc: 0.1875\n",
            "Batch 35/72 | Loss: 5.4794 | Batch Acc: 0.1250\n",
            "Batch 36/72 | Loss: 4.8235 | Batch Acc: 0.3125\n",
            "Batch 37/72 | Loss: 6.5959 | Batch Acc: 0.1250\n",
            "Batch 38/72 | Loss: 5.6901 | Batch Acc: 0.3750\n",
            "Batch 39/72 | Loss: 6.4266 | Batch Acc: 0.1875\n",
            "Batch 40/72 | Loss: 5.5797 | Batch Acc: 0.1250\n",
            "Batch 41/72 | Loss: 5.1177 | Batch Acc: 0.3125\n",
            "Batch 42/72 | Loss: 5.5368 | Batch Acc: 0.1250\n",
            "Batch 43/72 | Loss: 5.4680 | Batch Acc: 0.2500\n",
            "Batch 44/72 | Loss: 4.8859 | Batch Acc: 0.3125\n",
            "Batch 45/72 | Loss: 6.3010 | Batch Acc: 0.1250\n",
            "Batch 46/72 | Loss: 5.7596 | Batch Acc: 0.2500\n",
            "Batch 47/72 | Loss: 5.4687 | Batch Acc: 0.1875\n",
            "Batch 48/72 | Loss: 4.9851 | Batch Acc: 0.1875\n",
            "Batch 49/72 | Loss: 5.7638 | Batch Acc: 0.1875\n",
            "Batch 50/72 | Loss: 4.6879 | Batch Acc: 0.2500\n",
            "Batch 51/72 | Loss: 6.9561 | Batch Acc: 0.0625\n",
            "Batch 52/72 | Loss: 5.3319 | Batch Acc: 0.1250\n",
            "Batch 53/72 | Loss: 6.0232 | Batch Acc: 0.0625\n",
            "Batch 54/72 | Loss: 5.9612 | Batch Acc: 0.1875\n",
            "Batch 55/72 | Loss: 6.3917 | Batch Acc: 0.0625\n",
            "Batch 56/72 | Loss: 6.9961 | Batch Acc: 0.0625\n",
            "Batch 57/72 | Loss: 5.1579 | Batch Acc: 0.3125\n",
            "Batch 58/72 | Loss: 5.8492 | Batch Acc: 0.0625\n",
            "Batch 59/72 | Loss: 6.1072 | Batch Acc: 0.1250\n",
            "Batch 60/72 | Loss: 5.4336 | Batch Acc: 0.3125\n",
            "Batch 61/72 | Loss: 5.2777 | Batch Acc: 0.3125\n",
            "Batch 62/72 | Loss: 5.5324 | Batch Acc: 0.1875\n",
            "Batch 63/72 | Loss: 5.6469 | Batch Acc: 0.1875\n",
            "Batch 64/72 | Loss: 6.5842 | Batch Acc: 0.2500\n",
            "Batch 65/72 | Loss: 7.1600 | Batch Acc: 0.0625\n",
            "Batch 66/72 | Loss: 5.7719 | Batch Acc: 0.1250\n",
            "Batch 67/72 | Loss: 6.1839 | Batch Acc: 0.1875\n",
            "Batch 68/72 | Loss: 6.5903 | Batch Acc: 0.1250\n",
            "Batch 69/72 | Loss: 5.6483 | Batch Acc: 0.2500\n",
            "Batch 70/72 | Loss: 6.3195 | Batch Acc: 0.1250\n",
            "Batch 71/72 | Loss: 5.9595 | Batch Acc: 0.1875\n",
            "Batch 72/72 | Loss: 5.5594 | Batch Acc: 0.1538\n",
            "Epoch Train Loss: 419.4928, Accuracy: 0.1854\n",
            "Validation Accuracy: 0.2557\n",
            "‚úÖ ÂÑ≤Â≠òÊúÄ‰Ω≥Ê®°ÂûãÔºàAcc: 0.2557Ôºâ\n",
            "\n",
            "Epoch 13/50\n",
            "Batch 1/72 | Loss: 6.3065 | Batch Acc: 0.1250\n",
            "Batch 2/72 | Loss: 6.2441 | Batch Acc: 0.1250\n",
            "Batch 3/72 | Loss: 5.6425 | Batch Acc: 0.1250\n",
            "Batch 4/72 | Loss: 6.5909 | Batch Acc: 0.2500\n",
            "Batch 5/72 | Loss: 5.7275 | Batch Acc: 0.1250\n",
            "Batch 6/72 | Loss: 5.2742 | Batch Acc: 0.3125\n",
            "Batch 7/72 | Loss: 5.7181 | Batch Acc: 0.1250\n",
            "Batch 8/72 | Loss: 5.9790 | Batch Acc: 0.1875\n",
            "Batch 9/72 | Loss: 5.0369 | Batch Acc: 0.3125\n",
            "Batch 10/72 | Loss: 5.7093 | Batch Acc: 0.2500\n",
            "Batch 11/72 | Loss: 5.9980 | Batch Acc: 0.1250\n",
            "Batch 12/72 | Loss: 5.3824 | Batch Acc: 0.4375\n",
            "Batch 13/72 | Loss: 5.1426 | Batch Acc: 0.1875\n",
            "Batch 14/72 | Loss: 5.3053 | Batch Acc: 0.2500\n",
            "Batch 15/72 | Loss: 5.2173 | Batch Acc: 0.3750\n",
            "Batch 16/72 | Loss: 6.5904 | Batch Acc: 0.1875\n",
            "Batch 17/72 | Loss: 5.2966 | Batch Acc: 0.1875\n",
            "Batch 18/72 | Loss: 5.4239 | Batch Acc: 0.2500\n",
            "Batch 19/72 | Loss: 5.6709 | Batch Acc: 0.2500\n",
            "Batch 20/72 | Loss: 6.2392 | Batch Acc: 0.0625\n",
            "Batch 21/72 | Loss: 6.1080 | Batch Acc: 0.0625\n",
            "Batch 22/72 | Loss: 6.2600 | Batch Acc: 0.2500\n",
            "Batch 23/72 | Loss: 5.9520 | Batch Acc: 0.1875\n",
            "Batch 24/72 | Loss: 5.1234 | Batch Acc: 0.1875\n",
            "Batch 25/72 | Loss: 5.4753 | Batch Acc: 0.1250\n",
            "Batch 26/72 | Loss: 6.6562 | Batch Acc: 0.1875\n",
            "Batch 27/72 | Loss: 5.6778 | Batch Acc: 0.0625\n",
            "Batch 28/72 | Loss: 5.1750 | Batch Acc: 0.3125\n",
            "Batch 29/72 | Loss: 5.3079 | Batch Acc: 0.2500\n",
            "Batch 30/72 | Loss: 5.4232 | Batch Acc: 0.3125\n",
            "Batch 31/72 | Loss: 5.9836 | Batch Acc: 0.0625\n",
            "Batch 32/72 | Loss: 5.0114 | Batch Acc: 0.3125\n",
            "Batch 33/72 | Loss: 5.0711 | Batch Acc: 0.0625\n",
            "Batch 34/72 | Loss: 6.4541 | Batch Acc: 0.3125\n",
            "Batch 35/72 | Loss: 6.7720 | Batch Acc: 0.1875\n",
            "Batch 36/72 | Loss: 6.0928 | Batch Acc: 0.1875\n",
            "Batch 37/72 | Loss: 5.4270 | Batch Acc: 0.1250\n",
            "Batch 38/72 | Loss: 7.1606 | Batch Acc: 0.1250\n",
            "Batch 39/72 | Loss: 4.9460 | Batch Acc: 0.1875\n",
            "Batch 40/72 | Loss: 6.3811 | Batch Acc: 0.1250\n",
            "Batch 41/72 | Loss: 5.3708 | Batch Acc: 0.1875\n",
            "Batch 42/72 | Loss: 5.8269 | Batch Acc: 0.1875\n",
            "Batch 43/72 | Loss: 6.5081 | Batch Acc: 0.0625\n",
            "Batch 44/72 | Loss: 5.6336 | Batch Acc: 0.1250\n",
            "Batch 45/72 | Loss: 6.7643 | Batch Acc: 0.2500\n",
            "Batch 46/72 | Loss: 5.9728 | Batch Acc: 0.2500\n",
            "Batch 47/72 | Loss: 4.9262 | Batch Acc: 0.1250\n",
            "Batch 48/72 | Loss: 4.7352 | Batch Acc: 0.1250\n",
            "Batch 49/72 | Loss: 5.6336 | Batch Acc: 0.1875\n",
            "Batch 50/72 | Loss: 5.7142 | Batch Acc: 0.1250\n",
            "Batch 51/72 | Loss: 5.4644 | Batch Acc: 0.1250\n",
            "Batch 52/72 | Loss: 5.4622 | Batch Acc: 0.3125\n",
            "Batch 53/72 | Loss: 4.3169 | Batch Acc: 0.2500\n",
            "Batch 54/72 | Loss: 4.3171 | Batch Acc: 0.5000\n",
            "Batch 55/72 | Loss: 5.2205 | Batch Acc: 0.2500\n",
            "Batch 56/72 | Loss: 6.5919 | Batch Acc: 0.1250\n",
            "Batch 57/72 | Loss: 6.5079 | Batch Acc: 0.1250\n",
            "Batch 58/72 | Loss: 6.1790 | Batch Acc: 0.1250\n",
            "Batch 59/72 | Loss: 6.5104 | Batch Acc: 0.0625\n",
            "Batch 60/72 | Loss: 5.7832 | Batch Acc: 0.2500\n",
            "Batch 61/72 | Loss: 5.1504 | Batch Acc: 0.2500\n",
            "Batch 62/72 | Loss: 4.8841 | Batch Acc: 0.3125\n",
            "Batch 63/72 | Loss: 5.5057 | Batch Acc: 0.1250\n",
            "Batch 64/72 | Loss: 5.9277 | Batch Acc: 0.1875\n",
            "Batch 65/72 | Loss: 5.5763 | Batch Acc: 0.1250\n",
            "Batch 66/72 | Loss: 5.8243 | Batch Acc: 0.1875\n",
            "Batch 67/72 | Loss: 6.2722 | Batch Acc: 0.1250\n",
            "Batch 68/72 | Loss: 5.4717 | Batch Acc: 0.1875\n",
            "Batch 69/72 | Loss: 5.5440 | Batch Acc: 0.1250\n",
            "Batch 70/72 | Loss: 7.7926 | Batch Acc: 0.1250\n",
            "Batch 71/72 | Loss: 5.1782 | Batch Acc: 0.2500\n",
            "Batch 72/72 | Loss: 4.9878 | Batch Acc: 0.3077\n",
            "Epoch Train Loss: 412.5078, Accuracy: 0.1932\n",
            "Validation Accuracy: 0.2386\n",
            "\n",
            "Epoch 14/50\n",
            "Batch 1/72 | Loss: 5.4138 | Batch Acc: 0.1250\n",
            "Batch 2/72 | Loss: 5.6237 | Batch Acc: 0.1875\n",
            "Batch 3/72 | Loss: 5.8310 | Batch Acc: 0.1875\n",
            "Batch 4/72 | Loss: 6.0023 | Batch Acc: 0.0000\n",
            "Batch 5/72 | Loss: 4.2122 | Batch Acc: 0.3750\n",
            "Batch 6/72 | Loss: 4.5382 | Batch Acc: 0.3125\n",
            "Batch 7/72 | Loss: 6.2587 | Batch Acc: 0.1875\n",
            "Batch 8/72 | Loss: 6.5162 | Batch Acc: 0.1250\n",
            "Batch 9/72 | Loss: 5.5908 | Batch Acc: 0.4375\n",
            "Batch 10/72 | Loss: 6.0556 | Batch Acc: 0.2500\n",
            "Batch 11/72 | Loss: 6.0063 | Batch Acc: 0.2500\n",
            "Batch 12/72 | Loss: 5.9969 | Batch Acc: 0.1250\n",
            "Batch 13/72 | Loss: 5.8570 | Batch Acc: 0.1250\n",
            "Batch 14/72 | Loss: 5.9321 | Batch Acc: 0.0625\n",
            "Batch 15/72 | Loss: 4.9608 | Batch Acc: 0.3750\n",
            "Batch 16/72 | Loss: 6.2795 | Batch Acc: 0.1875\n",
            "Batch 17/72 | Loss: 5.8222 | Batch Acc: 0.3125\n",
            "Batch 18/72 | Loss: 6.1083 | Batch Acc: 0.0625\n",
            "Batch 19/72 | Loss: 6.4972 | Batch Acc: 0.1875\n",
            "Batch 20/72 | Loss: 5.3739 | Batch Acc: 0.1875\n",
            "Batch 21/72 | Loss: 4.9434 | Batch Acc: 0.4375\n",
            "Batch 22/72 | Loss: 4.2164 | Batch Acc: 0.3125\n",
            "Batch 23/72 | Loss: 5.7622 | Batch Acc: 0.2500\n",
            "Batch 24/72 | Loss: 6.6306 | Batch Acc: 0.1250\n",
            "Batch 25/72 | Loss: 5.3179 | Batch Acc: 0.4375\n",
            "Batch 26/72 | Loss: 5.3858 | Batch Acc: 0.3125\n",
            "Batch 27/72 | Loss: 6.1054 | Batch Acc: 0.2500\n",
            "Batch 28/72 | Loss: 4.6976 | Batch Acc: 0.3750\n",
            "Batch 29/72 | Loss: 4.7284 | Batch Acc: 0.2500\n",
            "Batch 30/72 | Loss: 5.7392 | Batch Acc: 0.3125\n",
            "Batch 31/72 | Loss: 5.0212 | Batch Acc: 0.3125\n",
            "Batch 32/72 | Loss: 5.1910 | Batch Acc: 0.1875\n",
            "Batch 33/72 | Loss: 5.1024 | Batch Acc: 0.3125\n",
            "Batch 34/72 | Loss: 6.9930 | Batch Acc: 0.0000\n",
            "Batch 35/72 | Loss: 5.2468 | Batch Acc: 0.1875\n",
            "Batch 36/72 | Loss: 5.6211 | Batch Acc: 0.3125\n",
            "Batch 37/72 | Loss: 4.6772 | Batch Acc: 0.3750\n",
            "Batch 38/72 | Loss: 4.8110 | Batch Acc: 0.3125\n",
            "Batch 39/72 | Loss: 5.2851 | Batch Acc: 0.2500\n",
            "Batch 40/72 | Loss: 5.5622 | Batch Acc: 0.2500\n",
            "Batch 41/72 | Loss: 4.8632 | Batch Acc: 0.3125\n",
            "Batch 42/72 | Loss: 6.2785 | Batch Acc: 0.1875\n",
            "Batch 43/72 | Loss: 5.6786 | Batch Acc: 0.1250\n",
            "Batch 44/72 | Loss: 5.2105 | Batch Acc: 0.2500\n",
            "Batch 45/72 | Loss: 5.7671 | Batch Acc: 0.1875\n",
            "Batch 46/72 | Loss: 5.0519 | Batch Acc: 0.0625\n",
            "Batch 47/72 | Loss: 6.8688 | Batch Acc: 0.1875\n",
            "Batch 48/72 | Loss: 5.0258 | Batch Acc: 0.3125\n",
            "Batch 49/72 | Loss: 5.1586 | Batch Acc: 0.2500\n",
            "Batch 50/72 | Loss: 4.8887 | Batch Acc: 0.2500\n",
            "Batch 51/72 | Loss: 4.7846 | Batch Acc: 0.1875\n",
            "Batch 52/72 | Loss: 5.5048 | Batch Acc: 0.3125\n",
            "Batch 53/72 | Loss: 5.1438 | Batch Acc: 0.1875\n",
            "Batch 54/72 | Loss: 6.1448 | Batch Acc: 0.2500\n",
            "Batch 55/72 | Loss: 6.3003 | Batch Acc: 0.1875\n",
            "Batch 56/72 | Loss: 5.6209 | Batch Acc: 0.2500\n",
            "Batch 57/72 | Loss: 6.4687 | Batch Acc: 0.1250\n",
            "Batch 58/72 | Loss: 6.9209 | Batch Acc: 0.2500\n",
            "Batch 59/72 | Loss: 5.4423 | Batch Acc: 0.3125\n",
            "Batch 60/72 | Loss: 5.2070 | Batch Acc: 0.3750\n",
            "Batch 61/72 | Loss: 6.5838 | Batch Acc: 0.1250\n",
            "Batch 62/72 | Loss: 5.3829 | Batch Acc: 0.3125\n",
            "Batch 63/72 | Loss: 5.2046 | Batch Acc: 0.1875\n",
            "Batch 64/72 | Loss: 6.1780 | Batch Acc: 0.1250\n",
            "Batch 65/72 | Loss: 6.6346 | Batch Acc: 0.1875\n",
            "Batch 66/72 | Loss: 5.6147 | Batch Acc: 0.3125\n",
            "Batch 67/72 | Loss: 5.5706 | Batch Acc: 0.3125\n",
            "Batch 68/72 | Loss: 4.7335 | Batch Acc: 0.1250\n",
            "Batch 69/72 | Loss: 6.6667 | Batch Acc: 0.0625\n",
            "Batch 70/72 | Loss: 5.2506 | Batch Acc: 0.2500\n",
            "Batch 71/72 | Loss: 6.0643 | Batch Acc: 0.3125\n",
            "Batch 72/72 | Loss: 5.6098 | Batch Acc: 0.3077\n",
            "Epoch Train Loss: 403.7386, Accuracy: 0.2332\n",
            "Validation Accuracy: 0.2216\n",
            "\n",
            "Epoch 15/50\n",
            "Batch 1/72 | Loss: 7.1046 | Batch Acc: 0.1250\n",
            "Batch 2/72 | Loss: 5.1012 | Batch Acc: 0.2500\n",
            "Batch 3/72 | Loss: 5.7992 | Batch Acc: 0.0625\n",
            "Batch 4/72 | Loss: 5.7681 | Batch Acc: 0.1875\n",
            "Batch 5/72 | Loss: 6.0080 | Batch Acc: 0.1875\n",
            "Batch 6/72 | Loss: 4.8898 | Batch Acc: 0.1875\n",
            "Batch 7/72 | Loss: 6.0003 | Batch Acc: 0.1250\n",
            "Batch 8/72 | Loss: 5.0022 | Batch Acc: 0.1875\n",
            "Batch 9/72 | Loss: 5.8919 | Batch Acc: 0.1875\n",
            "Batch 10/72 | Loss: 5.1073 | Batch Acc: 0.1875\n",
            "Batch 11/72 | Loss: 7.1132 | Batch Acc: 0.0625\n",
            "Batch 12/72 | Loss: 6.1485 | Batch Acc: 0.2500\n",
            "Batch 13/72 | Loss: 5.0660 | Batch Acc: 0.3750\n",
            "Batch 14/72 | Loss: 5.9974 | Batch Acc: 0.1875\n",
            "Batch 15/72 | Loss: 4.0084 | Batch Acc: 0.3750\n",
            "Batch 16/72 | Loss: 5.0693 | Batch Acc: 0.1875\n",
            "Batch 17/72 | Loss: 4.7653 | Batch Acc: 0.3125\n",
            "Batch 18/72 | Loss: 6.0306 | Batch Acc: 0.1250\n",
            "Batch 19/72 | Loss: 4.8473 | Batch Acc: 0.4375\n",
            "Batch 20/72 | Loss: 4.8827 | Batch Acc: 0.2500\n",
            "Batch 21/72 | Loss: 4.9751 | Batch Acc: 0.3125\n",
            "Batch 22/72 | Loss: 6.0821 | Batch Acc: 0.1875\n",
            "Batch 23/72 | Loss: 5.2108 | Batch Acc: 0.1250\n",
            "Batch 24/72 | Loss: 6.4368 | Batch Acc: 0.1250\n",
            "Batch 25/72 | Loss: 5.0759 | Batch Acc: 0.2500\n",
            "Batch 26/72 | Loss: 5.2666 | Batch Acc: 0.3750\n",
            "Batch 27/72 | Loss: 4.4213 | Batch Acc: 0.2500\n",
            "Batch 28/72 | Loss: 5.6128 | Batch Acc: 0.1250\n",
            "Batch 29/72 | Loss: 5.7497 | Batch Acc: 0.2500\n",
            "Batch 30/72 | Loss: 5.7378 | Batch Acc: 0.0625\n",
            "Batch 31/72 | Loss: 4.8637 | Batch Acc: 0.3125\n",
            "Batch 32/72 | Loss: 4.8850 | Batch Acc: 0.2500\n",
            "Batch 33/72 | Loss: 5.6000 | Batch Acc: 0.3125\n",
            "Batch 34/72 | Loss: 6.6630 | Batch Acc: 0.1875\n",
            "Batch 35/72 | Loss: 6.8024 | Batch Acc: 0.0625\n",
            "Batch 36/72 | Loss: 5.9067 | Batch Acc: 0.3125\n",
            "Batch 37/72 | Loss: 5.9567 | Batch Acc: 0.1250\n",
            "Batch 38/72 | Loss: 5.4969 | Batch Acc: 0.3125\n",
            "Batch 39/72 | Loss: 4.9619 | Batch Acc: 0.2500\n",
            "Batch 40/72 | Loss: 4.7181 | Batch Acc: 0.3750\n",
            "Batch 41/72 | Loss: 4.9332 | Batch Acc: 0.1250\n",
            "Batch 42/72 | Loss: 6.4185 | Batch Acc: 0.0625\n",
            "Batch 43/72 | Loss: 5.3408 | Batch Acc: 0.1875\n",
            "Batch 44/72 | Loss: 5.8449 | Batch Acc: 0.1250\n",
            "Batch 45/72 | Loss: 5.6934 | Batch Acc: 0.1250\n",
            "Batch 46/72 | Loss: 5.0511 | Batch Acc: 0.1875\n",
            "Batch 47/72 | Loss: 5.7035 | Batch Acc: 0.1875\n",
            "Batch 48/72 | Loss: 5.6891 | Batch Acc: 0.2500\n",
            "Batch 49/72 | Loss: 6.0891 | Batch Acc: 0.1250\n",
            "Batch 50/72 | Loss: 5.3585 | Batch Acc: 0.1875\n",
            "Batch 51/72 | Loss: 5.4216 | Batch Acc: 0.3125\n",
            "Batch 52/72 | Loss: 6.0669 | Batch Acc: 0.3750\n",
            "Batch 53/72 | Loss: 5.8629 | Batch Acc: 0.2500\n",
            "Batch 54/72 | Loss: 5.4411 | Batch Acc: 0.1250\n",
            "Batch 55/72 | Loss: 5.6589 | Batch Acc: 0.1250\n",
            "Batch 56/72 | Loss: 5.6063 | Batch Acc: 0.4375\n",
            "Batch 57/72 | Loss: 6.5073 | Batch Acc: 0.1250\n",
            "Batch 58/72 | Loss: 5.6912 | Batch Acc: 0.3125\n",
            "Batch 59/72 | Loss: 5.2800 | Batch Acc: 0.3750\n",
            "Batch 60/72 | Loss: 5.4521 | Batch Acc: 0.1875\n",
            "Batch 61/72 | Loss: 5.7420 | Batch Acc: 0.1875\n",
            "Batch 62/72 | Loss: 6.0354 | Batch Acc: 0.1875\n",
            "Batch 63/72 | Loss: 5.4129 | Batch Acc: 0.3125\n",
            "Batch 64/72 | Loss: 5.8355 | Batch Acc: 0.1875\n",
            "Batch 65/72 | Loss: 5.7357 | Batch Acc: 0.1250\n",
            "Batch 66/72 | Loss: 6.5155 | Batch Acc: 0.1875\n",
            "Batch 67/72 | Loss: 6.0300 | Batch Acc: 0.3125\n",
            "Batch 68/72 | Loss: 5.9915 | Batch Acc: 0.1250\n",
            "Batch 69/72 | Loss: 4.5589 | Batch Acc: 0.2500\n",
            "Batch 70/72 | Loss: 6.8669 | Batch Acc: 0.1250\n",
            "Batch 71/72 | Loss: 6.1403 | Batch Acc: 0.0000\n",
            "Batch 72/72 | Loss: 5.4645 | Batch Acc: 0.1538\n",
            "Epoch Train Loss: 404.4639, Accuracy: 0.2106\n",
            "Validation Accuracy: 0.2614\n",
            "‚úÖ ÂÑ≤Â≠òÊúÄ‰Ω≥Ê®°ÂûãÔºàAcc: 0.2614Ôºâ\n",
            "\n",
            "Epoch 16/50\n",
            "Batch 1/72 | Loss: 5.7658 | Batch Acc: 0.2500\n",
            "Batch 2/72 | Loss: 5.6806 | Batch Acc: 0.3125\n",
            "Batch 3/72 | Loss: 5.1192 | Batch Acc: 0.0000\n",
            "Batch 4/72 | Loss: 5.9146 | Batch Acc: 0.1875\n",
            "Batch 5/72 | Loss: 5.4151 | Batch Acc: 0.2500\n",
            "Batch 6/72 | Loss: 4.9885 | Batch Acc: 0.2500\n",
            "Batch 7/72 | Loss: 5.8479 | Batch Acc: 0.2500\n",
            "Batch 8/72 | Loss: 7.3466 | Batch Acc: 0.1875\n",
            "Batch 9/72 | Loss: 5.7953 | Batch Acc: 0.1875\n",
            "Batch 10/72 | Loss: 5.3153 | Batch Acc: 0.1875\n",
            "Batch 11/72 | Loss: 5.8195 | Batch Acc: 0.3125\n",
            "Batch 12/72 | Loss: 5.6121 | Batch Acc: 0.1250\n",
            "Batch 13/72 | Loss: 5.1611 | Batch Acc: 0.0625\n",
            "Batch 14/72 | Loss: 6.0673 | Batch Acc: 0.2500\n",
            "Batch 15/72 | Loss: 5.5827 | Batch Acc: 0.2500\n",
            "Batch 16/72 | Loss: 5.1426 | Batch Acc: 0.2500\n",
            "Batch 17/72 | Loss: 4.6998 | Batch Acc: 0.3125\n",
            "Batch 18/72 | Loss: 6.2987 | Batch Acc: 0.2500\n",
            "Batch 19/72 | Loss: 4.2298 | Batch Acc: 0.4375\n",
            "Batch 20/72 | Loss: 5.9744 | Batch Acc: 0.1875\n",
            "Batch 21/72 | Loss: 6.4761 | Batch Acc: 0.1250\n",
            "Batch 22/72 | Loss: 5.9992 | Batch Acc: 0.1875\n",
            "Batch 23/72 | Loss: 5.0701 | Batch Acc: 0.3750\n",
            "Batch 24/72 | Loss: 5.4424 | Batch Acc: 0.3125\n",
            "Batch 25/72 | Loss: 4.9451 | Batch Acc: 0.2500\n",
            "Batch 26/72 | Loss: 5.0465 | Batch Acc: 0.2500\n",
            "Batch 27/72 | Loss: 5.6425 | Batch Acc: 0.1250\n",
            "Batch 28/72 | Loss: 5.3676 | Batch Acc: 0.3125\n",
            "Batch 29/72 | Loss: 5.4730 | Batch Acc: 0.4375\n",
            "Batch 30/72 | Loss: 6.0582 | Batch Acc: 0.1250\n",
            "Batch 31/72 | Loss: 5.3277 | Batch Acc: 0.2500\n",
            "Batch 32/72 | Loss: 5.2472 | Batch Acc: 0.3125\n",
            "Batch 33/72 | Loss: 5.0047 | Batch Acc: 0.3750\n",
            "Batch 34/72 | Loss: 5.9390 | Batch Acc: 0.1875\n",
            "Batch 35/72 | Loss: 5.9675 | Batch Acc: 0.3750\n",
            "Batch 36/72 | Loss: 5.8131 | Batch Acc: 0.1875\n",
            "Batch 37/72 | Loss: 5.2904 | Batch Acc: 0.3125\n",
            "Batch 38/72 | Loss: 5.5663 | Batch Acc: 0.1250\n",
            "Batch 39/72 | Loss: 7.0727 | Batch Acc: 0.1875\n",
            "Batch 40/72 | Loss: 5.6878 | Batch Acc: 0.1875\n",
            "Batch 41/72 | Loss: 4.9630 | Batch Acc: 0.2500\n",
            "Batch 42/72 | Loss: 5.4533 | Batch Acc: 0.1875\n",
            "Batch 43/72 | Loss: 5.2316 | Batch Acc: 0.4375\n",
            "Batch 44/72 | Loss: 6.3102 | Batch Acc: 0.2500\n",
            "Batch 45/72 | Loss: 6.9715 | Batch Acc: 0.0625\n",
            "Batch 46/72 | Loss: 6.0038 | Batch Acc: 0.0625\n",
            "Batch 47/72 | Loss: 6.2118 | Batch Acc: 0.1250\n",
            "Batch 48/72 | Loss: 5.1576 | Batch Acc: 0.1875\n",
            "Batch 49/72 | Loss: 5.2202 | Batch Acc: 0.3125\n",
            "Batch 50/72 | Loss: 5.1430 | Batch Acc: 0.2500\n",
            "Batch 51/72 | Loss: 5.0657 | Batch Acc: 0.3750\n",
            "Batch 52/72 | Loss: 5.5078 | Batch Acc: 0.1875\n",
            "Batch 53/72 | Loss: 5.7202 | Batch Acc: 0.2500\n",
            "Batch 54/72 | Loss: 6.7976 | Batch Acc: 0.1875\n",
            "Batch 55/72 | Loss: 5.3578 | Batch Acc: 0.2500\n",
            "Batch 56/72 | Loss: 5.2340 | Batch Acc: 0.3125\n",
            "Batch 57/72 | Loss: 5.2628 | Batch Acc: 0.1875\n",
            "Batch 58/72 | Loss: 5.5443 | Batch Acc: 0.0625\n",
            "Batch 59/72 | Loss: 5.5461 | Batch Acc: 0.1875\n",
            "Batch 60/72 | Loss: 4.8336 | Batch Acc: 0.3125\n",
            "Batch 61/72 | Loss: 4.9724 | Batch Acc: 0.3125\n",
            "Batch 62/72 | Loss: 5.6808 | Batch Acc: 0.0000\n",
            "Batch 63/72 | Loss: 5.2576 | Batch Acc: 0.3125\n",
            "Batch 64/72 | Loss: 4.9142 | Batch Acc: 0.1875\n",
            "Batch 65/72 | Loss: 5.0373 | Batch Acc: 0.3750\n",
            "Batch 66/72 | Loss: 5.0211 | Batch Acc: 0.3750\n",
            "Batch 67/72 | Loss: 5.5750 | Batch Acc: 0.1875\n",
            "Batch 68/72 | Loss: 5.8411 | Batch Acc: 0.1875\n",
            "Batch 69/72 | Loss: 5.0065 | Batch Acc: 0.2500\n",
            "Batch 70/72 | Loss: 5.9124 | Batch Acc: 0.2500\n",
            "Batch 71/72 | Loss: 6.5834 | Batch Acc: 0.2500\n",
            "Batch 72/72 | Loss: 5.6574 | Batch Acc: 0.2308\n",
            "Epoch Train Loss: 401.2087, Accuracy: 0.2341\n",
            "Validation Accuracy: 0.2841\n",
            "‚úÖ ÂÑ≤Â≠òÊúÄ‰Ω≥Ê®°ÂûãÔºàAcc: 0.2841Ôºâ\n",
            "\n",
            "Epoch 17/50\n",
            "Batch 1/72 | Loss: 5.1344 | Batch Acc: 0.2500\n",
            "Batch 2/72 | Loss: 5.6274 | Batch Acc: 0.2500\n",
            "Batch 3/72 | Loss: 4.6182 | Batch Acc: 0.3125\n",
            "Batch 4/72 | Loss: 5.1533 | Batch Acc: 0.2500\n",
            "Batch 5/72 | Loss: 6.3580 | Batch Acc: 0.3750\n",
            "Batch 6/72 | Loss: 6.1612 | Batch Acc: 0.0625\n",
            "Batch 7/72 | Loss: 5.8546 | Batch Acc: 0.0625\n",
            "Batch 8/72 | Loss: 5.2418 | Batch Acc: 0.2500\n",
            "Batch 9/72 | Loss: 5.1541 | Batch Acc: 0.3750\n",
            "Batch 10/72 | Loss: 4.9731 | Batch Acc: 0.3750\n",
            "Batch 11/72 | Loss: 5.9013 | Batch Acc: 0.0625\n",
            "Batch 12/72 | Loss: 5.3306 | Batch Acc: 0.1875\n",
            "Batch 13/72 | Loss: 5.8446 | Batch Acc: 0.1875\n",
            "Batch 14/72 | Loss: 5.1644 | Batch Acc: 0.1875\n",
            "Batch 15/72 | Loss: 5.4695 | Batch Acc: 0.3125\n",
            "Batch 16/72 | Loss: 5.9238 | Batch Acc: 0.0625\n",
            "Batch 17/72 | Loss: 5.6521 | Batch Acc: 0.1875\n",
            "Batch 18/72 | Loss: 4.5403 | Batch Acc: 0.3750\n",
            "Batch 19/72 | Loss: 4.5510 | Batch Acc: 0.3750\n",
            "Batch 20/72 | Loss: 5.3619 | Batch Acc: 0.1875\n",
            "Batch 21/72 | Loss: 5.0712 | Batch Acc: 0.1875\n",
            "Batch 22/72 | Loss: 4.0255 | Batch Acc: 0.4375\n",
            "Batch 23/72 | Loss: 6.1626 | Batch Acc: 0.1875\n",
            "Batch 24/72 | Loss: 5.6150 | Batch Acc: 0.2500\n",
            "Batch 25/72 | Loss: 5.8602 | Batch Acc: 0.0625\n",
            "Batch 26/72 | Loss: 5.0754 | Batch Acc: 0.5000\n",
            "Batch 27/72 | Loss: 5.3727 | Batch Acc: 0.3125\n",
            "Batch 28/72 | Loss: 7.2456 | Batch Acc: 0.3125\n",
            "Batch 29/72 | Loss: 6.0792 | Batch Acc: 0.1875\n",
            "Batch 30/72 | Loss: 6.1987 | Batch Acc: 0.1250\n",
            "Batch 31/72 | Loss: 5.9496 | Batch Acc: 0.1875\n",
            "Batch 32/72 | Loss: 4.8495 | Batch Acc: 0.3750\n",
            "Batch 33/72 | Loss: 4.3649 | Batch Acc: 0.3125\n",
            "Batch 34/72 | Loss: 5.7881 | Batch Acc: 0.2500\n",
            "Batch 35/72 | Loss: 5.8400 | Batch Acc: 0.1250\n",
            "Batch 36/72 | Loss: 5.4623 | Batch Acc: 0.1250\n",
            "Batch 37/72 | Loss: 5.9261 | Batch Acc: 0.1250\n",
            "Batch 38/72 | Loss: 5.8306 | Batch Acc: 0.3125\n",
            "Batch 39/72 | Loss: 5.3858 | Batch Acc: 0.1250\n",
            "Batch 40/72 | Loss: 3.8240 | Batch Acc: 0.5000\n",
            "Batch 41/72 | Loss: 5.1492 | Batch Acc: 0.1875\n",
            "Batch 42/72 | Loss: 5.1580 | Batch Acc: 0.3750\n",
            "Batch 43/72 | Loss: 5.2473 | Batch Acc: 0.2500\n",
            "Batch 44/72 | Loss: 5.6955 | Batch Acc: 0.4375\n",
            "Batch 45/72 | Loss: 4.2801 | Batch Acc: 0.3125\n",
            "Batch 46/72 | Loss: 5.8980 | Batch Acc: 0.1250\n",
            "Batch 47/72 | Loss: 5.7320 | Batch Acc: 0.1875\n",
            "Batch 48/72 | Loss: 5.4709 | Batch Acc: 0.4375\n",
            "Batch 49/72 | Loss: 4.9954 | Batch Acc: 0.1875\n",
            "Batch 50/72 | Loss: 5.1017 | Batch Acc: 0.2500\n",
            "Batch 51/72 | Loss: 6.0453 | Batch Acc: 0.2500\n",
            "Batch 52/72 | Loss: 5.0965 | Batch Acc: 0.1250\n",
            "Batch 53/72 | Loss: 5.3897 | Batch Acc: 0.1250\n",
            "Batch 54/72 | Loss: 6.0900 | Batch Acc: 0.1875\n",
            "Batch 55/72 | Loss: 5.6841 | Batch Acc: 0.3125\n",
            "Batch 56/72 | Loss: 4.9044 | Batch Acc: 0.3125\n",
            "Batch 57/72 | Loss: 6.2903 | Batch Acc: 0.0625\n",
            "Batch 58/72 | Loss: 6.2120 | Batch Acc: 0.1250\n",
            "Batch 59/72 | Loss: 5.9433 | Batch Acc: 0.1875\n",
            "Batch 60/72 | Loss: 4.4129 | Batch Acc: 0.3125\n",
            "Batch 61/72 | Loss: 5.5594 | Batch Acc: 0.1250\n",
            "Batch 62/72 | Loss: 5.7893 | Batch Acc: 0.2500\n",
            "Batch 63/72 | Loss: 4.4587 | Batch Acc: 0.2500\n",
            "Batch 64/72 | Loss: 5.5523 | Batch Acc: 0.1875\n",
            "Batch 65/72 | Loss: 4.5171 | Batch Acc: 0.3750\n",
            "Batch 66/72 | Loss: 6.5044 | Batch Acc: 0.0625\n",
            "Batch 67/72 | Loss: 4.3074 | Batch Acc: 0.3750\n",
            "Batch 68/72 | Loss: 5.2228 | Batch Acc: 0.2500\n",
            "Batch 69/72 | Loss: 4.4605 | Batch Acc: 0.3750\n",
            "Batch 70/72 | Loss: 5.8103 | Batch Acc: 0.2500\n",
            "Batch 71/72 | Loss: 4.9693 | Batch Acc: 0.1875\n",
            "Batch 72/72 | Loss: 5.5794 | Batch Acc: 0.2308\n",
            "Epoch Train Loss: 389.4700, Accuracy: 0.2411\n",
            "Validation Accuracy: 0.2898\n",
            "‚úÖ ÂÑ≤Â≠òÊúÄ‰Ω≥Ê®°ÂûãÔºàAcc: 0.2898Ôºâ\n",
            "\n",
            "Epoch 18/50\n",
            "Batch 1/72 | Loss: 4.5565 | Batch Acc: 0.3125\n",
            "Batch 2/72 | Loss: 6.2328 | Batch Acc: 0.1875\n",
            "Batch 3/72 | Loss: 6.0621 | Batch Acc: 0.2500\n",
            "Batch 4/72 | Loss: 6.8750 | Batch Acc: 0.0625\n",
            "Batch 5/72 | Loss: 5.5312 | Batch Acc: 0.2500\n",
            "Batch 6/72 | Loss: 7.4177 | Batch Acc: 0.0625\n",
            "Batch 7/72 | Loss: 6.2260 | Batch Acc: 0.1875\n",
            "Batch 8/72 | Loss: 5.5531 | Batch Acc: 0.1875\n",
            "Batch 9/72 | Loss: 5.2624 | Batch Acc: 0.1250\n",
            "Batch 10/72 | Loss: 5.0959 | Batch Acc: 0.2500\n",
            "Batch 11/72 | Loss: 6.2139 | Batch Acc: 0.1875\n",
            "Batch 12/72 | Loss: 6.3831 | Batch Acc: 0.1875\n",
            "Batch 13/72 | Loss: 6.3615 | Batch Acc: 0.1875\n",
            "Batch 14/72 | Loss: 5.1293 | Batch Acc: 0.2500\n",
            "Batch 15/72 | Loss: 6.0059 | Batch Acc: 0.1250\n",
            "Batch 16/72 | Loss: 5.3892 | Batch Acc: 0.1875\n",
            "Batch 17/72 | Loss: 6.2203 | Batch Acc: 0.1875\n",
            "Batch 18/72 | Loss: 5.7608 | Batch Acc: 0.0625\n",
            "Batch 19/72 | Loss: 4.4499 | Batch Acc: 0.2500\n",
            "Batch 20/72 | Loss: 5.4484 | Batch Acc: 0.2500\n",
            "Batch 21/72 | Loss: 4.6869 | Batch Acc: 0.3750\n",
            "Batch 22/72 | Loss: 5.1531 | Batch Acc: 0.2500\n",
            "Batch 23/72 | Loss: 4.7483 | Batch Acc: 0.2500\n",
            "Batch 24/72 | Loss: 5.2658 | Batch Acc: 0.2500\n",
            "Batch 25/72 | Loss: 5.6082 | Batch Acc: 0.2500\n",
            "Batch 26/72 | Loss: 5.3593 | Batch Acc: 0.3125\n",
            "Batch 27/72 | Loss: 6.8676 | Batch Acc: 0.1875\n",
            "Batch 28/72 | Loss: 5.8161 | Batch Acc: 0.1250\n",
            "Batch 29/72 | Loss: 5.4715 | Batch Acc: 0.1250\n",
            "Batch 30/72 | Loss: 4.1225 | Batch Acc: 0.2500\n",
            "Batch 31/72 | Loss: 4.0694 | Batch Acc: 0.3750\n",
            "Batch 32/72 | Loss: 4.4811 | Batch Acc: 0.2500\n",
            "Batch 33/72 | Loss: 5.6969 | Batch Acc: 0.0000\n",
            "Batch 34/72 | Loss: 6.6073 | Batch Acc: 0.1250\n",
            "Batch 35/72 | Loss: 4.5522 | Batch Acc: 0.1875\n",
            "Batch 36/72 | Loss: 5.5521 | Batch Acc: 0.2500\n",
            "Batch 37/72 | Loss: 5.8328 | Batch Acc: 0.2500\n",
            "Batch 38/72 | Loss: 5.5668 | Batch Acc: 0.3750\n",
            "Batch 39/72 | Loss: 4.8168 | Batch Acc: 0.3125\n",
            "Batch 40/72 | Loss: 6.2315 | Batch Acc: 0.1250\n",
            "Batch 41/72 | Loss: 4.7987 | Batch Acc: 0.3750\n",
            "Batch 42/72 | Loss: 6.3948 | Batch Acc: 0.1875\n",
            "Batch 43/72 | Loss: 5.4690 | Batch Acc: 0.2500\n",
            "Batch 44/72 | Loss: 6.4572 | Batch Acc: 0.1250\n",
            "Batch 45/72 | Loss: 6.3539 | Batch Acc: 0.1875\n",
            "Batch 46/72 | Loss: 4.8842 | Batch Acc: 0.2500\n",
            "Batch 47/72 | Loss: 5.2728 | Batch Acc: 0.2500\n",
            "Batch 48/72 | Loss: 5.9078 | Batch Acc: 0.1250\n",
            "Batch 49/72 | Loss: 5.9006 | Batch Acc: 0.1875\n",
            "Batch 50/72 | Loss: 5.5333 | Batch Acc: 0.1875\n",
            "Batch 51/72 | Loss: 5.5166 | Batch Acc: 0.3125\n",
            "Batch 52/72 | Loss: 5.5448 | Batch Acc: 0.2500\n",
            "Batch 53/72 | Loss: 4.1591 | Batch Acc: 0.4375\n",
            "Batch 54/72 | Loss: 5.5974 | Batch Acc: 0.1875\n",
            "Batch 55/72 | Loss: 5.1956 | Batch Acc: 0.1875\n",
            "Batch 56/72 | Loss: 5.1778 | Batch Acc: 0.1875\n",
            "Batch 57/72 | Loss: 5.6444 | Batch Acc: 0.3125\n",
            "Batch 58/72 | Loss: 5.6454 | Batch Acc: 0.2500\n",
            "Batch 59/72 | Loss: 4.6404 | Batch Acc: 0.3125\n",
            "Batch 60/72 | Loss: 7.1507 | Batch Acc: 0.1250\n",
            "Batch 61/72 | Loss: 4.5492 | Batch Acc: 0.3125\n",
            "Batch 62/72 | Loss: 5.3565 | Batch Acc: 0.3750\n",
            "Batch 63/72 | Loss: 5.1976 | Batch Acc: 0.2500\n",
            "Batch 64/72 | Loss: 4.8864 | Batch Acc: 0.3125\n",
            "Batch 65/72 | Loss: 5.1829 | Batch Acc: 0.3750\n",
            "Batch 66/72 | Loss: 5.7555 | Batch Acc: 0.2500\n",
            "Batch 67/72 | Loss: 6.2995 | Batch Acc: 0.1250\n",
            "Batch 68/72 | Loss: 5.1958 | Batch Acc: 0.2500\n",
            "Batch 69/72 | Loss: 4.9242 | Batch Acc: 0.2500\n",
            "Batch 70/72 | Loss: 6.8101 | Batch Acc: 0.1875\n",
            "Batch 71/72 | Loss: 4.1905 | Batch Acc: 0.4375\n",
            "Batch 72/72 | Loss: 6.0714 | Batch Acc: 0.1538\n",
            "Epoch Train Loss: 398.3450, Accuracy: 0.2272\n",
            "Validation Accuracy: 0.2841\n",
            "\n",
            "Epoch 19/50\n",
            "Batch 1/72 | Loss: 5.6669 | Batch Acc: 0.0625\n",
            "Batch 2/72 | Loss: 5.4083 | Batch Acc: 0.3750\n",
            "Batch 3/72 | Loss: 5.5628 | Batch Acc: 0.3750\n",
            "Batch 4/72 | Loss: 5.0947 | Batch Acc: 0.2500\n",
            "Batch 5/72 | Loss: 5.3460 | Batch Acc: 0.2500\n",
            "Batch 6/72 | Loss: 5.5301 | Batch Acc: 0.3125\n",
            "Batch 7/72 | Loss: 4.3593 | Batch Acc: 0.4375\n",
            "Batch 8/72 | Loss: 5.3831 | Batch Acc: 0.0625\n",
            "Batch 9/72 | Loss: 6.5945 | Batch Acc: 0.1875\n",
            "Batch 10/72 | Loss: 4.9818 | Batch Acc: 0.1875\n",
            "Batch 11/72 | Loss: 6.0030 | Batch Acc: 0.1875\n",
            "Batch 12/72 | Loss: 5.2207 | Batch Acc: 0.1250\n",
            "Batch 13/72 | Loss: 5.6332 | Batch Acc: 0.1875\n",
            "Batch 14/72 | Loss: 5.2967 | Batch Acc: 0.1875\n",
            "Batch 15/72 | Loss: 5.4703 | Batch Acc: 0.2500\n",
            "Batch 16/72 | Loss: 5.9014 | Batch Acc: 0.1875\n",
            "Batch 17/72 | Loss: 4.9713 | Batch Acc: 0.3125\n",
            "Batch 18/72 | Loss: 5.9976 | Batch Acc: 0.1250\n",
            "Batch 19/72 | Loss: 5.2516 | Batch Acc: 0.2500\n",
            "Batch 20/72 | Loss: 5.8413 | Batch Acc: 0.1250\n",
            "Batch 21/72 | Loss: 4.6382 | Batch Acc: 0.3750\n",
            "Batch 22/72 | Loss: 6.3884 | Batch Acc: 0.1875\n",
            "Batch 23/72 | Loss: 5.7227 | Batch Acc: 0.2500\n",
            "Batch 24/72 | Loss: 4.9292 | Batch Acc: 0.3125\n",
            "Batch 25/72 | Loss: 5.7196 | Batch Acc: 0.1875\n",
            "Batch 26/72 | Loss: 6.4203 | Batch Acc: 0.3125\n",
            "Batch 27/72 | Loss: 5.2604 | Batch Acc: 0.3125\n",
            "Batch 28/72 | Loss: 5.2137 | Batch Acc: 0.3125\n",
            "Batch 29/72 | Loss: 5.3692 | Batch Acc: 0.1250\n",
            "Batch 30/72 | Loss: 4.9872 | Batch Acc: 0.1250\n",
            "Batch 31/72 | Loss: 5.3704 | Batch Acc: 0.3125\n",
            "Batch 32/72 | Loss: 5.8720 | Batch Acc: 0.2500\n",
            "Batch 33/72 | Loss: 5.6722 | Batch Acc: 0.1250\n",
            "Batch 34/72 | Loss: 5.9797 | Batch Acc: 0.2500\n",
            "Batch 35/72 | Loss: 4.5960 | Batch Acc: 0.3750\n",
            "Batch 36/72 | Loss: 4.1012 | Batch Acc: 0.3125\n",
            "Batch 37/72 | Loss: 5.4972 | Batch Acc: 0.1250\n",
            "Batch 38/72 | Loss: 3.5875 | Batch Acc: 0.3750\n",
            "Batch 39/72 | Loss: 5.1783 | Batch Acc: 0.2500\n",
            "Batch 40/72 | Loss: 5.5865 | Batch Acc: 0.2500\n",
            "Batch 41/72 | Loss: 5.4480 | Batch Acc: 0.3125\n",
            "Batch 42/72 | Loss: 4.2915 | Batch Acc: 0.3750\n",
            "Batch 43/72 | Loss: 4.6299 | Batch Acc: 0.3750\n",
            "Batch 44/72 | Loss: 6.3754 | Batch Acc: 0.1875\n",
            "Batch 45/72 | Loss: 5.5611 | Batch Acc: 0.1250\n",
            "Batch 46/72 | Loss: 4.5743 | Batch Acc: 0.3125\n",
            "Batch 47/72 | Loss: 6.0169 | Batch Acc: 0.3750\n",
            "Batch 48/72 | Loss: 6.0524 | Batch Acc: 0.1250\n",
            "Batch 49/72 | Loss: 4.8238 | Batch Acc: 0.3125\n",
            "Batch 50/72 | Loss: 5.4875 | Batch Acc: 0.1250\n",
            "Batch 51/72 | Loss: 5.6746 | Batch Acc: 0.3750\n",
            "Batch 52/72 | Loss: 5.2395 | Batch Acc: 0.3125\n",
            "Batch 53/72 | Loss: 5.6572 | Batch Acc: 0.0625\n",
            "Batch 54/72 | Loss: 5.1598 | Batch Acc: 0.3125\n",
            "Batch 55/72 | Loss: 5.7403 | Batch Acc: 0.1875\n",
            "Batch 56/72 | Loss: 5.1486 | Batch Acc: 0.2500\n",
            "Batch 57/72 | Loss: 5.9258 | Batch Acc: 0.2500\n",
            "Batch 58/72 | Loss: 5.8680 | Batch Acc: 0.1250\n",
            "Batch 59/72 | Loss: 4.9109 | Batch Acc: 0.2500\n",
            "Batch 60/72 | Loss: 4.3838 | Batch Acc: 0.3125\n",
            "Batch 61/72 | Loss: 5.5117 | Batch Acc: 0.0625\n",
            "Batch 62/72 | Loss: 5.3240 | Batch Acc: 0.2500\n",
            "Batch 63/72 | Loss: 4.9904 | Batch Acc: 0.1875\n",
            "Batch 64/72 | Loss: 5.0520 | Batch Acc: 0.1250\n",
            "Batch 65/72 | Loss: 6.0803 | Batch Acc: 0.2500\n",
            "Batch 66/72 | Loss: 5.9339 | Batch Acc: 0.1250\n",
            "Batch 67/72 | Loss: 4.6651 | Batch Acc: 0.3750\n",
            "Batch 68/72 | Loss: 4.8124 | Batch Acc: 0.3750\n",
            "Batch 69/72 | Loss: 6.4397 | Batch Acc: 0.0625\n",
            "Batch 70/72 | Loss: 6.1314 | Batch Acc: 0.1875\n",
            "Batch 71/72 | Loss: 4.9932 | Batch Acc: 0.3750\n",
            "Batch 72/72 | Loss: 5.0165 | Batch Acc: 0.0769\n",
            "Epoch Train Loss: 387.5243, Accuracy: 0.2376\n",
            "Validation Accuracy: 0.2955\n",
            "‚úÖ ÂÑ≤Â≠òÊúÄ‰Ω≥Ê®°ÂûãÔºàAcc: 0.2955Ôºâ\n",
            "\n",
            "Epoch 20/50\n",
            "Batch 1/72 | Loss: 5.9142 | Batch Acc: 0.0625\n",
            "Batch 2/72 | Loss: 4.7230 | Batch Acc: 0.2500\n",
            "Batch 3/72 | Loss: 5.6089 | Batch Acc: 0.1875\n",
            "Batch 4/72 | Loss: 4.4630 | Batch Acc: 0.3750\n",
            "Batch 5/72 | Loss: 5.9689 | Batch Acc: 0.3750\n",
            "Batch 6/72 | Loss: 5.4766 | Batch Acc: 0.3125\n",
            "Batch 7/72 | Loss: 4.9408 | Batch Acc: 0.3750\n",
            "Batch 8/72 | Loss: 5.8671 | Batch Acc: 0.3125\n",
            "Batch 9/72 | Loss: 5.7926 | Batch Acc: 0.1250\n",
            "Batch 10/72 | Loss: 5.9424 | Batch Acc: 0.1875\n",
            "Batch 11/72 | Loss: 4.0786 | Batch Acc: 0.2500\n",
            "Batch 12/72 | Loss: 5.0243 | Batch Acc: 0.3125\n",
            "Batch 13/72 | Loss: 4.7177 | Batch Acc: 0.3125\n",
            "Batch 14/72 | Loss: 7.0017 | Batch Acc: 0.1875\n",
            "Batch 15/72 | Loss: 4.9066 | Batch Acc: 0.2500\n",
            "Batch 16/72 | Loss: 4.2690 | Batch Acc: 0.3125\n",
            "Batch 17/72 | Loss: 6.2807 | Batch Acc: 0.0625\n",
            "Batch 18/72 | Loss: 5.3100 | Batch Acc: 0.1875\n",
            "Batch 19/72 | Loss: 6.5919 | Batch Acc: 0.2500\n",
            "Batch 20/72 | Loss: 5.9152 | Batch Acc: 0.2500\n",
            "Batch 21/72 | Loss: 5.0467 | Batch Acc: 0.2500\n",
            "Batch 22/72 | Loss: 4.9099 | Batch Acc: 0.2500\n",
            "Batch 23/72 | Loss: 5.4199 | Batch Acc: 0.3750\n",
            "Batch 24/72 | Loss: 5.1027 | Batch Acc: 0.3125\n",
            "Batch 25/72 | Loss: 5.9237 | Batch Acc: 0.1875\n",
            "Batch 26/72 | Loss: 4.9811 | Batch Acc: 0.3125\n",
            "Batch 27/72 | Loss: 5.4476 | Batch Acc: 0.0625\n",
            "Batch 28/72 | Loss: 5.6645 | Batch Acc: 0.1250\n",
            "Batch 29/72 | Loss: 4.1629 | Batch Acc: 0.3125\n",
            "Batch 30/72 | Loss: 5.0448 | Batch Acc: 0.4375\n",
            "Batch 31/72 | Loss: 4.3752 | Batch Acc: 0.5000\n",
            "Batch 32/72 | Loss: 6.3559 | Batch Acc: 0.1875\n",
            "Batch 33/72 | Loss: 4.8592 | Batch Acc: 0.4375\n",
            "Batch 34/72 | Loss: 4.9689 | Batch Acc: 0.5000\n",
            "Batch 35/72 | Loss: 6.2009 | Batch Acc: 0.1875\n",
            "Batch 36/72 | Loss: 4.3916 | Batch Acc: 0.2500\n",
            "Batch 37/72 | Loss: 5.9673 | Batch Acc: 0.1250\n",
            "Batch 38/72 | Loss: 5.2751 | Batch Acc: 0.0625\n",
            "Batch 39/72 | Loss: 6.4632 | Batch Acc: 0.1875\n",
            "Batch 40/72 | Loss: 4.8668 | Batch Acc: 0.4375\n",
            "Batch 41/72 | Loss: 5.2461 | Batch Acc: 0.1875\n",
            "Batch 42/72 | Loss: 4.9949 | Batch Acc: 0.1875\n",
            "Batch 43/72 | Loss: 4.3288 | Batch Acc: 0.3750\n",
            "Batch 44/72 | Loss: 5.6270 | Batch Acc: 0.1250\n",
            "Batch 45/72 | Loss: 4.6237 | Batch Acc: 0.1875\n",
            "Batch 46/72 | Loss: 4.9589 | Batch Acc: 0.3125\n",
            "Batch 47/72 | Loss: 5.3796 | Batch Acc: 0.1875\n",
            "Batch 48/72 | Loss: 6.3206 | Batch Acc: 0.0625\n",
            "Batch 49/72 | Loss: 5.4510 | Batch Acc: 0.2500\n",
            "Batch 50/72 | Loss: 7.7847 | Batch Acc: 0.0000\n",
            "Batch 51/72 | Loss: 5.1428 | Batch Acc: 0.1875\n",
            "Batch 52/72 | Loss: 4.2273 | Batch Acc: 0.3750\n",
            "Batch 53/72 | Loss: 4.0223 | Batch Acc: 0.4375\n",
            "Batch 54/72 | Loss: 4.7102 | Batch Acc: 0.4375\n",
            "Batch 55/72 | Loss: 5.2921 | Batch Acc: 0.1250\n",
            "Batch 56/72 | Loss: 4.8404 | Batch Acc: 0.3750\n",
            "Batch 57/72 | Loss: 5.7907 | Batch Acc: 0.2500\n",
            "Batch 58/72 | Loss: 6.2363 | Batch Acc: 0.0625\n",
            "Batch 59/72 | Loss: 5.6480 | Batch Acc: 0.1875\n",
            "Batch 60/72 | Loss: 5.5978 | Batch Acc: 0.2500\n",
            "Batch 61/72 | Loss: 5.1013 | Batch Acc: 0.3125\n",
            "Batch 62/72 | Loss: 6.4810 | Batch Acc: 0.2500\n",
            "Batch 63/72 | Loss: 5.3357 | Batch Acc: 0.1875\n",
            "Batch 64/72 | Loss: 6.5488 | Batch Acc: 0.1250\n",
            "Batch 65/72 | Loss: 4.9725 | Batch Acc: 0.3125\n",
            "Batch 66/72 | Loss: 5.4394 | Batch Acc: 0.1875\n",
            "Batch 67/72 | Loss: 5.9113 | Batch Acc: 0.1250\n",
            "Batch 68/72 | Loss: 5.3017 | Batch Acc: 0.1875\n",
            "Batch 69/72 | Loss: 4.8542 | Batch Acc: 0.3750\n",
            "Batch 70/72 | Loss: 4.8655 | Batch Acc: 0.4375\n",
            "Batch 71/72 | Loss: 5.9402 | Batch Acc: 0.1250\n",
            "Batch 72/72 | Loss: 6.0103 | Batch Acc: 0.3846\n",
            "Epoch Train Loss: 387.2065, Accuracy: 0.2498\n",
            "Validation Accuracy: 0.2784\n",
            "\n",
            "Epoch 21/50\n",
            "Batch 1/72 | Loss: 4.6831 | Batch Acc: 0.2500\n",
            "Batch 2/72 | Loss: 5.0347 | Batch Acc: 0.2500\n",
            "Batch 3/72 | Loss: 5.2336 | Batch Acc: 0.1250\n",
            "Batch 4/72 | Loss: 5.5346 | Batch Acc: 0.1250\n",
            "Batch 5/72 | Loss: 5.6117 | Batch Acc: 0.3125\n",
            "Batch 6/72 | Loss: 5.5025 | Batch Acc: 0.0625\n",
            "Batch 7/72 | Loss: 4.9428 | Batch Acc: 0.3125\n",
            "Batch 8/72 | Loss: 4.9210 | Batch Acc: 0.3750\n",
            "Batch 9/72 | Loss: 5.8692 | Batch Acc: 0.1250\n",
            "Batch 10/72 | Loss: 5.4363 | Batch Acc: 0.2500\n",
            "Batch 11/72 | Loss: 5.5667 | Batch Acc: 0.1250\n",
            "Batch 12/72 | Loss: 6.0704 | Batch Acc: 0.0625\n",
            "Batch 13/72 | Loss: 3.9508 | Batch Acc: 0.3125\n",
            "Batch 14/72 | Loss: 5.3520 | Batch Acc: 0.2500\n",
            "Batch 15/72 | Loss: 5.0681 | Batch Acc: 0.1875\n",
            "Batch 16/72 | Loss: 4.4515 | Batch Acc: 0.1875\n",
            "Batch 17/72 | Loss: 4.4629 | Batch Acc: 0.3750\n",
            "Batch 18/72 | Loss: 7.0470 | Batch Acc: 0.0000\n",
            "Batch 19/72 | Loss: 4.2006 | Batch Acc: 0.3125\n",
            "Batch 20/72 | Loss: 5.0480 | Batch Acc: 0.1250\n",
            "Batch 21/72 | Loss: 6.7898 | Batch Acc: 0.3125\n",
            "Batch 22/72 | Loss: 5.2514 | Batch Acc: 0.3125\n",
            "Batch 23/72 | Loss: 5.2323 | Batch Acc: 0.1875\n",
            "Batch 24/72 | Loss: 5.8496 | Batch Acc: 0.2500\n",
            "Batch 25/72 | Loss: 7.6729 | Batch Acc: 0.1250\n",
            "Batch 26/72 | Loss: 6.3953 | Batch Acc: 0.1875\n",
            "Batch 27/72 | Loss: 4.7488 | Batch Acc: 0.1875\n",
            "Batch 28/72 | Loss: 5.7188 | Batch Acc: 0.2500\n",
            "Batch 29/72 | Loss: 5.1540 | Batch Acc: 0.5000\n",
            "Batch 30/72 | Loss: 4.1567 | Batch Acc: 0.3125\n",
            "Batch 31/72 | Loss: 4.9814 | Batch Acc: 0.2500\n",
            "Batch 32/72 | Loss: 4.3021 | Batch Acc: 0.3750\n",
            "Batch 33/72 | Loss: 5.4904 | Batch Acc: 0.1875\n",
            "Batch 34/72 | Loss: 4.9792 | Batch Acc: 0.3125\n",
            "Batch 35/72 | Loss: 6.0237 | Batch Acc: 0.1250\n",
            "Batch 36/72 | Loss: 6.1967 | Batch Acc: 0.2500\n",
            "Batch 37/72 | Loss: 4.9535 | Batch Acc: 0.2500\n",
            "Batch 38/72 | Loss: 5.7345 | Batch Acc: 0.1875\n",
            "Batch 39/72 | Loss: 4.7162 | Batch Acc: 0.3125\n",
            "Batch 40/72 | Loss: 5.7931 | Batch Acc: 0.0625\n",
            "Batch 41/72 | Loss: 5.0725 | Batch Acc: 0.2500\n",
            "Batch 42/72 | Loss: 4.6486 | Batch Acc: 0.3125\n",
            "Batch 43/72 | Loss: 5.5004 | Batch Acc: 0.0625\n",
            "Batch 44/72 | Loss: 5.7343 | Batch Acc: 0.1250\n",
            "Batch 45/72 | Loss: 4.7886 | Batch Acc: 0.4375\n",
            "Batch 46/72 | Loss: 5.1440 | Batch Acc: 0.0625\n",
            "Batch 47/72 | Loss: 5.4461 | Batch Acc: 0.2500\n",
            "Batch 48/72 | Loss: 5.9384 | Batch Acc: 0.1250\n",
            "Batch 49/72 | Loss: 5.3349 | Batch Acc: 0.2500\n",
            "Batch 50/72 | Loss: 5.0700 | Batch Acc: 0.3750\n",
            "Batch 51/72 | Loss: 5.1817 | Batch Acc: 0.2500\n",
            "Batch 52/72 | Loss: 6.2932 | Batch Acc: 0.1250\n",
            "Batch 53/72 | Loss: 5.7759 | Batch Acc: 0.2500\n",
            "Batch 54/72 | Loss: 6.2059 | Batch Acc: 0.3125\n",
            "Batch 55/72 | Loss: 6.9791 | Batch Acc: 0.1250\n",
            "Batch 56/72 | Loss: 7.0256 | Batch Acc: 0.2500\n",
            "Batch 57/72 | Loss: 5.9094 | Batch Acc: 0.2500\n",
            "Batch 58/72 | Loss: 5.7295 | Batch Acc: 0.3125\n",
            "Batch 59/72 | Loss: 5.6441 | Batch Acc: 0.2500\n",
            "Batch 60/72 | Loss: 6.6558 | Batch Acc: 0.1875\n",
            "Batch 61/72 | Loss: 4.1062 | Batch Acc: 0.4375\n",
            "Batch 62/72 | Loss: 6.7465 | Batch Acc: 0.1875\n",
            "Batch 63/72 | Loss: 5.5800 | Batch Acc: 0.1250\n",
            "Batch 64/72 | Loss: 5.5157 | Batch Acc: 0.0000\n",
            "Batch 65/72 | Loss: 6.1923 | Batch Acc: 0.2500\n",
            "Batch 66/72 | Loss: 5.4170 | Batch Acc: 0.2500\n",
            "Batch 67/72 | Loss: 5.2201 | Batch Acc: 0.2500\n",
            "Batch 68/72 | Loss: 5.8415 | Batch Acc: 0.3125\n",
            "Batch 69/72 | Loss: 4.9186 | Batch Acc: 0.3125\n",
            "Batch 70/72 | Loss: 4.5262 | Batch Acc: 0.3750\n",
            "Batch 71/72 | Loss: 5.4196 | Batch Acc: 0.3125\n",
            "Batch 72/72 | Loss: 4.6517 | Batch Acc: 0.3846\n",
            "Epoch Train Loss: 392.3414, Accuracy: 0.2332\n",
            "Validation Accuracy: 0.3068\n",
            "‚úÖ ÂÑ≤Â≠òÊúÄ‰Ω≥Ê®°ÂûãÔºàAcc: 0.3068Ôºâ\n",
            "\n",
            "Epoch 22/50\n",
            "Batch 1/72 | Loss: 5.4548 | Batch Acc: 0.2500\n",
            "Batch 2/72 | Loss: 5.0960 | Batch Acc: 0.2500\n",
            "Batch 3/72 | Loss: 4.9605 | Batch Acc: 0.2500\n",
            "Batch 4/72 | Loss: 5.1185 | Batch Acc: 0.3750\n",
            "Batch 5/72 | Loss: 4.7369 | Batch Acc: 0.2500\n",
            "Batch 6/72 | Loss: 4.4576 | Batch Acc: 0.2500\n",
            "Batch 7/72 | Loss: 4.8254 | Batch Acc: 0.2500\n",
            "Batch 8/72 | Loss: 5.2506 | Batch Acc: 0.3125\n",
            "Batch 9/72 | Loss: 5.5653 | Batch Acc: 0.1875\n",
            "Batch 10/72 | Loss: 4.5670 | Batch Acc: 0.3125\n",
            "Batch 11/72 | Loss: 6.7443 | Batch Acc: 0.3125\n",
            "Batch 12/72 | Loss: 4.3429 | Batch Acc: 0.4375\n",
            "Batch 13/72 | Loss: 4.6779 | Batch Acc: 0.3125\n",
            "Batch 14/72 | Loss: 4.4284 | Batch Acc: 0.3125\n",
            "Batch 15/72 | Loss: 5.5493 | Batch Acc: 0.1875\n",
            "Batch 16/72 | Loss: 5.6308 | Batch Acc: 0.2500\n",
            "Batch 17/72 | Loss: 5.6025 | Batch Acc: 0.3125\n",
            "Batch 18/72 | Loss: 4.5271 | Batch Acc: 0.4375\n",
            "Batch 19/72 | Loss: 4.3172 | Batch Acc: 0.3750\n",
            "Batch 20/72 | Loss: 5.6890 | Batch Acc: 0.1250\n",
            "Batch 21/72 | Loss: 6.2057 | Batch Acc: 0.1875\n",
            "Batch 22/72 | Loss: 4.5019 | Batch Acc: 0.3750\n",
            "Batch 23/72 | Loss: 4.9191 | Batch Acc: 0.3125\n",
            "Batch 24/72 | Loss: 5.1603 | Batch Acc: 0.1875\n",
            "Batch 25/72 | Loss: 4.8379 | Batch Acc: 0.1250\n",
            "Batch 26/72 | Loss: 5.9819 | Batch Acc: 0.1250\n",
            "Batch 27/72 | Loss: 4.1526 | Batch Acc: 0.4375\n",
            "Batch 28/72 | Loss: 5.7594 | Batch Acc: 0.3750\n",
            "Batch 29/72 | Loss: 5.4400 | Batch Acc: 0.1875\n",
            "Batch 30/72 | Loss: 6.2247 | Batch Acc: 0.1875\n",
            "Batch 31/72 | Loss: 4.9531 | Batch Acc: 0.2500\n",
            "Batch 32/72 | Loss: 6.3344 | Batch Acc: 0.0000\n",
            "Batch 33/72 | Loss: 5.3027 | Batch Acc: 0.3125\n",
            "Batch 34/72 | Loss: 5.0647 | Batch Acc: 0.3125\n",
            "Batch 35/72 | Loss: 4.8615 | Batch Acc: 0.3750\n",
            "Batch 36/72 | Loss: 4.7733 | Batch Acc: 0.3125\n",
            "Batch 37/72 | Loss: 6.3091 | Batch Acc: 0.1250\n",
            "Batch 38/72 | Loss: 4.8433 | Batch Acc: 0.2500\n",
            "Batch 39/72 | Loss: 5.5390 | Batch Acc: 0.1250\n",
            "Batch 40/72 | Loss: 5.1126 | Batch Acc: 0.3125\n",
            "Batch 41/72 | Loss: 4.9105 | Batch Acc: 0.3125\n",
            "Batch 42/72 | Loss: 4.4499 | Batch Acc: 0.3750\n",
            "Batch 43/72 | Loss: 5.5680 | Batch Acc: 0.1250\n",
            "Batch 44/72 | Loss: 6.2912 | Batch Acc: 0.0625\n",
            "Batch 45/72 | Loss: 5.9215 | Batch Acc: 0.1875\n",
            "Batch 46/72 | Loss: 5.9217 | Batch Acc: 0.0625\n",
            "Batch 47/72 | Loss: 6.1251 | Batch Acc: 0.3125\n",
            "Batch 48/72 | Loss: 4.7322 | Batch Acc: 0.1875\n",
            "Batch 49/72 | Loss: 4.4952 | Batch Acc: 0.2500\n",
            "Batch 50/72 | Loss: 4.3633 | Batch Acc: 0.5000\n",
            "Batch 51/72 | Loss: 5.6641 | Batch Acc: 0.2500\n",
            "Batch 52/72 | Loss: 5.9709 | Batch Acc: 0.1875\n",
            "Batch 53/72 | Loss: 6.2372 | Batch Acc: 0.1250\n",
            "Batch 54/72 | Loss: 5.7921 | Batch Acc: 0.1875\n",
            "Batch 55/72 | Loss: 5.3776 | Batch Acc: 0.3750\n",
            "Batch 56/72 | Loss: 4.9130 | Batch Acc: 0.2500\n",
            "Batch 57/72 | Loss: 5.7173 | Batch Acc: 0.0625\n",
            "Batch 58/72 | Loss: 5.5015 | Batch Acc: 0.3125\n",
            "Batch 59/72 | Loss: 4.7451 | Batch Acc: 0.1875\n",
            "Batch 60/72 | Loss: 6.2796 | Batch Acc: 0.1875\n",
            "Batch 61/72 | Loss: 5.3430 | Batch Acc: 0.0000\n",
            "Batch 62/72 | Loss: 5.1831 | Batch Acc: 0.1250\n",
            "Batch 63/72 | Loss: 6.3323 | Batch Acc: 0.2500\n",
            "Batch 64/72 | Loss: 5.3339 | Batch Acc: 0.2500\n",
            "Batch 65/72 | Loss: 4.8683 | Batch Acc: 0.1875\n",
            "Batch 66/72 | Loss: 5.6773 | Batch Acc: 0.1875\n",
            "Batch 67/72 | Loss: 5.3339 | Batch Acc: 0.1875\n",
            "Batch 68/72 | Loss: 5.5747 | Batch Acc: 0.1875\n",
            "Batch 69/72 | Loss: 6.8937 | Batch Acc: 0.0625\n",
            "Batch 70/72 | Loss: 6.3849 | Batch Acc: 0.1875\n",
            "Batch 71/72 | Loss: 4.7467 | Batch Acc: 0.2500\n",
            "Batch 72/72 | Loss: 4.8997 | Batch Acc: 0.0769\n",
            "Epoch Train Loss: 383.3677, Accuracy: 0.2376\n",
            "Validation Accuracy: 0.2670\n",
            "\n",
            "Epoch 23/50\n",
            "Batch 1/72 | Loss: 5.2369 | Batch Acc: 0.2500\n",
            "Batch 2/72 | Loss: 5.2268 | Batch Acc: 0.3125\n",
            "Batch 3/72 | Loss: 5.7738 | Batch Acc: 0.1875\n",
            "Batch 4/72 | Loss: 5.8880 | Batch Acc: 0.1250\n",
            "Batch 5/72 | Loss: 6.8489 | Batch Acc: 0.1250\n",
            "Batch 6/72 | Loss: 4.6055 | Batch Acc: 0.2500\n",
            "Batch 7/72 | Loss: 6.0221 | Batch Acc: 0.1875\n",
            "Batch 8/72 | Loss: 5.5275 | Batch Acc: 0.2500\n",
            "Batch 9/72 | Loss: 5.1517 | Batch Acc: 0.2500\n",
            "Batch 10/72 | Loss: 4.7748 | Batch Acc: 0.4375\n",
            "Batch 11/72 | Loss: 6.0174 | Batch Acc: 0.1875\n",
            "Batch 12/72 | Loss: 6.4429 | Batch Acc: 0.1875\n",
            "Batch 13/72 | Loss: 5.1529 | Batch Acc: 0.3125\n",
            "Batch 14/72 | Loss: 5.0046 | Batch Acc: 0.2500\n",
            "Batch 15/72 | Loss: 5.1503 | Batch Acc: 0.3750\n",
            "Batch 16/72 | Loss: 4.5324 | Batch Acc: 0.1875\n",
            "Batch 17/72 | Loss: 4.4620 | Batch Acc: 0.2500\n",
            "Batch 18/72 | Loss: 5.2376 | Batch Acc: 0.1875\n",
            "Batch 19/72 | Loss: 4.5599 | Batch Acc: 0.2500\n",
            "Batch 20/72 | Loss: 5.2491 | Batch Acc: 0.2500\n",
            "Batch 21/72 | Loss: 5.3475 | Batch Acc: 0.2500\n",
            "Batch 22/72 | Loss: 6.1819 | Batch Acc: 0.1250\n",
            "Batch 23/72 | Loss: 4.4776 | Batch Acc: 0.3125\n",
            "Batch 24/72 | Loss: 6.0910 | Batch Acc: 0.2500\n",
            "Batch 25/72 | Loss: 6.2254 | Batch Acc: 0.1250\n",
            "Batch 26/72 | Loss: 5.0135 | Batch Acc: 0.2500\n",
            "Batch 27/72 | Loss: 5.3642 | Batch Acc: 0.4375\n",
            "Batch 28/72 | Loss: 5.3746 | Batch Acc: 0.3125\n",
            "Batch 29/72 | Loss: 5.9011 | Batch Acc: 0.1250\n",
            "Batch 30/72 | Loss: 4.6285 | Batch Acc: 0.4375\n",
            "Batch 31/72 | Loss: 5.8380 | Batch Acc: 0.0625\n",
            "Batch 32/72 | Loss: 5.4749 | Batch Acc: 0.3125\n",
            "Batch 33/72 | Loss: 5.1471 | Batch Acc: 0.2500\n",
            "Batch 34/72 | Loss: 4.6083 | Batch Acc: 0.3125\n",
            "Batch 35/72 | Loss: 5.4295 | Batch Acc: 0.1875\n",
            "Batch 36/72 | Loss: 5.0384 | Batch Acc: 0.3750\n",
            "Batch 37/72 | Loss: 6.2253 | Batch Acc: 0.1250\n",
            "Batch 38/72 | Loss: 5.3161 | Batch Acc: 0.1875\n",
            "Batch 39/72 | Loss: 5.2934 | Batch Acc: 0.2500\n",
            "Batch 40/72 | Loss: 3.9913 | Batch Acc: 0.2500\n",
            "Batch 41/72 | Loss: 5.5165 | Batch Acc: 0.2500\n",
            "Batch 42/72 | Loss: 4.7366 | Batch Acc: 0.1875\n",
            "Batch 43/72 | Loss: 4.6570 | Batch Acc: 0.1875\n",
            "Batch 44/72 | Loss: 6.8936 | Batch Acc: 0.1250\n",
            "Batch 45/72 | Loss: 6.1959 | Batch Acc: 0.1250\n",
            "Batch 46/72 | Loss: 5.1713 | Batch Acc: 0.1250\n",
            "Batch 47/72 | Loss: 5.3530 | Batch Acc: 0.2500\n",
            "Batch 48/72 | Loss: 5.2694 | Batch Acc: 0.2500\n",
            "Batch 49/72 | Loss: 5.3152 | Batch Acc: 0.2500\n",
            "Batch 50/72 | Loss: 5.3332 | Batch Acc: 0.2500\n",
            "Batch 51/72 | Loss: 4.7621 | Batch Acc: 0.3750\n",
            "Batch 52/72 | Loss: 4.8503 | Batch Acc: 0.3125\n",
            "Batch 53/72 | Loss: 5.1469 | Batch Acc: 0.1250\n",
            "Batch 54/72 | Loss: 4.3211 | Batch Acc: 0.3750\n",
            "Batch 55/72 | Loss: 6.4812 | Batch Acc: 0.1250\n",
            "Batch 56/72 | Loss: 5.0329 | Batch Acc: 0.3750\n",
            "Batch 57/72 | Loss: 5.4024 | Batch Acc: 0.4375\n",
            "Batch 58/72 | Loss: 6.3610 | Batch Acc: 0.2500\n",
            "Batch 59/72 | Loss: 6.0793 | Batch Acc: 0.2500\n",
            "Batch 60/72 | Loss: 4.1039 | Batch Acc: 0.3750\n",
            "Batch 61/72 | Loss: 6.9265 | Batch Acc: 0.1250\n",
            "Batch 62/72 | Loss: 6.0876 | Batch Acc: 0.0625\n",
            "Batch 63/72 | Loss: 4.0478 | Batch Acc: 0.5000\n",
            "Batch 64/72 | Loss: 6.4321 | Batch Acc: 0.1875\n",
            "Batch 65/72 | Loss: 4.1114 | Batch Acc: 0.2500\n",
            "Batch 66/72 | Loss: 5.2088 | Batch Acc: 0.2500\n",
            "Batch 67/72 | Loss: 3.7323 | Batch Acc: 0.4375\n",
            "Batch 68/72 | Loss: 4.6911 | Batch Acc: 0.2500\n",
            "Batch 69/72 | Loss: 6.1138 | Batch Acc: 0.1250\n",
            "Batch 70/72 | Loss: 5.1138 | Batch Acc: 0.1875\n",
            "Batch 71/72 | Loss: 3.9051 | Batch Acc: 0.4375\n",
            "Batch 72/72 | Loss: 4.5820 | Batch Acc: 0.4615\n",
            "Epoch Train Loss: 381.7674, Accuracy: 0.2498\n",
            "Validation Accuracy: 0.3182\n",
            "‚úÖ ÂÑ≤Â≠òÊúÄ‰Ω≥Ê®°ÂûãÔºàAcc: 0.3182Ôºâ\n",
            "\n",
            "Epoch 24/50\n",
            "Batch 1/72 | Loss: 5.2069 | Batch Acc: 0.3125\n",
            "Batch 2/72 | Loss: 6.1189 | Batch Acc: 0.2500\n",
            "Batch 3/72 | Loss: 4.7099 | Batch Acc: 0.3750\n",
            "Batch 4/72 | Loss: 4.7821 | Batch Acc: 0.4375\n",
            "Batch 5/72 | Loss: 5.2893 | Batch Acc: 0.3750\n",
            "Batch 6/72 | Loss: 4.7410 | Batch Acc: 0.4375\n",
            "Batch 7/72 | Loss: 5.4955 | Batch Acc: 0.3125\n",
            "Batch 8/72 | Loss: 4.9455 | Batch Acc: 0.5000\n",
            "Batch 9/72 | Loss: 4.6467 | Batch Acc: 0.2500\n",
            "Batch 10/72 | Loss: 4.7876 | Batch Acc: 0.3750\n",
            "Batch 11/72 | Loss: 5.9296 | Batch Acc: 0.3125\n",
            "Batch 12/72 | Loss: 6.9869 | Batch Acc: 0.0625\n",
            "Batch 13/72 | Loss: 5.3665 | Batch Acc: 0.2500\n",
            "Batch 14/72 | Loss: 5.7638 | Batch Acc: 0.1250\n",
            "Batch 15/72 | Loss: 4.3657 | Batch Acc: 0.2500\n",
            "Batch 16/72 | Loss: 5.6767 | Batch Acc: 0.1250\n",
            "Batch 17/72 | Loss: 5.0666 | Batch Acc: 0.1875\n",
            "Batch 18/72 | Loss: 5.5100 | Batch Acc: 0.3125\n",
            "Batch 19/72 | Loss: 5.0891 | Batch Acc: 0.2500\n",
            "Batch 20/72 | Loss: 4.5802 | Batch Acc: 0.5000\n",
            "Batch 21/72 | Loss: 4.7189 | Batch Acc: 0.2500\n",
            "Batch 22/72 | Loss: 5.0654 | Batch Acc: 0.4375\n",
            "Batch 23/72 | Loss: 6.2441 | Batch Acc: 0.1875\n",
            "Batch 24/72 | Loss: 4.6989 | Batch Acc: 0.2500\n",
            "Batch 25/72 | Loss: 6.8982 | Batch Acc: 0.2500\n",
            "Batch 26/72 | Loss: 4.0485 | Batch Acc: 0.4375\n",
            "Batch 27/72 | Loss: 5.7303 | Batch Acc: 0.1250\n",
            "Batch 28/72 | Loss: 5.5357 | Batch Acc: 0.2500\n",
            "Batch 29/72 | Loss: 5.3439 | Batch Acc: 0.3125\n",
            "Batch 30/72 | Loss: 5.2363 | Batch Acc: 0.3750\n",
            "Batch 31/72 | Loss: 4.8726 | Batch Acc: 0.3125\n",
            "Batch 32/72 | Loss: 6.1104 | Batch Acc: 0.1875\n",
            "Batch 33/72 | Loss: 5.9111 | Batch Acc: 0.2500\n",
            "Batch 34/72 | Loss: 5.3363 | Batch Acc: 0.3125\n",
            "Batch 35/72 | Loss: 4.7679 | Batch Acc: 0.3125\n",
            "Batch 36/72 | Loss: 5.8090 | Batch Acc: 0.2500\n",
            "Batch 37/72 | Loss: 5.1113 | Batch Acc: 0.2500\n",
            "Batch 38/72 | Loss: 4.8350 | Batch Acc: 0.4375\n",
            "Batch 39/72 | Loss: 7.1366 | Batch Acc: 0.1250\n",
            "Batch 40/72 | Loss: 5.1898 | Batch Acc: 0.2500\n",
            "Batch 41/72 | Loss: 4.6944 | Batch Acc: 0.3125\n",
            "Batch 42/72 | Loss: 4.6109 | Batch Acc: 0.1875\n",
            "Batch 43/72 | Loss: 6.2724 | Batch Acc: 0.1875\n",
            "Batch 44/72 | Loss: 5.3364 | Batch Acc: 0.2500\n",
            "Batch 45/72 | Loss: 5.0093 | Batch Acc: 0.2500\n",
            "Batch 46/72 | Loss: 5.6517 | Batch Acc: 0.2500\n",
            "Batch 47/72 | Loss: 4.4904 | Batch Acc: 0.3125\n",
            "Batch 48/72 | Loss: 5.1810 | Batch Acc: 0.2500\n",
            "Batch 49/72 | Loss: 5.2927 | Batch Acc: 0.3125\n",
            "Batch 50/72 | Loss: 4.1173 | Batch Acc: 0.5625\n",
            "Batch 51/72 | Loss: 5.2170 | Batch Acc: 0.3125\n",
            "Batch 52/72 | Loss: 5.7002 | Batch Acc: 0.2500\n",
            "Batch 53/72 | Loss: 5.6862 | Batch Acc: 0.3125\n",
            "Batch 54/72 | Loss: 5.5294 | Batch Acc: 0.1250\n",
            "Batch 55/72 | Loss: 5.3518 | Batch Acc: 0.3750\n",
            "Batch 56/72 | Loss: 5.1519 | Batch Acc: 0.0625\n",
            "Batch 57/72 | Loss: 5.4808 | Batch Acc: 0.2500\n",
            "Batch 58/72 | Loss: 5.5654 | Batch Acc: 0.0625\n",
            "Batch 59/72 | Loss: 4.1046 | Batch Acc: 0.5625\n",
            "Batch 60/72 | Loss: 5.9606 | Batch Acc: 0.0625\n",
            "Batch 61/72 | Loss: 5.6613 | Batch Acc: 0.1875\n",
            "Batch 62/72 | Loss: 4.8250 | Batch Acc: 0.1875\n",
            "Batch 63/72 | Loss: 5.2334 | Batch Acc: 0.2500\n",
            "Batch 64/72 | Loss: 6.0237 | Batch Acc: 0.1875\n",
            "Batch 65/72 | Loss: 6.1058 | Batch Acc: 0.1250\n",
            "Batch 66/72 | Loss: 6.3229 | Batch Acc: 0.1875\n",
            "Batch 67/72 | Loss: 5.2234 | Batch Acc: 0.1875\n",
            "Batch 68/72 | Loss: 5.2448 | Batch Acc: 0.3125\n",
            "Batch 69/72 | Loss: 5.2399 | Batch Acc: 0.1250\n",
            "Batch 70/72 | Loss: 6.5386 | Batch Acc: 0.1250\n",
            "Batch 71/72 | Loss: 4.6267 | Batch Acc: 0.2500\n",
            "Batch 72/72 | Loss: 5.9032 | Batch Acc: 0.3846\n",
            "Epoch Train Loss: 385.0116, Accuracy: 0.2681\n",
            "Validation Accuracy: 0.2841\n",
            "\n",
            "Epoch 25/50\n",
            "Batch 1/72 | Loss: 5.3845 | Batch Acc: 0.3125\n",
            "Batch 2/72 | Loss: 5.5800 | Batch Acc: 0.1875\n",
            "Batch 3/72 | Loss: 3.2658 | Batch Acc: 0.6250\n",
            "Batch 4/72 | Loss: 6.5774 | Batch Acc: 0.1250\n",
            "Batch 5/72 | Loss: 6.9430 | Batch Acc: 0.1250\n",
            "Batch 6/72 | Loss: 5.5213 | Batch Acc: 0.3125\n",
            "Batch 7/72 | Loss: 5.9331 | Batch Acc: 0.2500\n",
            "Batch 8/72 | Loss: 4.3811 | Batch Acc: 0.4375\n",
            "Batch 9/72 | Loss: 5.0728 | Batch Acc: 0.3125\n",
            "Batch 10/72 | Loss: 5.7581 | Batch Acc: 0.2500\n",
            "Batch 11/72 | Loss: 5.6316 | Batch Acc: 0.2500\n",
            "Batch 12/72 | Loss: 5.2307 | Batch Acc: 0.3750\n",
            "Batch 13/72 | Loss: 5.5772 | Batch Acc: 0.1875\n",
            "Batch 14/72 | Loss: 5.6240 | Batch Acc: 0.2500\n",
            "Batch 15/72 | Loss: 5.8118 | Batch Acc: 0.1875\n",
            "Batch 16/72 | Loss: 5.5260 | Batch Acc: 0.1875\n",
            "Batch 17/72 | Loss: 5.8499 | Batch Acc: 0.2500\n",
            "Batch 18/72 | Loss: 4.6165 | Batch Acc: 0.3750\n",
            "Batch 19/72 | Loss: 5.3488 | Batch Acc: 0.3125\n",
            "Batch 20/72 | Loss: 4.9249 | Batch Acc: 0.2500\n",
            "Batch 21/72 | Loss: 5.7788 | Batch Acc: 0.1250\n",
            "Batch 22/72 | Loss: 5.6769 | Batch Acc: 0.2500\n",
            "Batch 23/72 | Loss: 4.3811 | Batch Acc: 0.2500\n",
            "Batch 24/72 | Loss: 5.2482 | Batch Acc: 0.2500\n",
            "Batch 25/72 | Loss: 4.1980 | Batch Acc: 0.1875\n",
            "Batch 26/72 | Loss: 5.3496 | Batch Acc: 0.1875\n",
            "Batch 27/72 | Loss: 5.0194 | Batch Acc: 0.3125\n",
            "Batch 28/72 | Loss: 5.3333 | Batch Acc: 0.3125\n",
            "Batch 29/72 | Loss: 5.9819 | Batch Acc: 0.1250\n",
            "Batch 30/72 | Loss: 6.1272 | Batch Acc: 0.1250\n",
            "Batch 31/72 | Loss: 6.0726 | Batch Acc: 0.2500\n",
            "Batch 32/72 | Loss: 4.9920 | Batch Acc: 0.3125\n",
            "Batch 33/72 | Loss: 5.5521 | Batch Acc: 0.1875\n",
            "Batch 34/72 | Loss: 4.5426 | Batch Acc: 0.3125\n",
            "Batch 35/72 | Loss: 5.4002 | Batch Acc: 0.2500\n",
            "Batch 36/72 | Loss: 5.6074 | Batch Acc: 0.2500\n",
            "Batch 37/72 | Loss: 5.4611 | Batch Acc: 0.2500\n",
            "Batch 38/72 | Loss: 4.3488 | Batch Acc: 0.3750\n",
            "Batch 39/72 | Loss: 5.7080 | Batch Acc: 0.1875\n",
            "Batch 40/72 | Loss: 5.5635 | Batch Acc: 0.3125\n",
            "Batch 41/72 | Loss: 4.3304 | Batch Acc: 0.3750\n",
            "Batch 42/72 | Loss: 4.8463 | Batch Acc: 0.3125\n",
            "Batch 43/72 | Loss: 5.0003 | Batch Acc: 0.1250\n",
            "Batch 44/72 | Loss: 5.6398 | Batch Acc: 0.3125\n",
            "Batch 45/72 | Loss: 4.4399 | Batch Acc: 0.3750\n",
            "Batch 46/72 | Loss: 4.5517 | Batch Acc: 0.4375\n",
            "Batch 47/72 | Loss: 4.7545 | Batch Acc: 0.3750\n",
            "Batch 48/72 | Loss: 5.1897 | Batch Acc: 0.1250\n",
            "Batch 49/72 | Loss: 5.3819 | Batch Acc: 0.1875\n",
            "Batch 50/72 | Loss: 3.9417 | Batch Acc: 0.6250\n",
            "Batch 51/72 | Loss: 5.5608 | Batch Acc: 0.1875\n",
            "Batch 52/72 | Loss: 4.9649 | Batch Acc: 0.2500\n",
            "Batch 53/72 | Loss: 5.3047 | Batch Acc: 0.1875\n",
            "Batch 54/72 | Loss: 4.6583 | Batch Acc: 0.1875\n",
            "Batch 55/72 | Loss: 4.4412 | Batch Acc: 0.3125\n",
            "Batch 56/72 | Loss: 4.4929 | Batch Acc: 0.4375\n",
            "Batch 57/72 | Loss: 6.4807 | Batch Acc: 0.1875\n",
            "Batch 58/72 | Loss: 5.6548 | Batch Acc: 0.0625\n",
            "Batch 59/72 | Loss: 3.9669 | Batch Acc: 0.3750\n",
            "Batch 60/72 | Loss: 5.9088 | Batch Acc: 0.3750\n",
            "Batch 61/72 | Loss: 6.5150 | Batch Acc: 0.1875\n",
            "Batch 62/72 | Loss: 5.9924 | Batch Acc: 0.0625\n",
            "Batch 63/72 | Loss: 4.9574 | Batch Acc: 0.3125\n",
            "Batch 64/72 | Loss: 4.5329 | Batch Acc: 0.3750\n",
            "Batch 65/72 | Loss: 5.8234 | Batch Acc: 0.1875\n",
            "Batch 66/72 | Loss: 5.6194 | Batch Acc: 0.3125\n",
            "Batch 67/72 | Loss: 5.8230 | Batch Acc: 0.3125\n",
            "Batch 68/72 | Loss: 4.6208 | Batch Acc: 0.4375\n",
            "Batch 69/72 | Loss: 4.9937 | Batch Acc: 0.4375\n",
            "Batch 70/72 | Loss: 5.2631 | Batch Acc: 0.1875\n",
            "Batch 71/72 | Loss: 5.8467 | Batch Acc: 0.0625\n",
            "Batch 72/72 | Loss: 5.6941 | Batch Acc: 0.3077\n",
            "Epoch Train Loss: 380.0933, Accuracy: 0.2689\n",
            "Validation Accuracy: 0.3239\n",
            "‚úÖ ÂÑ≤Â≠òÊúÄ‰Ω≥Ê®°ÂûãÔºàAcc: 0.3239Ôºâ\n",
            "\n",
            "Epoch 26/50\n",
            "Batch 1/72 | Loss: 4.3250 | Batch Acc: 0.5000\n",
            "Batch 2/72 | Loss: 5.0151 | Batch Acc: 0.3750\n",
            "Batch 3/72 | Loss: 5.9073 | Batch Acc: 0.1250\n",
            "Batch 4/72 | Loss: 5.3872 | Batch Acc: 0.2500\n",
            "Batch 5/72 | Loss: 5.7758 | Batch Acc: 0.1250\n",
            "Batch 6/72 | Loss: 5.7382 | Batch Acc: 0.1250\n",
            "Batch 7/72 | Loss: 7.1233 | Batch Acc: 0.0000\n",
            "Batch 8/72 | Loss: 5.1681 | Batch Acc: 0.2500\n",
            "Batch 9/72 | Loss: 5.0314 | Batch Acc: 0.3125\n",
            "Batch 10/72 | Loss: 5.0628 | Batch Acc: 0.3750\n",
            "Batch 11/72 | Loss: 5.0316 | Batch Acc: 0.3125\n",
            "Batch 12/72 | Loss: 4.8932 | Batch Acc: 0.3750\n",
            "Batch 13/72 | Loss: 6.6802 | Batch Acc: 0.1875\n",
            "Batch 14/72 | Loss: 4.0442 | Batch Acc: 0.3125\n",
            "Batch 15/72 | Loss: 5.4102 | Batch Acc: 0.3125\n",
            "Batch 16/72 | Loss: 4.9234 | Batch Acc: 0.3125\n",
            "Batch 17/72 | Loss: 5.9333 | Batch Acc: 0.0625\n",
            "Batch 18/72 | Loss: 6.1992 | Batch Acc: 0.3750\n",
            "Batch 19/72 | Loss: 5.3125 | Batch Acc: 0.1875\n",
            "Batch 20/72 | Loss: 4.8800 | Batch Acc: 0.3750\n",
            "Batch 21/72 | Loss: 6.0286 | Batch Acc: 0.0625\n",
            "Batch 22/72 | Loss: 5.7033 | Batch Acc: 0.1250\n",
            "Batch 23/72 | Loss: 4.3791 | Batch Acc: 0.4375\n",
            "Batch 24/72 | Loss: 4.8569 | Batch Acc: 0.3750\n",
            "Batch 25/72 | Loss: 5.2682 | Batch Acc: 0.2500\n",
            "Batch 26/72 | Loss: 5.9889 | Batch Acc: 0.1250\n",
            "Batch 27/72 | Loss: 5.8223 | Batch Acc: 0.1875\n",
            "Batch 28/72 | Loss: 6.7164 | Batch Acc: 0.1875\n",
            "Batch 29/72 | Loss: 4.9948 | Batch Acc: 0.3125\n",
            "Batch 30/72 | Loss: 5.2982 | Batch Acc: 0.3750\n",
            "Batch 31/72 | Loss: 5.3922 | Batch Acc: 0.2500\n",
            "Batch 32/72 | Loss: 4.4054 | Batch Acc: 0.3750\n",
            "Batch 33/72 | Loss: 5.0755 | Batch Acc: 0.2500\n",
            "Batch 34/72 | Loss: 4.8102 | Batch Acc: 0.3125\n",
            "Batch 35/72 | Loss: 5.3664 | Batch Acc: 0.2500\n",
            "Batch 36/72 | Loss: 5.4256 | Batch Acc: 0.0625\n",
            "Batch 37/72 | Loss: 5.1986 | Batch Acc: 0.2500\n",
            "Batch 38/72 | Loss: 4.5773 | Batch Acc: 0.1875\n",
            "Batch 39/72 | Loss: 4.8383 | Batch Acc: 0.1875\n",
            "Batch 40/72 | Loss: 4.1744 | Batch Acc: 0.2500\n",
            "Batch 41/72 | Loss: 5.8304 | Batch Acc: 0.1875\n",
            "Batch 42/72 | Loss: 5.8773 | Batch Acc: 0.2500\n",
            "Batch 43/72 | Loss: 5.3279 | Batch Acc: 0.1250\n",
            "Batch 44/72 | Loss: 3.9788 | Batch Acc: 0.3125\n",
            "Batch 45/72 | Loss: 5.2885 | Batch Acc: 0.3125\n",
            "Batch 46/72 | Loss: 4.7724 | Batch Acc: 0.1875\n",
            "Batch 47/72 | Loss: 6.1721 | Batch Acc: 0.1250\n",
            "Batch 48/72 | Loss: 6.5666 | Batch Acc: 0.3125\n",
            "Batch 49/72 | Loss: 5.1974 | Batch Acc: 0.1250\n",
            "Batch 50/72 | Loss: 6.2950 | Batch Acc: 0.1875\n",
            "Batch 51/72 | Loss: 5.4239 | Batch Acc: 0.1250\n",
            "Batch 52/72 | Loss: 5.0380 | Batch Acc: 0.2500\n",
            "Batch 53/72 | Loss: 4.8445 | Batch Acc: 0.2500\n",
            "Batch 54/72 | Loss: 5.5061 | Batch Acc: 0.3125\n",
            "Batch 55/72 | Loss: 4.8958 | Batch Acc: 0.4375\n",
            "Batch 56/72 | Loss: 5.0438 | Batch Acc: 0.4375\n",
            "Batch 57/72 | Loss: 6.1740 | Batch Acc: 0.2500\n",
            "Batch 58/72 | Loss: 4.4057 | Batch Acc: 0.6250\n",
            "Batch 59/72 | Loss: 5.9133 | Batch Acc: 0.2500\n",
            "Batch 60/72 | Loss: 5.0112 | Batch Acc: 0.3125\n",
            "Batch 61/72 | Loss: 4.2201 | Batch Acc: 0.3750\n",
            "Batch 62/72 | Loss: 4.9851 | Batch Acc: 0.4375\n",
            "Batch 63/72 | Loss: 4.5435 | Batch Acc: 0.4375\n",
            "Batch 64/72 | Loss: 5.8938 | Batch Acc: 0.2500\n",
            "Batch 65/72 | Loss: 6.2849 | Batch Acc: 0.2500\n",
            "Batch 66/72 | Loss: 5.9938 | Batch Acc: 0.1250\n",
            "Batch 67/72 | Loss: 5.2980 | Batch Acc: 0.2500\n",
            "Batch 68/72 | Loss: 5.1429 | Batch Acc: 0.0625\n",
            "Batch 69/72 | Loss: 4.8321 | Batch Acc: 0.3125\n",
            "Batch 70/72 | Loss: 5.5148 | Batch Acc: 0.1250\n",
            "Batch 71/72 | Loss: 5.2904 | Batch Acc: 0.3125\n",
            "Batch 72/72 | Loss: 4.8010 | Batch Acc: 0.3077\n",
            "Epoch Train Loss: 382.5255, Accuracy: 0.2594\n",
            "Validation Accuracy: 0.3068\n",
            "\n",
            "Epoch 27/50\n",
            "Batch 1/72 | Loss: 4.9272 | Batch Acc: 0.3750\n",
            "Batch 2/72 | Loss: 4.6096 | Batch Acc: 0.2500\n",
            "Batch 3/72 | Loss: 4.4985 | Batch Acc: 0.6250\n",
            "Batch 4/72 | Loss: 4.8310 | Batch Acc: 0.3125\n",
            "Batch 5/72 | Loss: 5.9023 | Batch Acc: 0.2500\n",
            "Batch 6/72 | Loss: 4.7589 | Batch Acc: 0.3125\n",
            "Batch 7/72 | Loss: 5.6457 | Batch Acc: 0.2500\n",
            "Batch 8/72 | Loss: 3.9556 | Batch Acc: 0.3125\n",
            "Batch 9/72 | Loss: 4.7796 | Batch Acc: 0.2500\n",
            "Batch 10/72 | Loss: 5.5542 | Batch Acc: 0.1875\n",
            "Batch 11/72 | Loss: 4.6334 | Batch Acc: 0.1250\n",
            "Batch 12/72 | Loss: 4.3297 | Batch Acc: 0.3750\n",
            "Batch 13/72 | Loss: 5.2396 | Batch Acc: 0.3125\n",
            "Batch 14/72 | Loss: 4.0445 | Batch Acc: 0.1875\n",
            "Batch 15/72 | Loss: 4.8065 | Batch Acc: 0.1875\n",
            "Batch 16/72 | Loss: 4.6008 | Batch Acc: 0.1875\n",
            "Batch 17/72 | Loss: 5.6431 | Batch Acc: 0.1875\n",
            "Batch 18/72 | Loss: 5.4044 | Batch Acc: 0.1875\n",
            "Batch 19/72 | Loss: 4.6868 | Batch Acc: 0.3750\n",
            "Batch 20/72 | Loss: 4.8180 | Batch Acc: 0.2500\n",
            "Batch 21/72 | Loss: 4.1961 | Batch Acc: 0.4375\n",
            "Batch 22/72 | Loss: 5.2915 | Batch Acc: 0.3125\n",
            "Batch 23/72 | Loss: 4.4229 | Batch Acc: 0.3750\n",
            "Batch 24/72 | Loss: 5.7701 | Batch Acc: 0.0625\n",
            "Batch 25/72 | Loss: 6.6154 | Batch Acc: 0.1250\n",
            "Batch 26/72 | Loss: 5.5126 | Batch Acc: 0.4375\n",
            "Batch 27/72 | Loss: 4.6634 | Batch Acc: 0.1875\n",
            "Batch 28/72 | Loss: 4.9311 | Batch Acc: 0.1250\n",
            "Batch 29/72 | Loss: 4.4336 | Batch Acc: 0.4375\n",
            "Batch 30/72 | Loss: 5.4671 | Batch Acc: 0.3125\n",
            "Batch 31/72 | Loss: 5.5289 | Batch Acc: 0.1250\n",
            "Batch 32/72 | Loss: 4.9157 | Batch Acc: 0.3125\n",
            "Batch 33/72 | Loss: 5.4117 | Batch Acc: 0.3125\n",
            "Batch 34/72 | Loss: 5.3930 | Batch Acc: 0.2500\n",
            "Batch 35/72 | Loss: 5.0847 | Batch Acc: 0.2500\n",
            "Batch 36/72 | Loss: 5.3911 | Batch Acc: 0.3125\n",
            "Batch 37/72 | Loss: 5.1842 | Batch Acc: 0.3750\n",
            "Batch 38/72 | Loss: 5.9152 | Batch Acc: 0.2500\n",
            "Batch 39/72 | Loss: 6.1856 | Batch Acc: 0.0000\n",
            "Batch 40/72 | Loss: 5.3098 | Batch Acc: 0.1875\n",
            "Batch 41/72 | Loss: 5.3932 | Batch Acc: 0.3125\n",
            "Batch 42/72 | Loss: 6.8902 | Batch Acc: 0.2500\n",
            "Batch 43/72 | Loss: 4.0339 | Batch Acc: 0.3125\n",
            "Batch 44/72 | Loss: 5.8800 | Batch Acc: 0.3125\n",
            "Batch 45/72 | Loss: 5.7597 | Batch Acc: 0.0625\n",
            "Batch 46/72 | Loss: 6.1920 | Batch Acc: 0.1250\n",
            "Batch 47/72 | Loss: 7.0764 | Batch Acc: 0.2500\n",
            "Batch 48/72 | Loss: 5.7295 | Batch Acc: 0.2500\n",
            "Batch 49/72 | Loss: 5.1074 | Batch Acc: 0.1250\n",
            "Batch 50/72 | Loss: 4.2941 | Batch Acc: 0.2500\n",
            "Batch 51/72 | Loss: 4.0224 | Batch Acc: 0.5000\n",
            "Batch 52/72 | Loss: 5.3419 | Batch Acc: 0.2500\n",
            "Batch 53/72 | Loss: 4.6793 | Batch Acc: 0.4375\n",
            "Batch 54/72 | Loss: 4.1311 | Batch Acc: 0.2500\n",
            "Batch 55/72 | Loss: 4.3400 | Batch Acc: 0.4375\n",
            "Batch 56/72 | Loss: 5.3275 | Batch Acc: 0.1250\n",
            "Batch 57/72 | Loss: 5.3501 | Batch Acc: 0.3750\n",
            "Batch 58/72 | Loss: 5.9276 | Batch Acc: 0.1875\n",
            "Batch 59/72 | Loss: 5.8622 | Batch Acc: 0.1250\n",
            "Batch 60/72 | Loss: 5.3275 | Batch Acc: 0.1250\n",
            "Batch 61/72 | Loss: 5.5013 | Batch Acc: 0.1875\n",
            "Batch 62/72 | Loss: 4.9410 | Batch Acc: 0.4375\n",
            "Batch 63/72 | Loss: 3.8153 | Batch Acc: 0.5000\n",
            "Batch 64/72 | Loss: 3.8402 | Batch Acc: 0.5000\n",
            "Batch 65/72 | Loss: 6.0964 | Batch Acc: 0.1250\n",
            "Batch 66/72 | Loss: 5.6769 | Batch Acc: 0.3125\n",
            "Batch 67/72 | Loss: 6.5253 | Batch Acc: 0.1250\n",
            "Batch 68/72 | Loss: 5.2346 | Batch Acc: 0.3750\n",
            "Batch 69/72 | Loss: 5.5948 | Batch Acc: 0.1875\n",
            "Batch 70/72 | Loss: 6.2049 | Batch Acc: 0.3125\n",
            "Batch 71/72 | Loss: 5.8498 | Batch Acc: 0.0625\n",
            "Batch 72/72 | Loss: 7.9284 | Batch Acc: 0.1538\n",
            "Epoch Train Loss: 377.1679, Accuracy: 0.2646\n",
            "Validation Accuracy: 0.3580\n",
            "‚úÖ ÂÑ≤Â≠òÊúÄ‰Ω≥Ê®°ÂûãÔºàAcc: 0.3580Ôºâ\n",
            "\n",
            "Epoch 28/50\n",
            "Batch 1/72 | Loss: 5.2999 | Batch Acc: 0.3750\n",
            "Batch 2/72 | Loss: 5.4257 | Batch Acc: 0.1250\n",
            "Batch 3/72 | Loss: 5.0319 | Batch Acc: 0.2500\n",
            "Batch 4/72 | Loss: 4.3432 | Batch Acc: 0.5625\n",
            "Batch 5/72 | Loss: 6.1482 | Batch Acc: 0.1875\n",
            "Batch 6/72 | Loss: 4.7154 | Batch Acc: 0.4375\n",
            "Batch 7/72 | Loss: 4.7781 | Batch Acc: 0.3750\n",
            "Batch 8/72 | Loss: 4.9594 | Batch Acc: 0.1875\n",
            "Batch 9/72 | Loss: 4.6448 | Batch Acc: 0.2500\n",
            "Batch 10/72 | Loss: 5.3732 | Batch Acc: 0.1875\n",
            "Batch 11/72 | Loss: 3.7164 | Batch Acc: 0.6875\n",
            "Batch 12/72 | Loss: 6.8149 | Batch Acc: 0.1875\n",
            "Batch 13/72 | Loss: 6.2727 | Batch Acc: 0.1875\n",
            "Batch 14/72 | Loss: 6.1517 | Batch Acc: 0.0000\n",
            "Batch 15/72 | Loss: 5.4524 | Batch Acc: 0.3125\n",
            "Batch 16/72 | Loss: 4.4300 | Batch Acc: 0.5000\n",
            "Batch 17/72 | Loss: 6.0340 | Batch Acc: 0.1875\n",
            "Batch 18/72 | Loss: 4.8460 | Batch Acc: 0.4375\n",
            "Batch 19/72 | Loss: 4.0983 | Batch Acc: 0.3750\n",
            "Batch 20/72 | Loss: 4.1143 | Batch Acc: 0.3125\n",
            "Batch 21/72 | Loss: 4.3243 | Batch Acc: 0.2500\n",
            "Batch 22/72 | Loss: 4.1834 | Batch Acc: 0.3750\n",
            "Batch 23/72 | Loss: 6.1726 | Batch Acc: 0.2500\n",
            "Batch 24/72 | Loss: 4.8951 | Batch Acc: 0.2500\n",
            "Batch 25/72 | Loss: 5.9958 | Batch Acc: 0.1250\n",
            "Batch 26/72 | Loss: 5.7379 | Batch Acc: 0.3125\n",
            "Batch 27/72 | Loss: 4.2110 | Batch Acc: 0.3125\n",
            "Batch 28/72 | Loss: 4.8915 | Batch Acc: 0.2500\n",
            "Batch 29/72 | Loss: 4.1780 | Batch Acc: 0.3125\n",
            "Batch 30/72 | Loss: 6.1149 | Batch Acc: 0.3125\n",
            "Batch 31/72 | Loss: 3.9598 | Batch Acc: 0.2500\n",
            "Batch 32/72 | Loss: 5.2706 | Batch Acc: 0.4375\n",
            "Batch 33/72 | Loss: 5.3348 | Batch Acc: 0.3750\n",
            "Batch 34/72 | Loss: 4.9732 | Batch Acc: 0.3750\n",
            "Batch 35/72 | Loss: 5.6992 | Batch Acc: 0.4375\n",
            "Batch 36/72 | Loss: 5.8094 | Batch Acc: 0.3125\n",
            "Batch 37/72 | Loss: 5.6298 | Batch Acc: 0.1875\n",
            "Batch 38/72 | Loss: 5.5690 | Batch Acc: 0.3125\n",
            "Batch 39/72 | Loss: 6.2143 | Batch Acc: 0.1250\n",
            "Batch 40/72 | Loss: 4.8385 | Batch Acc: 0.2500\n",
            "Batch 41/72 | Loss: 4.7278 | Batch Acc: 0.3750\n",
            "Batch 42/72 | Loss: 4.5498 | Batch Acc: 0.3125\n",
            "Batch 43/72 | Loss: 5.2737 | Batch Acc: 0.3125\n",
            "Batch 44/72 | Loss: 3.9801 | Batch Acc: 0.3125\n",
            "Batch 45/72 | Loss: 5.0684 | Batch Acc: 0.3125\n",
            "Batch 46/72 | Loss: 6.9601 | Batch Acc: 0.1250\n",
            "Batch 47/72 | Loss: 3.9312 | Batch Acc: 0.4375\n",
            "Batch 48/72 | Loss: 4.9474 | Batch Acc: 0.2500\n",
            "Batch 49/72 | Loss: 5.4923 | Batch Acc: 0.1875\n",
            "Batch 50/72 | Loss: 5.2412 | Batch Acc: 0.1875\n",
            "Batch 51/72 | Loss: 5.2954 | Batch Acc: 0.1875\n",
            "Batch 52/72 | Loss: 5.3773 | Batch Acc: 0.3125\n",
            "Batch 53/72 | Loss: 5.9366 | Batch Acc: 0.1875\n",
            "Batch 54/72 | Loss: 5.3031 | Batch Acc: 0.2500\n",
            "Batch 55/72 | Loss: 5.5330 | Batch Acc: 0.3750\n",
            "Batch 56/72 | Loss: 4.4631 | Batch Acc: 0.2500\n",
            "Batch 57/72 | Loss: 6.2970 | Batch Acc: 0.1875\n",
            "Batch 58/72 | Loss: 4.9867 | Batch Acc: 0.3750\n",
            "Batch 59/72 | Loss: 5.3077 | Batch Acc: 0.1875\n",
            "Batch 60/72 | Loss: 5.5434 | Batch Acc: 0.2500\n",
            "Batch 61/72 | Loss: 4.9996 | Batch Acc: 0.3125\n",
            "Batch 62/72 | Loss: 4.9302 | Batch Acc: 0.1875\n",
            "Batch 63/72 | Loss: 4.9156 | Batch Acc: 0.2500\n",
            "Batch 64/72 | Loss: 5.5005 | Batch Acc: 0.2500\n",
            "Batch 65/72 | Loss: 4.5307 | Batch Acc: 0.2500\n",
            "Batch 66/72 | Loss: 5.4397 | Batch Acc: 0.1875\n",
            "Batch 67/72 | Loss: 5.4780 | Batch Acc: 0.4375\n",
            "Batch 68/72 | Loss: 4.0395 | Batch Acc: 0.2500\n",
            "Batch 69/72 | Loss: 5.5852 | Batch Acc: 0.2500\n",
            "Batch 70/72 | Loss: 5.1806 | Batch Acc: 0.1875\n",
            "Batch 71/72 | Loss: 5.5673 | Batch Acc: 0.3125\n",
            "Batch 72/72 | Loss: 6.3807 | Batch Acc: 0.3077\n",
            "Epoch Train Loss: 373.4166, Accuracy: 0.2846\n",
            "Validation Accuracy: 0.3295\n",
            "\n",
            "Epoch 29/50\n",
            "Batch 1/72 | Loss: 5.1029 | Batch Acc: 0.3125\n",
            "Batch 2/72 | Loss: 5.3005 | Batch Acc: 0.4375\n",
            "Batch 3/72 | Loss: 4.6756 | Batch Acc: 0.5000\n",
            "Batch 4/72 | Loss: 4.8337 | Batch Acc: 0.1875\n",
            "Batch 5/72 | Loss: 4.6406 | Batch Acc: 0.1875\n",
            "Batch 6/72 | Loss: 5.2412 | Batch Acc: 0.1250\n",
            "Batch 7/72 | Loss: 4.7264 | Batch Acc: 0.3125\n",
            "Batch 8/72 | Loss: 5.0822 | Batch Acc: 0.3125\n",
            "Batch 9/72 | Loss: 3.8612 | Batch Acc: 0.3750\n",
            "Batch 10/72 | Loss: 5.5565 | Batch Acc: 0.4375\n",
            "Batch 11/72 | Loss: 3.8247 | Batch Acc: 0.4375\n",
            "Batch 12/72 | Loss: 4.5624 | Batch Acc: 0.3125\n",
            "Batch 13/72 | Loss: 6.1503 | Batch Acc: 0.0625\n",
            "Batch 14/72 | Loss: 5.4178 | Batch Acc: 0.2500\n",
            "Batch 15/72 | Loss: 4.9309 | Batch Acc: 0.2500\n",
            "Batch 16/72 | Loss: 7.8695 | Batch Acc: 0.1875\n",
            "Batch 17/72 | Loss: 5.6551 | Batch Acc: 0.2500\n",
            "Batch 18/72 | Loss: 5.2759 | Batch Acc: 0.4375\n",
            "Batch 19/72 | Loss: 5.6606 | Batch Acc: 0.1875\n",
            "Batch 20/72 | Loss: 5.4222 | Batch Acc: 0.3750\n",
            "Batch 21/72 | Loss: 5.0063 | Batch Acc: 0.3750\n",
            "Batch 22/72 | Loss: 4.4424 | Batch Acc: 0.4375\n",
            "Batch 23/72 | Loss: 5.3808 | Batch Acc: 0.1875\n",
            "Batch 24/72 | Loss: 6.4835 | Batch Acc: 0.1875\n",
            "Batch 25/72 | Loss: 4.7482 | Batch Acc: 0.3750\n",
            "Batch 26/72 | Loss: 4.5175 | Batch Acc: 0.2500\n",
            "Batch 27/72 | Loss: 4.1346 | Batch Acc: 0.4375\n",
            "Batch 28/72 | Loss: 4.3595 | Batch Acc: 0.3750\n",
            "Batch 29/72 | Loss: 5.5064 | Batch Acc: 0.1250\n",
            "Batch 30/72 | Loss: 4.4958 | Batch Acc: 0.3750\n",
            "Batch 31/72 | Loss: 4.3686 | Batch Acc: 0.2500\n",
            "Batch 32/72 | Loss: 5.9916 | Batch Acc: 0.1250\n",
            "Batch 33/72 | Loss: 4.5712 | Batch Acc: 0.1250\n",
            "Batch 34/72 | Loss: 5.9288 | Batch Acc: 0.3125\n",
            "Batch 35/72 | Loss: 5.9948 | Batch Acc: 0.2500\n",
            "Batch 36/72 | Loss: 4.4168 | Batch Acc: 0.1875\n",
            "Batch 37/72 | Loss: 4.0181 | Batch Acc: 0.3750\n",
            "Batch 38/72 | Loss: 5.6440 | Batch Acc: 0.3125\n",
            "Batch 39/72 | Loss: 5.6951 | Batch Acc: 0.2500\n",
            "Batch 40/72 | Loss: 6.2421 | Batch Acc: 0.1875\n",
            "Batch 41/72 | Loss: 5.1648 | Batch Acc: 0.1250\n",
            "Batch 42/72 | Loss: 5.0731 | Batch Acc: 0.2500\n",
            "Batch 43/72 | Loss: 5.4162 | Batch Acc: 0.1250\n",
            "Batch 44/72 | Loss: 5.5382 | Batch Acc: 0.1875\n",
            "Batch 45/72 | Loss: 5.5064 | Batch Acc: 0.2500\n",
            "Batch 46/72 | Loss: 4.4616 | Batch Acc: 0.1875\n",
            "Batch 47/72 | Loss: 4.9345 | Batch Acc: 0.5000\n",
            "Batch 48/72 | Loss: 5.2863 | Batch Acc: 0.2500\n",
            "Batch 49/72 | Loss: 5.0561 | Batch Acc: 0.1250\n",
            "Batch 50/72 | Loss: 6.0235 | Batch Acc: 0.1875\n",
            "Batch 51/72 | Loss: 5.8411 | Batch Acc: 0.3750\n",
            "Batch 52/72 | Loss: 4.7962 | Batch Acc: 0.4375\n",
            "Batch 53/72 | Loss: 5.0021 | Batch Acc: 0.1875\n",
            "Batch 54/72 | Loss: 6.5665 | Batch Acc: 0.0625\n",
            "Batch 55/72 | Loss: 4.2659 | Batch Acc: 0.1875\n",
            "Batch 56/72 | Loss: 4.3318 | Batch Acc: 0.1875\n",
            "Batch 57/72 | Loss: 5.9732 | Batch Acc: 0.2500\n",
            "Batch 58/72 | Loss: 4.8287 | Batch Acc: 0.3125\n",
            "Batch 59/72 | Loss: 4.0757 | Batch Acc: 0.3750\n",
            "Batch 60/72 | Loss: 6.8478 | Batch Acc: 0.0000\n",
            "Batch 61/72 | Loss: 4.7759 | Batch Acc: 0.3750\n",
            "Batch 62/72 | Loss: 5.4383 | Batch Acc: 0.2500\n",
            "Batch 63/72 | Loss: 5.5659 | Batch Acc: 0.2500\n",
            "Batch 64/72 | Loss: 5.5789 | Batch Acc: 0.1875\n",
            "Batch 65/72 | Loss: 5.4572 | Batch Acc: 0.3125\n",
            "Batch 66/72 | Loss: 5.9668 | Batch Acc: 0.1875\n",
            "Batch 67/72 | Loss: 4.4872 | Batch Acc: 0.3750\n",
            "Batch 68/72 | Loss: 6.3777 | Batch Acc: 0.1250\n",
            "Batch 69/72 | Loss: 4.7889 | Batch Acc: 0.2500\n",
            "Batch 70/72 | Loss: 5.7105 | Batch Acc: 0.3125\n",
            "Batch 71/72 | Loss: 6.3550 | Batch Acc: 0.3125\n",
            "Batch 72/72 | Loss: 3.6024 | Batch Acc: 0.4615\n",
            "Epoch Train Loss: 374.8308, Accuracy: 0.2689\n",
            "Validation Accuracy: 0.3523\n",
            "\n",
            "Epoch 30/50\n",
            "Batch 1/72 | Loss: 5.1696 | Batch Acc: 0.3125\n",
            "Batch 2/72 | Loss: 3.7034 | Batch Acc: 0.3125\n",
            "Batch 3/72 | Loss: 4.5011 | Batch Acc: 0.3750\n",
            "Batch 4/72 | Loss: 5.5121 | Batch Acc: 0.1875\n",
            "Batch 5/72 | Loss: 5.2503 | Batch Acc: 0.1875\n",
            "Batch 6/72 | Loss: 5.6257 | Batch Acc: 0.0625\n",
            "Batch 7/72 | Loss: 4.6388 | Batch Acc: 0.2500\n",
            "Batch 8/72 | Loss: 4.6101 | Batch Acc: 0.2500\n",
            "Batch 9/72 | Loss: 5.2196 | Batch Acc: 0.1250\n",
            "Batch 10/72 | Loss: 5.3225 | Batch Acc: 0.1250\n",
            "Batch 11/72 | Loss: 4.5721 | Batch Acc: 0.2500\n",
            "Batch 12/72 | Loss: 4.8423 | Batch Acc: 0.3750\n",
            "Batch 13/72 | Loss: 6.0179 | Batch Acc: 0.2500\n",
            "Batch 14/72 | Loss: 6.3484 | Batch Acc: 0.1875\n",
            "Batch 15/72 | Loss: 5.6977 | Batch Acc: 0.1875\n",
            "Batch 16/72 | Loss: 5.4065 | Batch Acc: 0.1875\n",
            "Batch 17/72 | Loss: 4.9637 | Batch Acc: 0.1875\n",
            "Batch 18/72 | Loss: 4.3571 | Batch Acc: 0.5000\n",
            "Batch 19/72 | Loss: 5.9923 | Batch Acc: 0.3125\n",
            "Batch 20/72 | Loss: 7.0070 | Batch Acc: 0.1250\n",
            "Batch 21/72 | Loss: 4.4325 | Batch Acc: 0.3750\n",
            "Batch 22/72 | Loss: 4.5999 | Batch Acc: 0.4375\n",
            "Batch 23/72 | Loss: 4.9243 | Batch Acc: 0.3750\n",
            "Batch 24/72 | Loss: 4.9121 | Batch Acc: 0.3750\n",
            "Batch 25/72 | Loss: 5.4875 | Batch Acc: 0.1875\n",
            "Batch 26/72 | Loss: 5.3089 | Batch Acc: 0.2500\n",
            "Batch 27/72 | Loss: 4.8704 | Batch Acc: 0.3125\n",
            "Batch 28/72 | Loss: 4.8076 | Batch Acc: 0.3125\n",
            "Batch 29/72 | Loss: 4.0307 | Batch Acc: 0.4375\n",
            "Batch 30/72 | Loss: 4.4265 | Batch Acc: 0.5625\n",
            "Batch 31/72 | Loss: 4.8719 | Batch Acc: 0.3125\n",
            "Batch 32/72 | Loss: 3.7388 | Batch Acc: 0.5625\n",
            "Batch 33/72 | Loss: 4.4896 | Batch Acc: 0.3125\n",
            "Batch 34/72 | Loss: 4.4527 | Batch Acc: 0.4375\n",
            "Batch 35/72 | Loss: 6.4511 | Batch Acc: 0.0625\n",
            "Batch 36/72 | Loss: 5.8004 | Batch Acc: 0.3125\n",
            "Batch 37/72 | Loss: 4.3357 | Batch Acc: 0.2500\n",
            "Batch 38/72 | Loss: 5.5809 | Batch Acc: 0.3750\n",
            "Batch 39/72 | Loss: 5.1138 | Batch Acc: 0.3750\n",
            "Batch 40/72 | Loss: 6.1498 | Batch Acc: 0.2500\n",
            "Batch 41/72 | Loss: 4.0042 | Batch Acc: 0.4375\n",
            "Batch 42/72 | Loss: 4.9474 | Batch Acc: 0.4375\n",
            "Batch 43/72 | Loss: 4.0859 | Batch Acc: 0.3750\n",
            "Batch 44/72 | Loss: 4.7495 | Batch Acc: 0.2500\n",
            "Batch 45/72 | Loss: 6.9471 | Batch Acc: 0.1250\n",
            "Batch 46/72 | Loss: 5.6136 | Batch Acc: 0.2500\n",
            "Batch 47/72 | Loss: 4.7818 | Batch Acc: 0.3750\n",
            "Batch 48/72 | Loss: 4.7793 | Batch Acc: 0.2500\n",
            "Batch 49/72 | Loss: 5.5652 | Batch Acc: 0.1250\n",
            "Batch 50/72 | Loss: 5.4174 | Batch Acc: 0.2500\n",
            "Batch 51/72 | Loss: 6.3154 | Batch Acc: 0.3125\n",
            "Batch 52/72 | Loss: 4.3134 | Batch Acc: 0.3750\n",
            "Batch 53/72 | Loss: 4.9249 | Batch Acc: 0.3125\n",
            "Batch 54/72 | Loss: 6.1828 | Batch Acc: 0.1250\n",
            "Batch 55/72 | Loss: 5.2708 | Batch Acc: 0.3125\n",
            "Batch 56/72 | Loss: 5.7501 | Batch Acc: 0.1875\n",
            "Batch 57/72 | Loss: 5.1842 | Batch Acc: 0.3125\n",
            "Batch 58/72 | Loss: 4.8712 | Batch Acc: 0.4375\n",
            "Batch 59/72 | Loss: 4.0289 | Batch Acc: 0.5000\n",
            "Batch 60/72 | Loss: 6.5724 | Batch Acc: 0.1250\n",
            "Batch 61/72 | Loss: 4.7233 | Batch Acc: 0.2500\n",
            "Batch 62/72 | Loss: 5.9624 | Batch Acc: 0.2500\n",
            "Batch 63/72 | Loss: 4.8843 | Batch Acc: 0.4375\n",
            "Batch 64/72 | Loss: 5.4962 | Batch Acc: 0.0000\n",
            "Batch 65/72 | Loss: 5.1428 | Batch Acc: 0.1250\n",
            "Batch 66/72 | Loss: 5.4845 | Batch Acc: 0.3125\n",
            "Batch 67/72 | Loss: 5.3581 | Batch Acc: 0.1250\n",
            "Batch 68/72 | Loss: 6.5469 | Batch Acc: 0.1875\n",
            "Batch 69/72 | Loss: 4.6475 | Batch Acc: 0.4375\n",
            "Batch 70/72 | Loss: 5.4105 | Batch Acc: 0.1250\n",
            "Batch 71/72 | Loss: 5.6715 | Batch Acc: 0.1875\n",
            "Batch 72/72 | Loss: 5.3843 | Batch Acc: 0.2308\n",
            "Epoch Train Loss: 372.5611, Accuracy: 0.2802\n",
            "Validation Accuracy: 0.3466\n",
            "\n",
            "Epoch 31/50\n",
            "Batch 1/72 | Loss: 5.0880 | Batch Acc: 0.3125\n",
            "Batch 2/72 | Loss: 5.4288 | Batch Acc: 0.3125\n",
            "Batch 3/72 | Loss: 5.1182 | Batch Acc: 0.2500\n",
            "Batch 4/72 | Loss: 5.0814 | Batch Acc: 0.2500\n",
            "Batch 5/72 | Loss: 5.0698 | Batch Acc: 0.3750\n",
            "Batch 6/72 | Loss: 5.4774 | Batch Acc: 0.3125\n",
            "Batch 7/72 | Loss: 5.1234 | Batch Acc: 0.2500\n",
            "Batch 8/72 | Loss: 5.1518 | Batch Acc: 0.1875\n",
            "Batch 9/72 | Loss: 5.9021 | Batch Acc: 0.2500\n",
            "Batch 10/72 | Loss: 5.5476 | Batch Acc: 0.3125\n",
            "Batch 11/72 | Loss: 5.3765 | Batch Acc: 0.3750\n",
            "Batch 12/72 | Loss: 5.3776 | Batch Acc: 0.0625\n",
            "Batch 13/72 | Loss: 5.7647 | Batch Acc: 0.1250\n",
            "Batch 14/72 | Loss: 4.4002 | Batch Acc: 0.5000\n",
            "Batch 15/72 | Loss: 5.5646 | Batch Acc: 0.1875\n",
            "Batch 16/72 | Loss: 4.9810 | Batch Acc: 0.3750\n",
            "Batch 17/72 | Loss: 6.0550 | Batch Acc: 0.2500\n",
            "Batch 18/72 | Loss: 4.6985 | Batch Acc: 0.2500\n",
            "Batch 19/72 | Loss: 5.4091 | Batch Acc: 0.1250\n",
            "Batch 20/72 | Loss: 5.2869 | Batch Acc: 0.1250\n",
            "Batch 21/72 | Loss: 5.8377 | Batch Acc: 0.1875\n",
            "Batch 22/72 | Loss: 6.4349 | Batch Acc: 0.3750\n",
            "Batch 23/72 | Loss: 4.9186 | Batch Acc: 0.1875\n",
            "Batch 24/72 | Loss: 5.6520 | Batch Acc: 0.2500\n",
            "Batch 25/72 | Loss: 4.1119 | Batch Acc: 0.3750\n",
            "Batch 26/72 | Loss: 4.7683 | Batch Acc: 0.2500\n",
            "Batch 27/72 | Loss: 5.3503 | Batch Acc: 0.2500\n",
            "Batch 28/72 | Loss: 5.3636 | Batch Acc: 0.3125\n",
            "Batch 29/72 | Loss: 5.2673 | Batch Acc: 0.1875\n",
            "Batch 30/72 | Loss: 3.8532 | Batch Acc: 0.4375\n",
            "Batch 31/72 | Loss: 6.6246 | Batch Acc: 0.1250\n",
            "Batch 32/72 | Loss: 5.0854 | Batch Acc: 0.3750\n",
            "Batch 33/72 | Loss: 4.3057 | Batch Acc: 0.3750\n",
            "Batch 34/72 | Loss: 4.7664 | Batch Acc: 0.1875\n",
            "Batch 35/72 | Loss: 5.4470 | Batch Acc: 0.3750\n",
            "Batch 36/72 | Loss: 5.4789 | Batch Acc: 0.1875\n",
            "Batch 37/72 | Loss: 5.4592 | Batch Acc: 0.1250\n",
            "Batch 38/72 | Loss: 5.3962 | Batch Acc: 0.1875\n",
            "Batch 39/72 | Loss: 4.3322 | Batch Acc: 0.3125\n",
            "Batch 40/72 | Loss: 5.5211 | Batch Acc: 0.3125\n",
            "Batch 41/72 | Loss: 5.0644 | Batch Acc: 0.2500\n",
            "Batch 42/72 | Loss: 6.3300 | Batch Acc: 0.0625\n",
            "Batch 43/72 | Loss: 5.2929 | Batch Acc: 0.3750\n",
            "Batch 44/72 | Loss: 3.9499 | Batch Acc: 0.3750\n",
            "Batch 45/72 | Loss: 4.9615 | Batch Acc: 0.2500\n",
            "Batch 46/72 | Loss: 5.5194 | Batch Acc: 0.4375\n",
            "Batch 47/72 | Loss: 5.3180 | Batch Acc: 0.1875\n",
            "Batch 48/72 | Loss: 4.2572 | Batch Acc: 0.4375\n",
            "Batch 49/72 | Loss: 4.6700 | Batch Acc: 0.3750\n",
            "Batch 50/72 | Loss: 4.8228 | Batch Acc: 0.3750\n",
            "Batch 51/72 | Loss: 5.2118 | Batch Acc: 0.3125\n",
            "Batch 52/72 | Loss: 5.2243 | Batch Acc: 0.3125\n",
            "Batch 53/72 | Loss: 4.8549 | Batch Acc: 0.5000\n",
            "Batch 54/72 | Loss: 4.9794 | Batch Acc: 0.4375\n",
            "Batch 55/72 | Loss: 4.2578 | Batch Acc: 0.3125\n",
            "Batch 56/72 | Loss: 4.8138 | Batch Acc: 0.3125\n",
            "Batch 57/72 | Loss: 6.5483 | Batch Acc: 0.1250\n",
            "Batch 58/72 | Loss: 5.6690 | Batch Acc: 0.1250\n",
            "Batch 59/72 | Loss: 6.0792 | Batch Acc: 0.3125\n",
            "Batch 60/72 | Loss: 5.1162 | Batch Acc: 0.3125\n",
            "Batch 61/72 | Loss: 5.2278 | Batch Acc: 0.3125\n",
            "Batch 62/72 | Loss: 4.4037 | Batch Acc: 0.3125\n",
            "Batch 63/72 | Loss: 4.3818 | Batch Acc: 0.1875\n",
            "Batch 64/72 | Loss: 6.0346 | Batch Acc: 0.0625\n",
            "Batch 65/72 | Loss: 5.7600 | Batch Acc: 0.0625\n",
            "Batch 66/72 | Loss: 5.4102 | Batch Acc: 0.3125\n",
            "Batch 67/72 | Loss: 5.0621 | Batch Acc: 0.3750\n",
            "Batch 68/72 | Loss: 5.3371 | Batch Acc: 0.1875\n",
            "Batch 69/72 | Loss: 5.1086 | Batch Acc: 0.2500\n",
            "Batch 70/72 | Loss: 5.1491 | Batch Acc: 0.3125\n",
            "Batch 71/72 | Loss: 3.9326 | Batch Acc: 0.6250\n",
            "Batch 72/72 | Loss: 4.6934 | Batch Acc: 0.4615\n",
            "Epoch Train Loss: 373.3190, Accuracy: 0.2802\n",
            "Validation Accuracy: 0.2955\n",
            "‚èπÔ∏è Ëß∏Áôº Early Stopping\n",
            "Validation Accuracy: 0.3295\n",
            "\n",
            "Final Test Accuracy: 0.3295\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Ë®ìÁ∑¥ÊµÅÁ®ã\n",
        "best_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "    train_one_epoch_amp(model, train_loader)\n",
        "    val_acc = evaluate(model, val_loader)\n",
        "    # print(\"validation accuracy: \", val_acc)\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        print(f\"‚úÖ ÂÑ≤Â≠òÊúÄ‰Ω≥Ê®°ÂûãÔºàAcc: {best_acc:.4f}Ôºâ\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= early_stop_patience:\n",
        "            print(\"‚èπÔ∏è Ëß∏Áôº Early Stopping\")\n",
        "            break\n",
        "\n",
        "test_acc = evaluate(model, test_loader)\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRZynCkG5A_c",
        "outputId": "cece6b91-206d-497a-c458-68a3664983b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.3295\n",
            "\n",
            "Final Test Accuracy: 0.3295\n"
          ]
        }
      ],
      "source": [
        "test_acc = evaluate(model, test_loader)\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcFN_o-nhoXj",
        "outputId": "fcc60241-98c4-4818-ae1f-fc16a99be3ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PatchGNNNet(\n",
              "  (stem): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): Mish()\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): Mish()\n",
              "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (9): Mish()\n",
              "    (10): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (12): Mish()\n",
              "  )\n",
              "  (gc_in): Linear(in_features=128, out_features=256, bias=False)\n",
              "  (gc_nei): Linear(in_features=128, out_features=256, bias=False)\n",
              "  (gc_act): Mish()\n",
              "  (mlp_head): Sequential(\n",
              "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (1): Mish()\n",
              "    (2): Dropout(p=0.3, inplace=False)\n",
              "    (3): Linear(in_features=256, out_features=22, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Step 8: ËºâÂÖ•ÊúÄ‰Ω≥Ê®°ÂûãÔºàÊé®Ë´ñÂâçÔºâ\n",
        "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSdGpwm95A_d",
        "outputId": "e8742d7c-84f9-4fb7-814c-26307fb0ab02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node 1: 0.0444\n",
            "Node 2: 0.1711\n",
            "Node 3: 0.0841\n",
            "Node 4: 0.0410\n",
            "Node 5: 0.0562\n",
            "Node 6: 0.0062\n",
            "Node 7: 0.1582\n",
            "Node 8: 0.0930\n",
            "Node 9: 0.0484\n",
            "Node 10: 0.0333\n",
            "Node 11: 0.0445\n",
            "Node 12: 0.0031\n",
            "Node 13: 0.0734\n",
            "Node 14: 0.0353\n",
            "Node 15: 0.0077\n",
            "Node 16: 0.0038\n",
            "Node 17: 0.0027\n",
            "Node 18: 0.0017\n",
            "Node 19: 0.0045\n",
            "Node 20: 0.0068\n",
            "Node 21: 0.0030\n",
            "Node 22: 0.0776\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Load model\n",
        "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# Load and preprocess image\n",
        "img_path = \"/content/drive/MyDrive/DLA_term_project_data/classified_data/test/node_15/IMG_4393.jpg\"\n",
        "img = Image.open(img_path).convert(\"RGB\")\n",
        "input_tensor = test_transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "# Predict\n",
        "with torch.no_grad():\n",
        "    logits = model(input_tensor)\n",
        "    probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "\n",
        "# Print probabilities\n",
        "for idx, prob in enumerate(probs):\n",
        "    print(f\"Node {idx+1}: {prob:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMyafEEN5A_d",
        "outputId": "f900a6e9-11d4-4574-940b-11cc2e3ae07c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 1: True Node=1, Predicted Node=14 ‚ùå\n",
            "Sample 2: True Node=1, Predicted Node=2 ‚ùå\n",
            "Sample 3: True Node=1, Predicted Node=14 ‚ùå\n",
            "Sample 4: True Node=1, Predicted Node=16 ‚ùå\n",
            "Sample 5: True Node=1, Predicted Node=14 ‚ùå\n",
            "Sample 6: True Node=1, Predicted Node=14 ‚ùå\n",
            "Sample 7: True Node=1, Predicted Node=14 ‚ùå\n",
            "Sample 8: True Node=1, Predicted Node=14 ‚ùå\n",
            "Sample 9: True Node=2, Predicted Node=2 ‚úÖ\n",
            "Sample 10: True Node=2, Predicted Node=2 ‚úÖ\n",
            "Sample 11: True Node=2, Predicted Node=19 ‚ùå\n",
            "Sample 12: True Node=2, Predicted Node=20 ‚ùå\n",
            "Sample 13: True Node=2, Predicted Node=2 ‚úÖ\n",
            "Sample 14: True Node=2, Predicted Node=2 ‚úÖ\n",
            "Sample 15: True Node=2, Predicted Node=2 ‚úÖ\n",
            "Sample 16: True Node=2, Predicted Node=2 ‚úÖ\n",
            "Sample 17: True Node=3, Predicted Node=2 ‚ùå\n",
            "Sample 18: True Node=3, Predicted Node=8 ‚ùå\n",
            "Sample 19: True Node=3, Predicted Node=19 ‚ùå\n",
            "Sample 20: True Node=3, Predicted Node=15 ‚ùå\n",
            "Sample 21: True Node=3, Predicted Node=9 ‚ùå\n",
            "Sample 22: True Node=3, Predicted Node=5 ‚ùå\n",
            "Sample 23: True Node=3, Predicted Node=8 ‚ùå\n",
            "Sample 24: True Node=3, Predicted Node=4 ‚ùå\n",
            "Sample 25: True Node=4, Predicted Node=10 ‚ùå\n",
            "Sample 26: True Node=4, Predicted Node=4 ‚úÖ\n",
            "Sample 27: True Node=4, Predicted Node=7 ‚ùå\n",
            "Sample 28: True Node=4, Predicted Node=6 ‚ùå\n",
            "Sample 29: True Node=4, Predicted Node=19 ‚ùå\n",
            "Sample 30: True Node=4, Predicted Node=4 ‚úÖ\n",
            "Sample 31: True Node=4, Predicted Node=10 ‚ùå\n",
            "Sample 32: True Node=4, Predicted Node=4 ‚úÖ\n",
            "Sample 33: True Node=5, Predicted Node=4 ‚ùå\n",
            "Sample 34: True Node=5, Predicted Node=4 ‚ùå\n",
            "Sample 35: True Node=5, Predicted Node=5 ‚úÖ\n",
            "Sample 36: True Node=5, Predicted Node=5 ‚úÖ\n",
            "Sample 37: True Node=5, Predicted Node=5 ‚úÖ\n",
            "Sample 38: True Node=5, Predicted Node=4 ‚ùå\n",
            "Sample 39: True Node=5, Predicted Node=5 ‚úÖ\n",
            "Sample 40: True Node=5, Predicted Node=4 ‚ùå\n",
            "Sample 41: True Node=6, Predicted Node=21 ‚ùå\n",
            "Sample 42: True Node=6, Predicted Node=18 ‚ùå\n",
            "Sample 43: True Node=6, Predicted Node=21 ‚ùå\n",
            "Sample 44: True Node=6, Predicted Node=21 ‚ùå\n",
            "Sample 45: True Node=6, Predicted Node=6 ‚úÖ\n",
            "Sample 46: True Node=6, Predicted Node=21 ‚ùå\n",
            "Sample 47: True Node=6, Predicted Node=21 ‚ùå\n",
            "Sample 48: True Node=6, Predicted Node=21 ‚ùå\n",
            "Sample 49: True Node=7, Predicted Node=7 ‚úÖ\n",
            "Sample 50: True Node=7, Predicted Node=9 ‚ùå\n",
            "Sample 51: True Node=7, Predicted Node=5 ‚ùå\n",
            "Sample 52: True Node=7, Predicted Node=2 ‚ùå\n",
            "Sample 53: True Node=7, Predicted Node=7 ‚úÖ\n",
            "Sample 54: True Node=7, Predicted Node=9 ‚ùå\n",
            "Sample 55: True Node=7, Predicted Node=7 ‚úÖ\n",
            "Sample 56: True Node=7, Predicted Node=8 ‚ùå\n",
            "Sample 57: True Node=8, Predicted Node=8 ‚úÖ\n",
            "Sample 58: True Node=8, Predicted Node=8 ‚úÖ\n",
            "Sample 59: True Node=8, Predicted Node=8 ‚úÖ\n",
            "Sample 60: True Node=8, Predicted Node=8 ‚úÖ\n",
            "Sample 61: True Node=8, Predicted Node=5 ‚ùå\n",
            "Sample 62: True Node=8, Predicted Node=8 ‚úÖ\n",
            "Sample 63: True Node=8, Predicted Node=8 ‚úÖ\n",
            "Sample 64: True Node=8, Predicted Node=8 ‚úÖ\n",
            "Sample 65: True Node=9, Predicted Node=2 ‚ùå\n",
            "Sample 66: True Node=9, Predicted Node=9 ‚úÖ\n",
            "Sample 67: True Node=9, Predicted Node=7 ‚ùå\n",
            "Sample 68: True Node=9, Predicted Node=2 ‚ùå\n",
            "Sample 69: True Node=9, Predicted Node=2 ‚ùå\n",
            "Sample 70: True Node=9, Predicted Node=14 ‚ùå\n",
            "Sample 71: True Node=9, Predicted Node=2 ‚ùå\n",
            "Sample 72: True Node=9, Predicted Node=2 ‚ùå\n",
            "Sample 73: True Node=10, Predicted Node=10 ‚úÖ\n",
            "Sample 74: True Node=10, Predicted Node=10 ‚úÖ\n",
            "Sample 75: True Node=10, Predicted Node=10 ‚úÖ\n",
            "Sample 76: True Node=10, Predicted Node=19 ‚ùå\n",
            "Sample 77: True Node=10, Predicted Node=14 ‚ùå\n",
            "Sample 78: True Node=10, Predicted Node=10 ‚úÖ\n",
            "Sample 79: True Node=10, Predicted Node=14 ‚ùå\n",
            "Sample 80: True Node=10, Predicted Node=14 ‚ùå\n",
            "Sample 81: True Node=11, Predicted Node=10 ‚ùå\n",
            "Sample 82: True Node=11, Predicted Node=10 ‚ùå\n",
            "Sample 83: True Node=11, Predicted Node=2 ‚ùå\n",
            "Sample 84: True Node=11, Predicted Node=13 ‚ùå\n",
            "Sample 85: True Node=11, Predicted Node=10 ‚ùå\n",
            "Sample 86: True Node=11, Predicted Node=10 ‚ùå\n",
            "Sample 87: True Node=11, Predicted Node=10 ‚ùå\n",
            "Sample 88: True Node=11, Predicted Node=13 ‚ùå\n",
            "Sample 89: True Node=12, Predicted Node=18 ‚ùå\n",
            "Sample 90: True Node=12, Predicted Node=18 ‚ùå\n",
            "Sample 91: True Node=12, Predicted Node=18 ‚ùå\n",
            "Sample 92: True Node=12, Predicted Node=19 ‚ùå\n",
            "Sample 93: True Node=12, Predicted Node=18 ‚ùå\n",
            "Sample 94: True Node=12, Predicted Node=18 ‚ùå\n",
            "Sample 95: True Node=12, Predicted Node=18 ‚ùå\n",
            "Sample 96: True Node=12, Predicted Node=18 ‚ùå\n",
            "Sample 97: True Node=13, Predicted Node=9 ‚ùå\n",
            "Sample 98: True Node=13, Predicted Node=18 ‚ùå\n",
            "Sample 99: True Node=13, Predicted Node=20 ‚ùå\n",
            "Sample 100: True Node=13, Predicted Node=2 ‚ùå\n",
            "Sample 101: True Node=13, Predicted Node=18 ‚ùå\n",
            "Sample 102: True Node=13, Predicted Node=14 ‚ùå\n",
            "Sample 103: True Node=13, Predicted Node=2 ‚ùå\n",
            "Sample 104: True Node=13, Predicted Node=13 ‚úÖ\n",
            "Sample 105: True Node=14, Predicted Node=2 ‚ùå\n",
            "Sample 106: True Node=14, Predicted Node=14 ‚úÖ\n",
            "Sample 107: True Node=14, Predicted Node=14 ‚úÖ\n",
            "Sample 108: True Node=14, Predicted Node=14 ‚úÖ\n",
            "Sample 109: True Node=14, Predicted Node=14 ‚úÖ\n",
            "Sample 110: True Node=14, Predicted Node=14 ‚úÖ\n",
            "Sample 111: True Node=14, Predicted Node=14 ‚úÖ\n",
            "Sample 112: True Node=14, Predicted Node=14 ‚úÖ\n",
            "Sample 113: True Node=15, Predicted Node=19 ‚ùå\n",
            "Sample 114: True Node=15, Predicted Node=15 ‚úÖ\n",
            "Sample 115: True Node=15, Predicted Node=6 ‚ùå\n",
            "Sample 116: True Node=15, Predicted Node=15 ‚úÖ\n",
            "Sample 117: True Node=15, Predicted Node=6 ‚ùå\n",
            "Sample 118: True Node=15, Predicted Node=22 ‚ùå\n",
            "Sample 119: True Node=15, Predicted Node=5 ‚ùå\n",
            "Sample 120: True Node=15, Predicted Node=22 ‚ùå\n",
            "Sample 121: True Node=16, Predicted Node=21 ‚ùå\n",
            "Sample 122: True Node=16, Predicted Node=18 ‚ùå\n",
            "Sample 123: True Node=16, Predicted Node=20 ‚ùå\n",
            "Sample 124: True Node=16, Predicted Node=19 ‚ùå\n",
            "Sample 125: True Node=16, Predicted Node=18 ‚ùå\n",
            "Sample 126: True Node=16, Predicted Node=18 ‚ùå\n",
            "Sample 127: True Node=16, Predicted Node=18 ‚ùå\n",
            "Sample 128: True Node=16, Predicted Node=18 ‚ùå\n",
            "Sample 129: True Node=17, Predicted Node=18 ‚ùå\n",
            "Sample 130: True Node=17, Predicted Node=18 ‚ùå\n",
            "Sample 131: True Node=17, Predicted Node=19 ‚ùå\n",
            "Sample 132: True Node=17, Predicted Node=19 ‚ùå\n",
            "Sample 133: True Node=17, Predicted Node=18 ‚ùå\n",
            "Sample 134: True Node=17, Predicted Node=18 ‚ùå\n",
            "Sample 135: True Node=17, Predicted Node=18 ‚ùå\n",
            "Sample 136: True Node=17, Predicted Node=19 ‚ùå\n",
            "Sample 137: True Node=18, Predicted Node=18 ‚úÖ\n",
            "Sample 138: True Node=18, Predicted Node=18 ‚úÖ\n",
            "Sample 139: True Node=18, Predicted Node=19 ‚ùå\n",
            "Sample 140: True Node=18, Predicted Node=18 ‚úÖ\n",
            "Sample 141: True Node=18, Predicted Node=18 ‚úÖ\n",
            "Sample 142: True Node=18, Predicted Node=18 ‚úÖ\n",
            "Sample 143: True Node=18, Predicted Node=18 ‚úÖ\n",
            "Sample 144: True Node=18, Predicted Node=19 ‚ùå\n",
            "Sample 145: True Node=19, Predicted Node=18 ‚ùå\n",
            "Sample 146: True Node=19, Predicted Node=18 ‚ùå\n",
            "Sample 147: True Node=19, Predicted Node=19 ‚úÖ\n",
            "Sample 148: True Node=19, Predicted Node=19 ‚úÖ\n",
            "Sample 149: True Node=19, Predicted Node=18 ‚ùå\n",
            "Sample 150: True Node=19, Predicted Node=18 ‚ùå\n",
            "Sample 151: True Node=19, Predicted Node=18 ‚ùå\n",
            "Sample 152: True Node=19, Predicted Node=18 ‚ùå\n",
            "Sample 153: True Node=20, Predicted Node=19 ‚ùå\n",
            "Sample 154: True Node=20, Predicted Node=18 ‚ùå\n",
            "Sample 155: True Node=20, Predicted Node=19 ‚ùå\n",
            "Sample 156: True Node=20, Predicted Node=19 ‚ùå\n",
            "Sample 157: True Node=20, Predicted Node=18 ‚ùå\n",
            "Sample 158: True Node=20, Predicted Node=18 ‚ùå\n",
            "Sample 159: True Node=20, Predicted Node=18 ‚ùå\n",
            "Sample 160: True Node=20, Predicted Node=18 ‚ùå\n",
            "Sample 161: True Node=21, Predicted Node=21 ‚úÖ\n",
            "Sample 162: True Node=21, Predicted Node=21 ‚úÖ\n",
            "Sample 163: True Node=21, Predicted Node=21 ‚úÖ\n",
            "Sample 164: True Node=21, Predicted Node=21 ‚úÖ\n",
            "Sample 165: True Node=21, Predicted Node=22 ‚ùå\n",
            "Sample 166: True Node=21, Predicted Node=21 ‚úÖ\n",
            "Sample 167: True Node=21, Predicted Node=21 ‚úÖ\n",
            "Sample 168: True Node=21, Predicted Node=21 ‚úÖ\n",
            "Sample 169: True Node=22, Predicted Node=14 ‚ùå\n",
            "Sample 170: True Node=22, Predicted Node=22 ‚úÖ\n",
            "Sample 171: True Node=22, Predicted Node=21 ‚ùå\n",
            "Sample 172: True Node=22, Predicted Node=14 ‚ùå\n",
            "Sample 173: True Node=22, Predicted Node=19 ‚ùå\n",
            "Sample 174: True Node=22, Predicted Node=2 ‚ùå\n",
            "Sample 175: True Node=22, Predicted Node=14 ‚ùå\n",
            "Sample 176: True Node=22, Predicted Node=19 ‚ùå\n",
            "\n",
            "Misclassification count per node (True label):\n",
            "Node 1: 8\n",
            "Node 2: 2\n",
            "Node 3: 8\n",
            "Node 4: 5\n",
            "Node 5: 4\n",
            "Node 6: 7\n",
            "Node 7: 5\n",
            "Node 8: 1\n",
            "Node 9: 7\n",
            "Node 10: 4\n",
            "Node 11: 8\n",
            "Node 12: 8\n",
            "Node 13: 7\n",
            "Node 14: 1\n",
            "Node 15: 6\n",
            "Node 16: 8\n",
            "Node 17: 8\n",
            "Node 18: 2\n",
            "Node 19: 6\n",
            "Node 20: 8\n",
            "Node 21: 1\n",
            "Node 22: 7\n",
            "\n",
            "Node most often misclassified: Node 1 (8 times)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        preds = torch.argmax(probs, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Print predictions and ground truths\n",
        "for i, (pred, label) in enumerate(zip(all_preds, all_labels)):\n",
        "    correct = \"‚úÖ\" if pred == label else \"‚ùå\"\n",
        "    print(f\"Sample {i+1}: True Node={label+1}, Predicted Node={pred+1} {correct}\")\n",
        "\n",
        "# Count misclassifications per node\n",
        "misclass_counts = Counter()\n",
        "for pred, label in zip(all_preds, all_labels):\n",
        "    if pred != label:\n",
        "        misclass_counts[label+1] += 1  # +1 for 1-based node index\n",
        "\n",
        "print(\"\\nMisclassification count per node (True label):\")\n",
        "for node in range(1, len(test_loader.dataset.classes)+1):\n",
        "    print(f\"Node {node}: {misclass_counts[node]}\")\n",
        "\n",
        "# Find the most misclassified node(s)\n",
        "if misclass_counts:\n",
        "    most_misclassified = misclass_counts.most_common(1)[0]\n",
        "    print(f\"\\nNode most often misclassified: Node {most_misclassified[0]} ({most_misclassified[1]} times)\")\n",
        "else:\n",
        "    print(\"\\nNo misclassifications!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
