{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pau5RAGu5JFd",
        "outputId": "e02aa444-b9b6-4fcf-aa95-24a5c2700e1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#在Colab用\n",
        "!pip install -Uq timm==0.9.12\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbxBw8HI5RzE",
        "outputId": "a7706a47-74ab-426c-e10e-02638892a633"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#授權\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9EKp_bKGhoXg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import torch.nn.functional as F\n",
        "import timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "58oc3dAUQQAB"
      },
      "outputs": [],
      "source": [
        "num_epochs = 50\n",
        "batch_size = 16\n",
        "early_stop_patience = 4\n",
        "patience_counter = 0\n",
        "loss_alpha=1.0\n",
        "loss_beta=0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bS_YldbqhoXg",
        "outputId": "80ea742f-7ad3-436f-cda8-6078fb08021f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "類別數量： 22\n"
          ]
        }
      ],
      "source": [
        "base_dir = \"/content/drive/MyDrive/DLA_term_project_data/classified_data\"\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "imagenet_stats = [(0.485, 0.456, 0.406), (0.229, 0.224, 0.225)]\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(imagenet_stats[0], imagenet_stats[1])\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "validation_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(imagenet_stats[0], imagenet_stats[1])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(imagenet_stats[0], imagenet_stats[1])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(os.path.join(base_dir, 'train'), transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(os.path.join(base_dir, 'validation'), transform=validation_transform)\n",
        "test_data = datasets.ImageFolder(os.path.join(base_dir, \"test\"), transform=test_transform)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"類別數量：\", len(train_dataset.classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "a1xpCePFPJef"
      },
      "outputs": [],
      "source": [
        "# customed loss function\n",
        "# ----- 1. Node-to-Node Distance Matrix -----\n",
        "node_distance_matrix = torch.tensor([\n",
        "    [0,1,2,2,1,2,2,2.5,3.5,4,5.5,5,4,2.5,3.5,4,5,3.5,4.5,3.5,2.5,2.5],\n",
        "    [1,0,1,3,2,3,1,1.5,2.5,3,4.5,4,3,1.5,4.5,5,6,4.5,5.5,4.5,3.5,3.5],\n",
        "    [2,1,0,2,3,4,2,2.5,3.5,4,5.5,3,2,0.5,3.5,4,5,3.5,4.5,5.5,4.5,2.5],\n",
        "    [2,3,2,0,1,2,4,4.5,5.5,6,7.5,5,4,2.5,1.5,2,3,1.5,2.5,3.5,2.5,0.5],\n",
        "    [1,2,3,1,0,1,3,3.5,4.5,5,6.5,6,5,3.5,2.5,3,4,2.5,3.5,2.5,1.5,1.5],\n",
        "    [2,3,4,2,1,0,2,2.5,3.5,4,5.5,7,6,4.5,3.5,4,5,3.5,2.5,1.5,0.5,2.5],\n",
        "    [2,1,2,4,3,2,0,0.5,1.5,2,3.5,5,4,2.5,5.5,6,7,5.5,4.5,3.5,2.5,4.5],\n",
        "    [2.5,1.5,2.5,4.5,3.5,2.5,0.5,0,1,1.5,3,3.5,4.5,3,6,6.5,7.5,6,5,4,3,5],\n",
        "    [3.5,2.5,3.5,5.5,4.5,3.5,1.5,1,0,0.5,2,2.5,3.5,4,7,7.5,8.5,7,6,5,4,6],\n",
        "    [4,3,4,6,5,4,2,1.5,0.5,0,1.5,2,3,4.5,7.5,8,9,7.5,6.5,5.5,4.5,6.5],\n",
        "    [5.5,4.5,5.5,7.5,6.5,5.5,3.5,3,2,1.5,0,0.5,1.5,3,7,7.5,8.5,7,8,7,6,6.5],\n",
        "    [5,4,3,5,6,7,5,3.5,2.5,2,0.5,0,1,2.5,6.5,7,8,6.5,7.5,7.5,6.5,5.5],\n",
        "    [4,3,2,4,5,6,4,4.5,3.5,3,1.5,1,0,1.5,5.5,6,7,5.5,6.5,7.5,7,4.5],\n",
        "    [2.5,1.5,0.5,2.5,3.5,4.5,2.5,3,4,4.5,3,2.5,1.5,0,4,4.5,5.5,4,5,6,5.5,3],\n",
        "    [3.5,4.5,3.5,1.5,2.5,3.5,5.5,6,7,7.5,7,6.5,5.5,4,0,0.5,1.5,0.5,1.5,2.5,3.5,1],\n",
        "    [4,5,4,2,3,4,6,6.5,7.5,8,7.5,7,6,4.5,0.5,0,1,1,2,3,4,1.5],\n",
        "    [5,6,5,3,4,5,7,7.5,8.5,9,8.5,8,7,5.5,1.5,1,0,1.5,2.5,3.5,4.5,2.5],\n",
        "    [3.5,4.5,3.5,1.5,2.5,3.5,5.5,6,7,7.5,7,6.5,5.5,4,0.5,1,1.5,0,1,2,3,1],\n",
        "    [4.5,5.5,4.5,2.5,3.5,2.5,4.5,5,6,6.5,8,7.5,6.5,5,1.5,2,2.5,1,0,1,2,2],\n",
        "    [3.5,4.5,5.5,3.5,2.5,1.5,3.5,4,5,5.5,7,7.5,7.5,6,2.5,3,3.5,2,1,0,1,3],\n",
        "    [2.5,3.5,4.5,2.5,1.5,0.5,2.5,3,4,4.5,6,6.5,7,5.5,3.5,4,4.5,3,2,1,0,3],\n",
        "    [2.5,3.5,2.5,0.5,1.5,2.5,4.5,5,6,6.5,6.5,5.5,4.5,3,1,1.5,2.5,1,2,3,3,0]\n",
        "], dtype=torch.float)\n",
        "\n",
        "# ----- 2. Area-to-Area Distance Matrix -----\n",
        "area_distance_matrix = torch.tensor([\n",
        "    [0, 1, 2, 2, 2, 2],\n",
        "    [1, 0, 1, 1, 1, 1],\n",
        "    [2, 1, 0, 1, 3, 3],\n",
        "    [2, 1, 1, 0, 3, 3],\n",
        "    [2, 1, 3, 3, 0, 1],\n",
        "    [2, 1, 3, 3, 1, 0]\n",
        "], dtype=torch.float)\n",
        "\n",
        "# ----- 3. Node-to-Area Mapping -----\n",
        "node_to_area = torch.tensor([\n",
        "    0, 0, 0, 0, 0, 0, 0,    # 1~7 → area 0\n",
        "    1,                      # 8  → area 1\n",
        "    2, 2,                   # 9~10 → area 2\n",
        "    3, 3, 3,                # 11~13 → area 3\n",
        "    1,                      # 14 → area 1\n",
        "    4, 4, 4,                # 15~17 → area 4\n",
        "    5, 5, 5,                # 18~20 → area 5\n",
        "    1,                      # 21 → area 1\n",
        "    1                       # 22 → area 1\n",
        "], dtype=torch.long)\n",
        "\n",
        "# ----- 4. Custom Loss Function -----\n",
        "class DistancePenaltyLoss(nn.Module):\n",
        "    def __init__(self, node_distance_matrix, area_distance_matrix, node_to_area, alpha=1.0, beta=1.0):\n",
        "        super().__init__()\n",
        "        self.node_distance_matrix = node_distance_matrix\n",
        "        self.area_distance_matrix = area_distance_matrix\n",
        "        self.node_to_area = node_to_area\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.ce = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        \"\"\"\n",
        "        logits: (batch_size, 22)\n",
        "        targets: (batch_size,) - ground truth class indices\n",
        "        \"\"\"\n",
        "        ce_loss = self.ce(logits, targets)\n",
        "\n",
        "        probs = F.softmax(logits, dim=1)  # (B, 22)\n",
        "        batch_size = targets.size(0)\n",
        "\n",
        "        node_penalty = 0.0\n",
        "        area_penalty = 0.0\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            target = targets[i]\n",
        "            prob = probs[i]  # shape (22,)\n",
        "\n",
        "            # Node distance penalty\n",
        "            node_distances = self.node_distance_matrix[target]  # (22,)\n",
        "            node_penalty += (prob * node_distances).sum()\n",
        "\n",
        "            # Area distance penalty\n",
        "            target_area = self.node_to_area[target]\n",
        "            pred_areas = self.node_to_area  # shape (22,)\n",
        "            area_dists = self.area_distance_matrix[target_area][pred_areas]  # shape (22,)\n",
        "            area_penalty += (prob * area_dists).sum()\n",
        "\n",
        "        node_penalty /= batch_size\n",
        "        area_penalty /= batch_size\n",
        "\n",
        "        total_loss = ce_loss + self.alpha * node_penalty + self.beta * area_penalty\n",
        "        return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "k5mafh1TQ0pA"
      },
      "outputs": [],
      "source": [
        "# timm.list_models('convnext*')\n",
        "# timm.list_models('convnext*', pretrained=True)\n",
        "# timm.list_models('efficient*', pretrained=True)\n",
        "# timm.list_models('res*', pretrained=True)\n",
        "# timm.list_models('swin*', pretrained=True)\n",
        "\n",
        "# convnextv2_tiny.fcmae\n",
        "# efficientvit_b0.r224_in1k\n",
        "# efficientvit_b1.r224_in1k\n",
        "# efficientvit_b2.r224_in1k\n",
        "# efficientvit_b3.r224_in1k\n",
        "# efficientvit_m0.r224_in1k\n",
        "# efficientvit_m1.r224_in1k\n",
        "# efficientvit_m2.r224_in1k\n",
        "# efficientvit_m3.r224_in1k\n",
        "# efficientvit_m4.r224_in1k\n",
        "# efficientvit_m5.r224_in1k\n",
        "# efficientformerv2_s0.snap_dist_in1k\n",
        "# efficientformerv2_s1.snap_dist_in1k\n",
        "# efficientformerv2_s2.snap_dist_in1k\n",
        "# resnet50.a1_in1k\n",
        "# resnet50d.ra2_in1k\n",
        "# resnet50.fb_swsl_ig1b_ft_in1k\n",
        "# resnet50.tv_in1k\n",
        "# resnet50.ra_in1k\n",
        "# res2net50d.in1k\n",
        "# res2net50_14w_8s.in1k\n",
        "# mobilevitv2_100.cvnets_in1k\n",
        "# mobilevitv2_150.cvnets_in22k_ft_in1k\n",
        "# mobilenetv3_large_100.ra_in1k\n",
        "# mobileone_s1.apple_in1k\n",
        "# mobilenetv2_100.ra_in1k\n",
        "# swin_tiny_patch4_window7_224.ms_in1k\n",
        "# swinv2_tiny_window8_256.ms_in1k\n",
        "# swinv2_cr_tiny_ns_224.sw_in1k\n",
        "\n",
        "\n",
        "\n",
        "# ✅ Overall Best Choices (Balanced: Accuracy, Speed, Size)\n",
        "# mobilevitv2_100.cvnets_in1k\t       Transformer-CNN hybrid, small but very accurate\n",
        "# efficientnet_b0\t             Lightweight, easy to train, great accuracy/size tradeoff\n",
        "# mobilenetv3_large_100.ra_in1k       Very fast, widely supported, good baseline for small datasets\n",
        "# res2net50d.in1k\t             Strong feature extractor, better than plain ResNet50\n",
        "# resnet50d.ra2_in1k             Modern ResNet variant with strong augmentations\n",
        "# swinv2_tiny_window8_256.ms_in1k\t     Lightweight transformer, good if you want to try Swin family\n",
        "\n",
        "# 🧠 If You Want Transfer Learning Strength\n",
        "# mobilevitv2_150.cvnets_in22k_ft_in1k\t  Pretrained on ImageNet-22K → fine-tuned on IN1K\n",
        "# resnet50.fb_swsl_ig1b_ft_in1k\t       Trained on IG-1B dataset — great features even for few-shot\n",
        "\n",
        "# 🚀 If You Want Ultra-Fast Training\n",
        "# mobileone_s1.apple_in1k          \tTiny, deployable, optimized for fast inference\n",
        "# mobilenetv2_100.ra_in1k         \tClassic, efficient, and still performs well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "w4G_OmJF-J1M"
      },
      "outputs": [],
      "source": [
        "# ---------- 100% 手刻 Hybrid CNN + GNN：Patch-GNNNet ----------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PatchGNNNet(nn.Module):\n",
        "    \"\"\"\n",
        "    將輸入影像先透過小型 CNN Stem 提取出一張特徵圖 (C, H, W)，\n",
        "    然後把特徵圖切成 patch_size×patch_size 的小格，每格當作圖上的一個 Node；\n",
        "    接著做一次純手刻 GraphConv（以 patch 間的 8 鄰域為鄰接），\n",
        "    最後做全域平均 + MLP Head 來預測 22 類節點。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=22, in_ch=3, stem_channels=64, patch_size=4, hidden_dim=256, gc_hidden=256):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_classes: 分類節點數 (22)\n",
        "            in_ch: 輸入影像通道 (RGB→3)\n",
        "            stem_channels: CNN Stem 第一層輸出通道數\n",
        "            patch_size:  把特徵圖切成 patch_size×patch_size 的塊\n",
        "            hidden_dim: CNN Stem 收尾通道數 (最後會輸出 (hidden_dim, H, W))\n",
        "            gc_hidden: GNN 隱藏層維度\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # 1) CNN Stem：4 層小型 Conv → BatchNorm → Mish\n",
        "        #    從 (3×224×224) 下採樣到 (hidden_dim, 56, 56) 以下：\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, stem_channels, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(stem_channels),\n",
        "            nn.Mish(),\n",
        "            nn.MaxPool2d(3, 2, 1),  # -> (stem_channels, 56, 56)\n",
        "\n",
        "            nn.Conv2d(stem_channels, stem_channels*2, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(stem_channels*2),\n",
        "            nn.Mish(),              # -> (stem_channels*2, 28, 28)\n",
        "\n",
        "            nn.Conv2d(stem_channels*2, stem_channels*4, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(stem_channels*4),\n",
        "            nn.Mish(),              # -> (stem_channels*4, 14, 14)\n",
        "\n",
        "            nn.Conv2d(stem_channels*4, hidden_dim, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(hidden_dim),\n",
        "            nn.Mish()               # -> (hidden_dim, 14, 14)\n",
        "        )\n",
        "        # 之所以讓 Stem 最終空間大小保持 (14×14)，是因為 patch_size=4 時 (14 % 4 == 2)\n",
        "        # 但這裡我們要 (hidden_dim, 56, 56) / 4 -> (hidden_dim, 14, 14)\n",
        "\n",
        "        # 2) GraphConv 層：單層 GCN-like 更新\n",
        "        #    對每個 patch node，把自己與鄰居 (8 鄰域) feature 做一次融合\n",
        "        self.gc_in = nn.Linear(hidden_dim, gc_hidden, bias=False)   # W_self\n",
        "        self.gc_nei = nn.Linear(hidden_dim, gc_hidden, bias=False)  # W_neigh\n",
        "        self.gc_act = nn.Mish()\n",
        "\n",
        "        # 3) MLP Head：把圖卷積後的 patch features 全域平均，接一個隱藏層 + 分類層\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.Linear(gc_hidden, gc_hidden),\n",
        "            nn.Mish(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(gc_hidden, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (B, 3, 224, 224)  → Stem → (B, hidden_dim, 14, 14)\n",
        "        → 切 Patch → (B, N, hidden_dim)，其中 N = (14/patch_size) * (14/patch_size)\n",
        "          (若 patch_size=2，則 N=49；若 patch_size=4，則 N=12.25(非整數)\n",
        "        → GraphConv → (B, N, gc_hidden)\n",
        "        → 全域 average → (B, gc_hidden) → MLP Head → (B, num_classes)\n",
        "        \"\"\"\n",
        "        B = x.size(0)\n",
        "\n",
        "        # --- 1. CNN Stem ---\n",
        "        f = self.stem(x)  # (B, hidden_dim, 14, 14)\n",
        "\n",
        "        # --- 2. 切 patch: 當作圖上的 Node ---\n",
        "        C, H, W = f.shape[1], f.shape[2], f.shape[3]\n",
        "        p = self.patch_size\n",
        "        assert H % p == 0 and W % p == 0, \"patch_size 必須整除特徵圖大小\"\n",
        "\n",
        "        # 把 f reshape 成 [B, N, C, p, p]，再做平均: (p,p) → 1\n",
        "        f = f.unfold(2, p, p).unfold(3, p, p)\n",
        "        # f: (B, C, H/p, p, W/p, p) → transpose / reshape\n",
        "        f = f.contiguous().view(B, C, H//p, p, W//p, p)\n",
        "        # 先把最後兩個 p×p 維度壓掉：patch_avg = avg_pool  → (B, C, H/p, W/p)\n",
        "        patch_avg = f.mean(dim=3).mean(dim=4)  # (B, C, H/p, W/p)\n",
        "\n",
        "        # 再展平成 (B, N, C)，其中 N = (H/p)*(W/p)\n",
        "        N = (H // p) * (W // p)\n",
        "        patch_feat = patch_avg.view(B, C, N).permute(0, 2, 1)  # (B, N, C)\n",
        "\n",
        "        # --- 3. 純手刻 GraphConv  ---\n",
        "        #   構建 patch 間的 8-neighbor adjacency index\n",
        "        #   先把 (B, N, C) 轉成格點 (B, C, H/p, W/p)，再對每個 node 蒐集 8 鄰域 feature\n",
        "        h = H // p\n",
        "        w = W // p  # h = w = 14/p (p=2 → h=w=7)\n",
        "\n",
        "        # reshape 回 2D Grid\n",
        "        grid = patch_feat.view(B, h, w, C)  # (B, h, w, C)\n",
        "\n",
        "        # 先 pad 一圈 (value=0)，方便取鄰居\n",
        "        pad_grid = F.pad(grid, (0, 0, 1, 1, 1, 1))  # pad top/bottom/left/right → (B, h+2, w+2, C)\n",
        "\n",
        "        # 蒐集 8 鄰域：我們逐方向取 slice\n",
        "        nei_sum = torch.zeros(B, h, w, C, device=x.device)\n",
        "        for dy in [-1, 0, 1]:\n",
        "            for dx in [-1, 0, 1]:\n",
        "                if dy == 0 and dx == 0:\n",
        "                    continue\n",
        "                nei = pad_grid[:, 1+dy : 1+dy+h, 1+dx : 1+dx+w, :]  # (B, h, w, C)\n",
        "                nei_sum += nei\n",
        "        # 平均化（每個 node 最多有 8 個鄰居；邊界少一點，這裡直接除 8 保持簡單）\n",
        "        nei_mean = nei_sum / 8.0  # (B, h, w, C)\n",
        "\n",
        "        # 把自己 & 鄰居 feature 都做一次線性層：\n",
        "        self_feat = self.gc_in(patch_feat)   # (B, N, gc_hidden)\n",
        "        nei_feat  = self.gc_nei(nei_mean.view(B, N, C))  # (B, N, gc_hidden)\n",
        "        gconv = self.gc_act(self_feat + nei_feat)  # (B, N, gc_hidden)\n",
        "\n",
        "        # --- 4. 全球平均 + MLP Head ---\n",
        "        # 先 (B, N, gc_hidden) → (B, gc_hidden) by 平均 over N\n",
        "        graph_feat = gconv.mean(dim=1)  # (B, gc_hidden)\n",
        "\n",
        "        logits = self.mlp_head(graph_feat)  # (B, num_classes)\n",
        "        return logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoGafpvlhoXh",
        "outputId": "c820faf0-73a0-4a9a-f12b-fe74b795db50"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-1734f8dfd10f>:24: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# Step 5: 初始化模型與優化器\n",
        "\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "\n",
        "model = PatchGNNNet(\n",
        "    num_classes=len(train_dataset.classes),  # 22\n",
        "    in_ch=3,                       # 輸入三通道 RGB\n",
        "    stem_channels=64,              # CNN Stem 第一層輸出 64\n",
        "    patch_size=2,                  # 把特徵圖 14×14 切成 2×2 → 7×7=49 nodes\n",
        "    hidden_dim=128,                # CNN Stem 最後輸出通道 128 → patch feature dim\n",
        "    gc_hidden=256                  # GNN 隱藏層維度 256\n",
        ").to(device)\n",
        "\n",
        "\n",
        "criterion = DistancePenaltyLoss(\n",
        "    node_distance_matrix.to(device),\n",
        "    area_distance_matrix.to(device),\n",
        "    node_to_area.to(device),\n",
        "    alpha=loss_alpha,\n",
        "    beta=loss_beta\n",
        ")\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
        "scaler = GradScaler()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EQtCaXoWhoXi"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch_amp(model, loader):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    for batch_idx, (images, labels) in enumerate(loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.autocast(device.type if device.type != \"mps\" else \"cpu\", enabled=(device.type != \"cpu\")):\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        # Print batch details\n",
        "        print(f\"Batch {batch_idx+1}/{len(loader)} | Loss: {loss.item():.4f} | Batch Acc: {(predicted == labels).float().mean().item():.4f}\")\n",
        "    acc = correct / total\n",
        "    print(f\"Epoch Train Loss: {total_loss:.4f}, Accuracy: {acc:.4f}\")\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    acc = correct / total\n",
        "    print(f\"Validation Accuracy: {acc:.4f}\")\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhmyI6EhhoXj",
        "outputId": "051426c4-cb66-414e-ea29-c54dbf8a8f0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/50\n",
            "Batch 1/72 | Loss: 7.1431 | Batch Acc: 0.0000\n",
            "Batch 2/72 | Loss: 7.3595 | Batch Acc: 0.0000\n",
            "Batch 3/72 | Loss: 7.5631 | Batch Acc: 0.0000\n",
            "Batch 4/72 | Loss: 7.3832 | Batch Acc: 0.0625\n",
            "Batch 5/72 | Loss: 7.5407 | Batch Acc: 0.0000\n",
            "Batch 6/72 | Loss: 7.5120 | Batch Acc: 0.0000\n",
            "Batch 7/72 | Loss: 7.5102 | Batch Acc: 0.0625\n",
            "Batch 8/72 | Loss: 7.2032 | Batch Acc: 0.1250\n",
            "Batch 9/72 | Loss: 7.5139 | Batch Acc: 0.0000\n",
            "Batch 10/72 | Loss: 7.4157 | Batch Acc: 0.0625\n",
            "Batch 11/72 | Loss: 7.4605 | Batch Acc: 0.0625\n",
            "Batch 12/72 | Loss: 7.2414 | Batch Acc: 0.0625\n",
            "Batch 13/72 | Loss: 7.5600 | Batch Acc: 0.0000\n",
            "Batch 14/72 | Loss: 7.0495 | Batch Acc: 0.1875\n",
            "Batch 15/72 | Loss: 7.7380 | Batch Acc: 0.0000\n",
            "Batch 16/72 | Loss: 7.6669 | Batch Acc: 0.0000\n",
            "Batch 17/72 | Loss: 7.5044 | Batch Acc: 0.0000\n",
            "Batch 18/72 | Loss: 7.3517 | Batch Acc: 0.0625\n",
            "Batch 19/72 | Loss: 7.1604 | Batch Acc: 0.2500\n",
            "Batch 20/72 | Loss: 7.4905 | Batch Acc: 0.0000\n",
            "Batch 21/72 | Loss: 7.3923 | Batch Acc: 0.1250\n",
            "Batch 22/72 | Loss: 7.6148 | Batch Acc: 0.0625\n",
            "Batch 23/72 | Loss: 7.3136 | Batch Acc: 0.0625\n",
            "Batch 24/72 | Loss: 7.5118 | Batch Acc: 0.1250\n",
            "Batch 25/72 | Loss: 7.3880 | Batch Acc: 0.2500\n",
            "Batch 26/72 | Loss: 7.4595 | Batch Acc: 0.1250\n",
            "Batch 27/72 | Loss: 7.0637 | Batch Acc: 0.0625\n",
            "Batch 28/72 | Loss: 7.5088 | Batch Acc: 0.1250\n",
            "Batch 29/72 | Loss: 7.8152 | Batch Acc: 0.0625\n",
            "Batch 30/72 | Loss: 7.3274 | Batch Acc: 0.0625\n",
            "Batch 31/72 | Loss: 7.3388 | Batch Acc: 0.0000\n",
            "Batch 32/72 | Loss: 7.1121 | Batch Acc: 0.1250\n",
            "Batch 33/72 | Loss: 6.8876 | Batch Acc: 0.1250\n",
            "Batch 34/72 | Loss: 7.2351 | Batch Acc: 0.3125\n",
            "Batch 35/72 | Loss: 7.1923 | Batch Acc: 0.0625\n",
            "Batch 36/72 | Loss: 7.4052 | Batch Acc: 0.0625\n",
            "Batch 37/72 | Loss: 7.4098 | Batch Acc: 0.1875\n",
            "Batch 38/72 | Loss: 6.8255 | Batch Acc: 0.3125\n",
            "Batch 39/72 | Loss: 7.3006 | Batch Acc: 0.0000\n",
            "Batch 40/72 | Loss: 7.4383 | Batch Acc: 0.1250\n",
            "Batch 41/72 | Loss: 7.2895 | Batch Acc: 0.0625\n",
            "Batch 42/72 | Loss: 7.4487 | Batch Acc: 0.0625\n",
            "Batch 43/72 | Loss: 7.3212 | Batch Acc: 0.0625\n",
            "Batch 44/72 | Loss: 7.0873 | Batch Acc: 0.1250\n",
            "Batch 45/72 | Loss: 7.4479 | Batch Acc: 0.0625\n",
            "Batch 46/72 | Loss: 7.2928 | Batch Acc: 0.1250\n",
            "Batch 47/72 | Loss: 7.6937 | Batch Acc: 0.0000\n",
            "Batch 48/72 | Loss: 6.9745 | Batch Acc: 0.0625\n",
            "Batch 49/72 | Loss: 7.3864 | Batch Acc: 0.0000\n",
            "Batch 50/72 | Loss: 7.2528 | Batch Acc: 0.0625\n",
            "Batch 51/72 | Loss: 7.1961 | Batch Acc: 0.1250\n",
            "Batch 52/72 | Loss: 7.4488 | Batch Acc: 0.0000\n",
            "Batch 53/72 | Loss: 7.3045 | Batch Acc: 0.0625\n",
            "Batch 54/72 | Loss: 7.1334 | Batch Acc: 0.1250\n",
            "Batch 55/72 | Loss: 6.9851 | Batch Acc: 0.1250\n",
            "Batch 56/72 | Loss: 7.1899 | Batch Acc: 0.0625\n",
            "Batch 57/72 | Loss: 6.9671 | Batch Acc: 0.2500\n",
            "Batch 58/72 | Loss: 7.2903 | Batch Acc: 0.1250\n",
            "Batch 59/72 | Loss: 6.9761 | Batch Acc: 0.1250\n",
            "Batch 60/72 | Loss: 7.0010 | Batch Acc: 0.0000\n",
            "Batch 61/72 | Loss: 6.9797 | Batch Acc: 0.0000\n",
            "Batch 62/72 | Loss: 7.1177 | Batch Acc: 0.2500\n",
            "Batch 63/72 | Loss: 6.7069 | Batch Acc: 0.1250\n",
            "Batch 64/72 | Loss: 7.4613 | Batch Acc: 0.0625\n",
            "Batch 65/72 | Loss: 7.1533 | Batch Acc: 0.0625\n",
            "Batch 66/72 | Loss: 7.0660 | Batch Acc: 0.1875\n",
            "Batch 67/72 | Loss: 6.7782 | Batch Acc: 0.1875\n",
            "Batch 68/72 | Loss: 6.6717 | Batch Acc: 0.1250\n",
            "Batch 69/72 | Loss: 6.6844 | Batch Acc: 0.1875\n",
            "Batch 70/72 | Loss: 6.7710 | Batch Acc: 0.0625\n",
            "Batch 71/72 | Loss: 7.0685 | Batch Acc: 0.0625\n",
            "Batch 72/72 | Loss: 6.5733 | Batch Acc: 0.0000\n",
            "Epoch Train Loss: 522.6273, Accuracy: 0.0870\n",
            "Validation Accuracy: 0.1420\n",
            "✅ 儲存最佳模型（Acc: 0.1420）\n",
            "\n",
            "Epoch 2/50\n",
            "Batch 1/72 | Loss: 7.0670 | Batch Acc: 0.1875\n",
            "Batch 2/72 | Loss: 7.5732 | Batch Acc: 0.0625\n",
            "Batch 3/72 | Loss: 7.0240 | Batch Acc: 0.1250\n",
            "Batch 4/72 | Loss: 6.9088 | Batch Acc: 0.2500\n",
            "Batch 5/72 | Loss: 7.3837 | Batch Acc: 0.0625\n",
            "Batch 6/72 | Loss: 7.0733 | Batch Acc: 0.0000\n",
            "Batch 7/72 | Loss: 6.6977 | Batch Acc: 0.0625\n",
            "Batch 8/72 | Loss: 6.4104 | Batch Acc: 0.3750\n",
            "Batch 9/72 | Loss: 6.2666 | Batch Acc: 0.3125\n",
            "Batch 10/72 | Loss: 6.7715 | Batch Acc: 0.1250\n",
            "Batch 11/72 | Loss: 6.8362 | Batch Acc: 0.2500\n",
            "Batch 12/72 | Loss: 6.7812 | Batch Acc: 0.1875\n",
            "Batch 13/72 | Loss: 7.4405 | Batch Acc: 0.0625\n",
            "Batch 14/72 | Loss: 6.6472 | Batch Acc: 0.1875\n",
            "Batch 15/72 | Loss: 6.7572 | Batch Acc: 0.2500\n",
            "Batch 16/72 | Loss: 7.3708 | Batch Acc: 0.0000\n",
            "Batch 17/72 | Loss: 6.4624 | Batch Acc: 0.1875\n",
            "Batch 18/72 | Loss: 6.5498 | Batch Acc: 0.0625\n",
            "Batch 19/72 | Loss: 6.8838 | Batch Acc: 0.1250\n",
            "Batch 20/72 | Loss: 6.7489 | Batch Acc: 0.1875\n",
            "Batch 21/72 | Loss: 6.8818 | Batch Acc: 0.0000\n",
            "Batch 22/72 | Loss: 7.4509 | Batch Acc: 0.1250\n",
            "Batch 23/72 | Loss: 6.8435 | Batch Acc: 0.0625\n",
            "Batch 24/72 | Loss: 6.9073 | Batch Acc: 0.0625\n",
            "Batch 25/72 | Loss: 6.9977 | Batch Acc: 0.1875\n",
            "Batch 26/72 | Loss: 6.1713 | Batch Acc: 0.1250\n",
            "Batch 27/72 | Loss: 7.7476 | Batch Acc: 0.0000\n",
            "Batch 28/72 | Loss: 7.4891 | Batch Acc: 0.0625\n",
            "Batch 29/72 | Loss: 6.9587 | Batch Acc: 0.0625\n",
            "Batch 30/72 | Loss: 7.1073 | Batch Acc: 0.1250\n",
            "Batch 31/72 | Loss: 6.7050 | Batch Acc: 0.1875\n",
            "Batch 32/72 | Loss: 6.9236 | Batch Acc: 0.1250\n",
            "Batch 33/72 | Loss: 6.2524 | Batch Acc: 0.2500\n",
            "Batch 34/72 | Loss: 7.4294 | Batch Acc: 0.0625\n",
            "Batch 35/72 | Loss: 6.3318 | Batch Acc: 0.0625\n",
            "Batch 36/72 | Loss: 7.4047 | Batch Acc: 0.0625\n",
            "Batch 37/72 | Loss: 7.3128 | Batch Acc: 0.1250\n",
            "Batch 38/72 | Loss: 6.7631 | Batch Acc: 0.0625\n",
            "Batch 39/72 | Loss: 6.3821 | Batch Acc: 0.1875\n",
            "Batch 40/72 | Loss: 6.8083 | Batch Acc: 0.0000\n",
            "Batch 41/72 | Loss: 6.3523 | Batch Acc: 0.1875\n",
            "Batch 42/72 | Loss: 6.3663 | Batch Acc: 0.1250\n",
            "Batch 43/72 | Loss: 5.9466 | Batch Acc: 0.3125\n",
            "Batch 44/72 | Loss: 6.0498 | Batch Acc: 0.0625\n",
            "Batch 45/72 | Loss: 6.7562 | Batch Acc: 0.1250\n",
            "Batch 46/72 | Loss: 7.3838 | Batch Acc: 0.0625\n",
            "Batch 47/72 | Loss: 6.0222 | Batch Acc: 0.1875\n",
            "Batch 48/72 | Loss: 6.9600 | Batch Acc: 0.0625\n",
            "Batch 49/72 | Loss: 7.0712 | Batch Acc: 0.0000\n",
            "Batch 50/72 | Loss: 6.0868 | Batch Acc: 0.1250\n",
            "Batch 51/72 | Loss: 5.9468 | Batch Acc: 0.1875\n",
            "Batch 52/72 | Loss: 5.8745 | Batch Acc: 0.1250\n",
            "Batch 53/72 | Loss: 6.0500 | Batch Acc: 0.1250\n",
            "Batch 54/72 | Loss: 6.1379 | Batch Acc: 0.0625\n",
            "Batch 55/72 | Loss: 7.5229 | Batch Acc: 0.1250\n",
            "Batch 56/72 | Loss: 6.0823 | Batch Acc: 0.2500\n",
            "Batch 57/72 | Loss: 5.5234 | Batch Acc: 0.1875\n",
            "Batch 58/72 | Loss: 6.7982 | Batch Acc: 0.1875\n",
            "Batch 59/72 | Loss: 5.6414 | Batch Acc: 0.1250\n",
            "Batch 60/72 | Loss: 6.8853 | Batch Acc: 0.1875\n",
            "Batch 61/72 | Loss: 7.1768 | Batch Acc: 0.0000\n",
            "Batch 62/72 | Loss: 6.4313 | Batch Acc: 0.0625\n",
            "Batch 63/72 | Loss: 6.5249 | Batch Acc: 0.1875\n",
            "Batch 64/72 | Loss: 7.4218 | Batch Acc: 0.0625\n",
            "Batch 65/72 | Loss: 6.9832 | Batch Acc: 0.0625\n",
            "Batch 66/72 | Loss: 6.8048 | Batch Acc: 0.0625\n",
            "Batch 67/72 | Loss: 7.0551 | Batch Acc: 0.0625\n",
            "Batch 68/72 | Loss: 6.4811 | Batch Acc: 0.1250\n",
            "Batch 69/72 | Loss: 6.4397 | Batch Acc: 0.1250\n",
            "Batch 70/72 | Loss: 5.8860 | Batch Acc: 0.1875\n",
            "Batch 71/72 | Loss: 5.8038 | Batch Acc: 0.1875\n",
            "Batch 72/72 | Loss: 6.1337 | Batch Acc: 0.1538\n",
            "Epoch Train Loss: 483.1227, Accuracy: 0.1262\n",
            "Validation Accuracy: 0.1307\n",
            "\n",
            "Epoch 3/50\n",
            "Batch 1/72 | Loss: 5.5345 | Batch Acc: 0.1250\n",
            "Batch 2/72 | Loss: 6.1901 | Batch Acc: 0.1250\n",
            "Batch 3/72 | Loss: 6.5172 | Batch Acc: 0.1250\n",
            "Batch 4/72 | Loss: 6.0462 | Batch Acc: 0.0625\n",
            "Batch 5/72 | Loss: 6.0108 | Batch Acc: 0.0625\n",
            "Batch 6/72 | Loss: 7.1853 | Batch Acc: 0.0625\n",
            "Batch 7/72 | Loss: 6.6871 | Batch Acc: 0.1250\n",
            "Batch 8/72 | Loss: 5.8478 | Batch Acc: 0.1875\n",
            "Batch 9/72 | Loss: 6.6072 | Batch Acc: 0.2500\n",
            "Batch 10/72 | Loss: 6.2355 | Batch Acc: 0.0625\n",
            "Batch 11/72 | Loss: 5.4172 | Batch Acc: 0.1875\n",
            "Batch 12/72 | Loss: 6.5003 | Batch Acc: 0.1875\n",
            "Batch 13/72 | Loss: 6.9001 | Batch Acc: 0.0625\n",
            "Batch 14/72 | Loss: 7.0381 | Batch Acc: 0.0000\n",
            "Batch 15/72 | Loss: 6.6584 | Batch Acc: 0.0000\n",
            "Batch 16/72 | Loss: 6.4953 | Batch Acc: 0.1250\n",
            "Batch 17/72 | Loss: 6.6976 | Batch Acc: 0.0000\n",
            "Batch 18/72 | Loss: 6.3798 | Batch Acc: 0.1250\n",
            "Batch 19/72 | Loss: 6.6964 | Batch Acc: 0.2500\n",
            "Batch 20/72 | Loss: 6.2526 | Batch Acc: 0.0625\n",
            "Batch 21/72 | Loss: 7.4511 | Batch Acc: 0.0625\n",
            "Batch 22/72 | Loss: 5.8527 | Batch Acc: 0.2500\n",
            "Batch 23/72 | Loss: 6.0993 | Batch Acc: 0.1250\n",
            "Batch 24/72 | Loss: 6.1700 | Batch Acc: 0.0625\n",
            "Batch 25/72 | Loss: 5.9081 | Batch Acc: 0.1250\n",
            "Batch 26/72 | Loss: 6.2101 | Batch Acc: 0.2500\n",
            "Batch 27/72 | Loss: 5.9250 | Batch Acc: 0.2500\n",
            "Batch 28/72 | Loss: 7.1146 | Batch Acc: 0.0625\n",
            "Batch 29/72 | Loss: 7.4591 | Batch Acc: 0.0000\n",
            "Batch 30/72 | Loss: 6.6681 | Batch Acc: 0.1875\n",
            "Batch 31/72 | Loss: 6.2118 | Batch Acc: 0.1875\n",
            "Batch 32/72 | Loss: 6.3803 | Batch Acc: 0.1250\n",
            "Batch 33/72 | Loss: 5.9500 | Batch Acc: 0.1875\n",
            "Batch 34/72 | Loss: 6.0574 | Batch Acc: 0.1875\n",
            "Batch 35/72 | Loss: 6.8877 | Batch Acc: 0.1250\n",
            "Batch 36/72 | Loss: 6.5153 | Batch Acc: 0.0625\n",
            "Batch 37/72 | Loss: 6.7189 | Batch Acc: 0.0625\n",
            "Batch 38/72 | Loss: 6.4858 | Batch Acc: 0.0625\n",
            "Batch 39/72 | Loss: 6.1520 | Batch Acc: 0.1250\n",
            "Batch 40/72 | Loss: 5.7826 | Batch Acc: 0.2500\n",
            "Batch 41/72 | Loss: 6.5479 | Batch Acc: 0.0625\n",
            "Batch 42/72 | Loss: 5.9577 | Batch Acc: 0.2500\n",
            "Batch 43/72 | Loss: 6.2886 | Batch Acc: 0.2500\n",
            "Batch 44/72 | Loss: 7.0025 | Batch Acc: 0.1250\n",
            "Batch 45/72 | Loss: 6.3036 | Batch Acc: 0.1875\n",
            "Batch 46/72 | Loss: 6.2021 | Batch Acc: 0.1250\n",
            "Batch 47/72 | Loss: 6.5575 | Batch Acc: 0.1250\n",
            "Batch 48/72 | Loss: 7.0864 | Batch Acc: 0.0000\n",
            "Batch 49/72 | Loss: 7.1259 | Batch Acc: 0.0625\n",
            "Batch 50/72 | Loss: 6.3399 | Batch Acc: 0.1250\n",
            "Batch 51/72 | Loss: 5.7181 | Batch Acc: 0.2500\n",
            "Batch 52/72 | Loss: 6.8960 | Batch Acc: 0.2500\n",
            "Batch 53/72 | Loss: 6.5468 | Batch Acc: 0.1250\n",
            "Batch 54/72 | Loss: 6.5480 | Batch Acc: 0.0000\n",
            "Batch 55/72 | Loss: 6.7286 | Batch Acc: 0.1250\n",
            "Batch 56/72 | Loss: 5.5122 | Batch Acc: 0.1250\n",
            "Batch 57/72 | Loss: 6.4822 | Batch Acc: 0.0625\n",
            "Batch 58/72 | Loss: 6.6761 | Batch Acc: 0.1875\n",
            "Batch 59/72 | Loss: 6.5001 | Batch Acc: 0.1250\n",
            "Batch 60/72 | Loss: 6.0909 | Batch Acc: 0.1875\n",
            "Batch 61/72 | Loss: 5.5509 | Batch Acc: 0.1250\n",
            "Batch 62/72 | Loss: 6.7648 | Batch Acc: 0.0625\n",
            "Batch 63/72 | Loss: 6.7222 | Batch Acc: 0.1875\n",
            "Batch 64/72 | Loss: 6.2979 | Batch Acc: 0.0625\n",
            "Batch 65/72 | Loss: 5.9109 | Batch Acc: 0.1250\n",
            "Batch 66/72 | Loss: 6.4366 | Batch Acc: 0.0000\n",
            "Batch 67/72 | Loss: 5.9842 | Batch Acc: 0.2500\n",
            "Batch 68/72 | Loss: 6.9976 | Batch Acc: 0.0625\n",
            "Batch 69/72 | Loss: 6.2521 | Batch Acc: 0.0625\n",
            "Batch 70/72 | Loss: 6.5779 | Batch Acc: 0.2500\n",
            "Batch 71/72 | Loss: 6.4180 | Batch Acc: 0.1875\n",
            "Batch 72/72 | Loss: 5.5360 | Batch Acc: 0.0769\n",
            "Epoch Train Loss: 460.4573, Accuracy: 0.1271\n",
            "Validation Accuracy: 0.0966\n",
            "\n",
            "Epoch 4/50\n",
            "Batch 1/72 | Loss: 7.8302 | Batch Acc: 0.0000\n",
            "Batch 2/72 | Loss: 5.7761 | Batch Acc: 0.2500\n",
            "Batch 3/72 | Loss: 6.3908 | Batch Acc: 0.0625\n",
            "Batch 4/72 | Loss: 6.1273 | Batch Acc: 0.0625\n",
            "Batch 5/72 | Loss: 4.9968 | Batch Acc: 0.3125\n",
            "Batch 6/72 | Loss: 5.8810 | Batch Acc: 0.2500\n",
            "Batch 7/72 | Loss: 6.0089 | Batch Acc: 0.2500\n",
            "Batch 8/72 | Loss: 6.9855 | Batch Acc: 0.0000\n",
            "Batch 9/72 | Loss: 5.8517 | Batch Acc: 0.0625\n",
            "Batch 10/72 | Loss: 6.6490 | Batch Acc: 0.0625\n",
            "Batch 11/72 | Loss: 5.4364 | Batch Acc: 0.2500\n",
            "Batch 12/72 | Loss: 5.7457 | Batch Acc: 0.1875\n",
            "Batch 13/72 | Loss: 6.5214 | Batch Acc: 0.1250\n",
            "Batch 14/72 | Loss: 8.0364 | Batch Acc: 0.0625\n",
            "Batch 15/72 | Loss: 6.3196 | Batch Acc: 0.0000\n",
            "Batch 16/72 | Loss: 6.6746 | Batch Acc: 0.0625\n",
            "Batch 17/72 | Loss: 5.8329 | Batch Acc: 0.1250\n",
            "Batch 18/72 | Loss: 5.1892 | Batch Acc: 0.3125\n",
            "Batch 19/72 | Loss: 6.3986 | Batch Acc: 0.1250\n",
            "Batch 20/72 | Loss: 5.9809 | Batch Acc: 0.1250\n",
            "Batch 21/72 | Loss: 5.8643 | Batch Acc: 0.1875\n",
            "Batch 22/72 | Loss: 6.4116 | Batch Acc: 0.0625\n",
            "Batch 23/72 | Loss: 5.4011 | Batch Acc: 0.3750\n",
            "Batch 24/72 | Loss: 6.1195 | Batch Acc: 0.1875\n",
            "Batch 25/72 | Loss: 5.1497 | Batch Acc: 0.1875\n",
            "Batch 26/72 | Loss: 6.2822 | Batch Acc: 0.1250\n",
            "Batch 27/72 | Loss: 6.4744 | Batch Acc: 0.1250\n",
            "Batch 28/72 | Loss: 5.7571 | Batch Acc: 0.0000\n",
            "Batch 29/72 | Loss: 5.2780 | Batch Acc: 0.2500\n",
            "Batch 30/72 | Loss: 6.0469 | Batch Acc: 0.1875\n",
            "Batch 31/72 | Loss: 7.3037 | Batch Acc: 0.1250\n",
            "Batch 32/72 | Loss: 6.5076 | Batch Acc: 0.0625\n",
            "Batch 33/72 | Loss: 5.8914 | Batch Acc: 0.1875\n",
            "Batch 34/72 | Loss: 5.0699 | Batch Acc: 0.2500\n",
            "Batch 35/72 | Loss: 6.7921 | Batch Acc: 0.1250\n",
            "Batch 36/72 | Loss: 6.6997 | Batch Acc: 0.0000\n",
            "Batch 37/72 | Loss: 6.3812 | Batch Acc: 0.2500\n",
            "Batch 38/72 | Loss: 7.2212 | Batch Acc: 0.1250\n",
            "Batch 39/72 | Loss: 6.2812 | Batch Acc: 0.2500\n",
            "Batch 40/72 | Loss: 6.1451 | Batch Acc: 0.2500\n",
            "Batch 41/72 | Loss: 6.9752 | Batch Acc: 0.1250\n",
            "Batch 42/72 | Loss: 6.5899 | Batch Acc: 0.1250\n",
            "Batch 43/72 | Loss: 5.8638 | Batch Acc: 0.1250\n",
            "Batch 44/72 | Loss: 6.0799 | Batch Acc: 0.1250\n",
            "Batch 45/72 | Loss: 6.7227 | Batch Acc: 0.1875\n",
            "Batch 46/72 | Loss: 5.7333 | Batch Acc: 0.1875\n",
            "Batch 47/72 | Loss: 6.3038 | Batch Acc: 0.0000\n",
            "Batch 48/72 | Loss: 5.4661 | Batch Acc: 0.1875\n",
            "Batch 49/72 | Loss: 6.3290 | Batch Acc: 0.1250\n",
            "Batch 50/72 | Loss: 5.6302 | Batch Acc: 0.1875\n",
            "Batch 51/72 | Loss: 6.4312 | Batch Acc: 0.0000\n",
            "Batch 52/72 | Loss: 6.3856 | Batch Acc: 0.1250\n",
            "Batch 53/72 | Loss: 6.9321 | Batch Acc: 0.0625\n",
            "Batch 54/72 | Loss: 6.0680 | Batch Acc: 0.1250\n",
            "Batch 55/72 | Loss: 7.1024 | Batch Acc: 0.2500\n",
            "Batch 56/72 | Loss: 6.8584 | Batch Acc: 0.0625\n",
            "Batch 57/72 | Loss: 6.6628 | Batch Acc: 0.1875\n",
            "Batch 58/72 | Loss: 5.9868 | Batch Acc: 0.1250\n",
            "Batch 59/72 | Loss: 6.0260 | Batch Acc: 0.0625\n",
            "Batch 60/72 | Loss: 6.3911 | Batch Acc: 0.1250\n",
            "Batch 61/72 | Loss: 6.2668 | Batch Acc: 0.1250\n",
            "Batch 62/72 | Loss: 6.6152 | Batch Acc: 0.0000\n",
            "Batch 63/72 | Loss: 5.6527 | Batch Acc: 0.2500\n",
            "Batch 64/72 | Loss: 6.2971 | Batch Acc: 0.2500\n",
            "Batch 65/72 | Loss: 5.8751 | Batch Acc: 0.1250\n",
            "Batch 66/72 | Loss: 6.5614 | Batch Acc: 0.1875\n",
            "Batch 67/72 | Loss: 6.1764 | Batch Acc: 0.0625\n",
            "Batch 68/72 | Loss: 6.1640 | Batch Acc: 0.0625\n",
            "Batch 69/72 | Loss: 6.3977 | Batch Acc: 0.1875\n",
            "Batch 70/72 | Loss: 5.7964 | Batch Acc: 0.1875\n",
            "Batch 71/72 | Loss: 5.8732 | Batch Acc: 0.1875\n",
            "Batch 72/72 | Loss: 5.8524 | Batch Acc: 0.1538\n",
            "Epoch Train Loss: 447.7775, Accuracy: 0.1427\n",
            "Validation Accuracy: 0.1761\n",
            "✅ 儲存最佳模型（Acc: 0.1761）\n",
            "\n",
            "Epoch 5/50\n",
            "Batch 1/72 | Loss: 6.6335 | Batch Acc: 0.0625\n",
            "Batch 2/72 | Loss: 5.8253 | Batch Acc: 0.0625\n",
            "Batch 3/72 | Loss: 5.3426 | Batch Acc: 0.3125\n",
            "Batch 4/72 | Loss: 6.2329 | Batch Acc: 0.0625\n",
            "Batch 5/72 | Loss: 6.4727 | Batch Acc: 0.1250\n",
            "Batch 6/72 | Loss: 5.8912 | Batch Acc: 0.3125\n",
            "Batch 7/72 | Loss: 6.1212 | Batch Acc: 0.0000\n",
            "Batch 8/72 | Loss: 4.9415 | Batch Acc: 0.2500\n",
            "Batch 9/72 | Loss: 6.8402 | Batch Acc: 0.1250\n",
            "Batch 10/72 | Loss: 6.5593 | Batch Acc: 0.0000\n",
            "Batch 11/72 | Loss: 5.0817 | Batch Acc: 0.1250\n",
            "Batch 12/72 | Loss: 6.1545 | Batch Acc: 0.0625\n",
            "Batch 13/72 | Loss: 5.7955 | Batch Acc: 0.0625\n",
            "Batch 14/72 | Loss: 6.3965 | Batch Acc: 0.1250\n",
            "Batch 15/72 | Loss: 5.9911 | Batch Acc: 0.0625\n",
            "Batch 16/72 | Loss: 6.6420 | Batch Acc: 0.0625\n",
            "Batch 17/72 | Loss: 7.0403 | Batch Acc: 0.1250\n",
            "Batch 18/72 | Loss: 6.0666 | Batch Acc: 0.0625\n",
            "Batch 19/72 | Loss: 6.4145 | Batch Acc: 0.1250\n",
            "Batch 20/72 | Loss: 6.1589 | Batch Acc: 0.1875\n",
            "Batch 21/72 | Loss: 6.6604 | Batch Acc: 0.1250\n",
            "Batch 22/72 | Loss: 6.0290 | Batch Acc: 0.1875\n",
            "Batch 23/72 | Loss: 6.5880 | Batch Acc: 0.0625\n",
            "Batch 24/72 | Loss: 5.3195 | Batch Acc: 0.1250\n",
            "Batch 25/72 | Loss: 6.2856 | Batch Acc: 0.2500\n",
            "Batch 26/72 | Loss: 5.7945 | Batch Acc: 0.2500\n",
            "Batch 27/72 | Loss: 5.7482 | Batch Acc: 0.2500\n",
            "Batch 28/72 | Loss: 5.5638 | Batch Acc: 0.3125\n",
            "Batch 29/72 | Loss: 6.3150 | Batch Acc: 0.1250\n",
            "Batch 30/72 | Loss: 5.5508 | Batch Acc: 0.3125\n",
            "Batch 31/72 | Loss: 7.4411 | Batch Acc: 0.0000\n",
            "Batch 32/72 | Loss: 6.6654 | Batch Acc: 0.1250\n",
            "Batch 33/72 | Loss: 7.3627 | Batch Acc: 0.1250\n",
            "Batch 34/72 | Loss: 6.3144 | Batch Acc: 0.1875\n",
            "Batch 35/72 | Loss: 5.1989 | Batch Acc: 0.3125\n",
            "Batch 36/72 | Loss: 6.0027 | Batch Acc: 0.3125\n",
            "Batch 37/72 | Loss: 5.8109 | Batch Acc: 0.2500\n",
            "Batch 38/72 | Loss: 6.2895 | Batch Acc: 0.2500\n",
            "Batch 39/72 | Loss: 6.1811 | Batch Acc: 0.1875\n",
            "Batch 40/72 | Loss: 5.8610 | Batch Acc: 0.1875\n",
            "Batch 41/72 | Loss: 5.9123 | Batch Acc: 0.0625\n",
            "Batch 42/72 | Loss: 5.9086 | Batch Acc: 0.1875\n",
            "Batch 43/72 | Loss: 5.9944 | Batch Acc: 0.1250\n",
            "Batch 44/72 | Loss: 5.5317 | Batch Acc: 0.1875\n",
            "Batch 45/72 | Loss: 5.8068 | Batch Acc: 0.1875\n",
            "Batch 46/72 | Loss: 6.1239 | Batch Acc: 0.1875\n",
            "Batch 47/72 | Loss: 6.1502 | Batch Acc: 0.1250\n",
            "Batch 48/72 | Loss: 6.2574 | Batch Acc: 0.1250\n",
            "Batch 49/72 | Loss: 5.7594 | Batch Acc: 0.1875\n",
            "Batch 50/72 | Loss: 5.6031 | Batch Acc: 0.1250\n",
            "Batch 51/72 | Loss: 5.9142 | Batch Acc: 0.1250\n",
            "Batch 52/72 | Loss: 6.0995 | Batch Acc: 0.1875\n",
            "Batch 53/72 | Loss: 7.2813 | Batch Acc: 0.0000\n",
            "Batch 54/72 | Loss: 5.2882 | Batch Acc: 0.1875\n",
            "Batch 55/72 | Loss: 7.6727 | Batch Acc: 0.1875\n",
            "Batch 56/72 | Loss: 7.1923 | Batch Acc: 0.1250\n",
            "Batch 57/72 | Loss: 6.2748 | Batch Acc: 0.0625\n",
            "Batch 58/72 | Loss: 6.3090 | Batch Acc: 0.1250\n",
            "Batch 59/72 | Loss: 6.6989 | Batch Acc: 0.0000\n",
            "Batch 60/72 | Loss: 6.5541 | Batch Acc: 0.1250\n",
            "Batch 61/72 | Loss: 6.2785 | Batch Acc: 0.0625\n",
            "Batch 62/72 | Loss: 5.5630 | Batch Acc: 0.1875\n",
            "Batch 63/72 | Loss: 5.8122 | Batch Acc: 0.3750\n",
            "Batch 64/72 | Loss: 5.2868 | Batch Acc: 0.2500\n",
            "Batch 65/72 | Loss: 6.4498 | Batch Acc: 0.1250\n",
            "Batch 66/72 | Loss: 6.1795 | Batch Acc: 0.1875\n",
            "Batch 67/72 | Loss: 5.7757 | Batch Acc: 0.0625\n",
            "Batch 68/72 | Loss: 5.1350 | Batch Acc: 0.1875\n",
            "Batch 69/72 | Loss: 5.5159 | Batch Acc: 0.0625\n",
            "Batch 70/72 | Loss: 6.2501 | Batch Acc: 0.1875\n",
            "Batch 71/72 | Loss: 6.1001 | Batch Acc: 0.2500\n",
            "Batch 72/72 | Loss: 5.2503 | Batch Acc: 0.1538\n",
            "Epoch Train Loss: 439.5141, Accuracy: 0.1523\n",
            "Validation Accuracy: 0.1705\n",
            "\n",
            "Epoch 6/50\n",
            "Batch 1/72 | Loss: 6.2848 | Batch Acc: 0.1250\n",
            "Batch 2/72 | Loss: 5.4236 | Batch Acc: 0.1875\n",
            "Batch 3/72 | Loss: 5.9690 | Batch Acc: 0.0625\n",
            "Batch 4/72 | Loss: 6.0131 | Batch Acc: 0.1250\n",
            "Batch 5/72 | Loss: 6.2655 | Batch Acc: 0.1875\n",
            "Batch 6/72 | Loss: 6.2012 | Batch Acc: 0.0625\n",
            "Batch 7/72 | Loss: 6.1260 | Batch Acc: 0.4375\n",
            "Batch 8/72 | Loss: 6.7068 | Batch Acc: 0.0625\n",
            "Batch 9/72 | Loss: 5.2962 | Batch Acc: 0.2500\n",
            "Batch 10/72 | Loss: 5.6922 | Batch Acc: 0.0625\n",
            "Batch 11/72 | Loss: 5.7646 | Batch Acc: 0.0625\n",
            "Batch 12/72 | Loss: 6.6761 | Batch Acc: 0.0625\n",
            "Batch 13/72 | Loss: 5.8884 | Batch Acc: 0.1250\n",
            "Batch 14/72 | Loss: 5.4762 | Batch Acc: 0.3125\n",
            "Batch 15/72 | Loss: 6.1043 | Batch Acc: 0.2500\n",
            "Batch 16/72 | Loss: 5.5031 | Batch Acc: 0.2500\n",
            "Batch 17/72 | Loss: 6.6645 | Batch Acc: 0.1875\n",
            "Batch 18/72 | Loss: 5.4673 | Batch Acc: 0.1875\n",
            "Batch 19/72 | Loss: 5.5567 | Batch Acc: 0.3750\n",
            "Batch 20/72 | Loss: 7.3435 | Batch Acc: 0.0000\n",
            "Batch 21/72 | Loss: 5.6867 | Batch Acc: 0.1250\n",
            "Batch 22/72 | Loss: 5.6483 | Batch Acc: 0.1875\n",
            "Batch 23/72 | Loss: 5.0748 | Batch Acc: 0.3125\n",
            "Batch 24/72 | Loss: 5.9687 | Batch Acc: 0.1250\n",
            "Batch 25/72 | Loss: 6.2773 | Batch Acc: 0.1250\n",
            "Batch 26/72 | Loss: 4.5215 | Batch Acc: 0.5000\n",
            "Batch 27/72 | Loss: 5.4358 | Batch Acc: 0.1250\n",
            "Batch 28/72 | Loss: 5.4648 | Batch Acc: 0.1875\n",
            "Batch 29/72 | Loss: 6.8484 | Batch Acc: 0.0625\n",
            "Batch 30/72 | Loss: 6.5039 | Batch Acc: 0.1250\n",
            "Batch 31/72 | Loss: 7.0745 | Batch Acc: 0.1250\n",
            "Batch 32/72 | Loss: 5.4217 | Batch Acc: 0.3750\n",
            "Batch 33/72 | Loss: 6.4442 | Batch Acc: 0.1250\n",
            "Batch 34/72 | Loss: 6.7260 | Batch Acc: 0.1875\n",
            "Batch 35/72 | Loss: 6.2138 | Batch Acc: 0.1250\n",
            "Batch 36/72 | Loss: 6.8424 | Batch Acc: 0.1250\n",
            "Batch 37/72 | Loss: 6.6342 | Batch Acc: 0.1250\n",
            "Batch 38/72 | Loss: 5.8795 | Batch Acc: 0.1250\n",
            "Batch 39/72 | Loss: 6.9597 | Batch Acc: 0.1250\n",
            "Batch 40/72 | Loss: 5.9082 | Batch Acc: 0.0625\n",
            "Batch 41/72 | Loss: 5.9116 | Batch Acc: 0.1250\n",
            "Batch 42/72 | Loss: 6.9391 | Batch Acc: 0.1250\n",
            "Batch 43/72 | Loss: 6.0596 | Batch Acc: 0.1250\n",
            "Batch 44/72 | Loss: 6.9161 | Batch Acc: 0.0625\n",
            "Batch 45/72 | Loss: 6.7135 | Batch Acc: 0.1250\n",
            "Batch 46/72 | Loss: 5.8986 | Batch Acc: 0.1875\n",
            "Batch 47/72 | Loss: 5.7151 | Batch Acc: 0.3125\n",
            "Batch 48/72 | Loss: 6.9744 | Batch Acc: 0.1250\n",
            "Batch 49/72 | Loss: 6.3325 | Batch Acc: 0.0625\n",
            "Batch 50/72 | Loss: 5.8070 | Batch Acc: 0.0625\n",
            "Batch 51/72 | Loss: 6.2418 | Batch Acc: 0.0625\n",
            "Batch 52/72 | Loss: 7.4433 | Batch Acc: 0.0625\n",
            "Batch 53/72 | Loss: 5.9541 | Batch Acc: 0.1875\n",
            "Batch 54/72 | Loss: 5.9297 | Batch Acc: 0.1875\n",
            "Batch 55/72 | Loss: 5.8516 | Batch Acc: 0.1875\n",
            "Batch 56/72 | Loss: 5.5640 | Batch Acc: 0.0000\n",
            "Batch 57/72 | Loss: 5.2612 | Batch Acc: 0.1875\n",
            "Batch 58/72 | Loss: 5.1780 | Batch Acc: 0.1250\n",
            "Batch 59/72 | Loss: 5.3138 | Batch Acc: 0.1250\n",
            "Batch 60/72 | Loss: 5.9890 | Batch Acc: 0.0625\n",
            "Batch 61/72 | Loss: 5.7122 | Batch Acc: 0.1250\n",
            "Batch 62/72 | Loss: 5.7995 | Batch Acc: 0.1250\n",
            "Batch 63/72 | Loss: 6.5298 | Batch Acc: 0.1250\n",
            "Batch 64/72 | Loss: 5.5160 | Batch Acc: 0.2500\n",
            "Batch 65/72 | Loss: 5.6288 | Batch Acc: 0.1250\n",
            "Batch 66/72 | Loss: 6.1858 | Batch Acc: 0.2500\n",
            "Batch 67/72 | Loss: 5.6299 | Batch Acc: 0.2500\n",
            "Batch 68/72 | Loss: 6.1430 | Batch Acc: 0.1875\n",
            "Batch 69/72 | Loss: 6.3556 | Batch Acc: 0.1875\n",
            "Batch 70/72 | Loss: 5.1475 | Batch Acc: 0.3750\n",
            "Batch 71/72 | Loss: 6.1499 | Batch Acc: 0.0625\n",
            "Batch 72/72 | Loss: 6.4298 | Batch Acc: 0.0769\n",
            "Epoch Train Loss: 435.1794, Accuracy: 0.1584\n",
            "Validation Accuracy: 0.1705\n",
            "\n",
            "Epoch 7/50\n",
            "Batch 1/72 | Loss: 5.1746 | Batch Acc: 0.1250\n",
            "Batch 2/72 | Loss: 6.6038 | Batch Acc: 0.0000\n",
            "Batch 3/72 | Loss: 6.1364 | Batch Acc: 0.0000\n",
            "Batch 4/72 | Loss: 6.4089 | Batch Acc: 0.1250\n",
            "Batch 5/72 | Loss: 4.9381 | Batch Acc: 0.3125\n",
            "Batch 6/72 | Loss: 6.1536 | Batch Acc: 0.3125\n",
            "Batch 7/72 | Loss: 5.4280 | Batch Acc: 0.1250\n",
            "Batch 8/72 | Loss: 5.3746 | Batch Acc: 0.1875\n",
            "Batch 9/72 | Loss: 6.0052 | Batch Acc: 0.2500\n",
            "Batch 10/72 | Loss: 5.6921 | Batch Acc: 0.1875\n",
            "Batch 11/72 | Loss: 6.1317 | Batch Acc: 0.0000\n",
            "Batch 12/72 | Loss: 5.3032 | Batch Acc: 0.3125\n",
            "Batch 13/72 | Loss: 6.2088 | Batch Acc: 0.1875\n",
            "Batch 14/72 | Loss: 5.1210 | Batch Acc: 0.1250\n",
            "Batch 15/72 | Loss: 6.6596 | Batch Acc: 0.1250\n",
            "Batch 16/72 | Loss: 6.0542 | Batch Acc: 0.2500\n",
            "Batch 17/72 | Loss: 5.9858 | Batch Acc: 0.1250\n",
            "Batch 18/72 | Loss: 5.7645 | Batch Acc: 0.2500\n",
            "Batch 19/72 | Loss: 5.8635 | Batch Acc: 0.3125\n",
            "Batch 20/72 | Loss: 6.5594 | Batch Acc: 0.0625\n",
            "Batch 21/72 | Loss: 6.0216 | Batch Acc: 0.1250\n",
            "Batch 22/72 | Loss: 7.2304 | Batch Acc: 0.0625\n",
            "Batch 23/72 | Loss: 6.2494 | Batch Acc: 0.1250\n",
            "Batch 24/72 | Loss: 5.3118 | Batch Acc: 0.1250\n",
            "Batch 25/72 | Loss: 5.8405 | Batch Acc: 0.1875\n",
            "Batch 26/72 | Loss: 5.9715 | Batch Acc: 0.0625\n",
            "Batch 27/72 | Loss: 6.1315 | Batch Acc: 0.1875\n",
            "Batch 28/72 | Loss: 6.3936 | Batch Acc: 0.2500\n",
            "Batch 29/72 | Loss: 5.5930 | Batch Acc: 0.1250\n",
            "Batch 30/72 | Loss: 5.4923 | Batch Acc: 0.3750\n",
            "Batch 31/72 | Loss: 5.0223 | Batch Acc: 0.3125\n",
            "Batch 32/72 | Loss: 5.7475 | Batch Acc: 0.0625\n",
            "Batch 33/72 | Loss: 5.9587 | Batch Acc: 0.0625\n",
            "Batch 34/72 | Loss: 5.7962 | Batch Acc: 0.3125\n",
            "Batch 35/72 | Loss: 6.1489 | Batch Acc: 0.1875\n",
            "Batch 36/72 | Loss: 6.2581 | Batch Acc: 0.0625\n",
            "Batch 37/72 | Loss: 6.8880 | Batch Acc: 0.0000\n",
            "Batch 38/72 | Loss: 5.6526 | Batch Acc: 0.1250\n",
            "Batch 39/72 | Loss: 6.4927 | Batch Acc: 0.1875\n",
            "Batch 40/72 | Loss: 5.6871 | Batch Acc: 0.2500\n",
            "Batch 41/72 | Loss: 5.3870 | Batch Acc: 0.1875\n",
            "Batch 42/72 | Loss: 5.8248 | Batch Acc: 0.3125\n",
            "Batch 43/72 | Loss: 6.3729 | Batch Acc: 0.1875\n",
            "Batch 44/72 | Loss: 5.9673 | Batch Acc: 0.2500\n",
            "Batch 45/72 | Loss: 6.5278 | Batch Acc: 0.2500\n",
            "Batch 46/72 | Loss: 5.8906 | Batch Acc: 0.1875\n",
            "Batch 47/72 | Loss: 5.8607 | Batch Acc: 0.3125\n",
            "Batch 48/72 | Loss: 5.9247 | Batch Acc: 0.0000\n",
            "Batch 49/72 | Loss: 5.1600 | Batch Acc: 0.3125\n",
            "Batch 50/72 | Loss: 5.4199 | Batch Acc: 0.1875\n",
            "Batch 51/72 | Loss: 5.6914 | Batch Acc: 0.1250\n",
            "Batch 52/72 | Loss: 6.2681 | Batch Acc: 0.0000\n",
            "Batch 53/72 | Loss: 5.7179 | Batch Acc: 0.3125\n",
            "Batch 54/72 | Loss: 5.3988 | Batch Acc: 0.3125\n",
            "Batch 55/72 | Loss: 5.2801 | Batch Acc: 0.1875\n",
            "Batch 56/72 | Loss: 5.5357 | Batch Acc: 0.1875\n",
            "Batch 57/72 | Loss: 6.0367 | Batch Acc: 0.0625\n",
            "Batch 58/72 | Loss: 5.5056 | Batch Acc: 0.1250\n",
            "Batch 59/72 | Loss: 5.7220 | Batch Acc: 0.1875\n",
            "Batch 60/72 | Loss: 6.0933 | Batch Acc: 0.0625\n",
            "Batch 61/72 | Loss: 5.4986 | Batch Acc: 0.1875\n",
            "Batch 62/72 | Loss: 5.9145 | Batch Acc: 0.1250\n",
            "Batch 63/72 | Loss: 5.8393 | Batch Acc: 0.1875\n",
            "Batch 64/72 | Loss: 6.5275 | Batch Acc: 0.0625\n",
            "Batch 65/72 | Loss: 4.9463 | Batch Acc: 0.1875\n",
            "Batch 66/72 | Loss: 5.3426 | Batch Acc: 0.3750\n",
            "Batch 67/72 | Loss: 6.6229 | Batch Acc: 0.1250\n",
            "Batch 68/72 | Loss: 5.5966 | Batch Acc: 0.1250\n",
            "Batch 69/72 | Loss: 6.4171 | Batch Acc: 0.0625\n",
            "Batch 70/72 | Loss: 5.3329 | Batch Acc: 0.1875\n",
            "Batch 71/72 | Loss: 6.0838 | Batch Acc: 0.0625\n",
            "Batch 72/72 | Loss: 4.9180 | Batch Acc: 0.2308\n",
            "Epoch Train Loss: 422.0803, Accuracy: 0.1697\n",
            "Validation Accuracy: 0.2159\n",
            "✅ 儲存最佳模型（Acc: 0.2159）\n",
            "\n",
            "Epoch 8/50\n",
            "Batch 1/72 | Loss: 6.5592 | Batch Acc: 0.1250\n",
            "Batch 2/72 | Loss: 6.5946 | Batch Acc: 0.1250\n",
            "Batch 3/72 | Loss: 5.5705 | Batch Acc: 0.2500\n",
            "Batch 4/72 | Loss: 6.6623 | Batch Acc: 0.1250\n",
            "Batch 5/72 | Loss: 5.2515 | Batch Acc: 0.0625\n",
            "Batch 6/72 | Loss: 5.2078 | Batch Acc: 0.0625\n",
            "Batch 7/72 | Loss: 4.9960 | Batch Acc: 0.3750\n",
            "Batch 8/72 | Loss: 6.5101 | Batch Acc: 0.1875\n",
            "Batch 9/72 | Loss: 6.0261 | Batch Acc: 0.1250\n",
            "Batch 10/72 | Loss: 5.4470 | Batch Acc: 0.1250\n",
            "Batch 11/72 | Loss: 6.1110 | Batch Acc: 0.1875\n",
            "Batch 12/72 | Loss: 6.9217 | Batch Acc: 0.0000\n",
            "Batch 13/72 | Loss: 5.0377 | Batch Acc: 0.1875\n",
            "Batch 14/72 | Loss: 5.6234 | Batch Acc: 0.2500\n",
            "Batch 15/72 | Loss: 5.0081 | Batch Acc: 0.2500\n",
            "Batch 16/72 | Loss: 5.6196 | Batch Acc: 0.0625\n",
            "Batch 17/72 | Loss: 5.8405 | Batch Acc: 0.0625\n",
            "Batch 18/72 | Loss: 6.6695 | Batch Acc: 0.1875\n",
            "Batch 19/72 | Loss: 6.1547 | Batch Acc: 0.0625\n",
            "Batch 20/72 | Loss: 5.6150 | Batch Acc: 0.1875\n",
            "Batch 21/72 | Loss: 5.7620 | Batch Acc: 0.1875\n",
            "Batch 22/72 | Loss: 6.5325 | Batch Acc: 0.0625\n",
            "Batch 23/72 | Loss: 6.6340 | Batch Acc: 0.1250\n",
            "Batch 24/72 | Loss: 5.9723 | Batch Acc: 0.2500\n",
            "Batch 25/72 | Loss: 5.8755 | Batch Acc: 0.3125\n",
            "Batch 26/72 | Loss: 6.2076 | Batch Acc: 0.1875\n",
            "Batch 27/72 | Loss: 5.6265 | Batch Acc: 0.2500\n",
            "Batch 28/72 | Loss: 6.5342 | Batch Acc: 0.0000\n",
            "Batch 29/72 | Loss: 6.3106 | Batch Acc: 0.1875\n",
            "Batch 30/72 | Loss: 5.2315 | Batch Acc: 0.2500\n",
            "Batch 31/72 | Loss: 6.0263 | Batch Acc: 0.3125\n",
            "Batch 32/72 | Loss: 5.3374 | Batch Acc: 0.4375\n",
            "Batch 33/72 | Loss: 5.7932 | Batch Acc: 0.1250\n",
            "Batch 34/72 | Loss: 6.5799 | Batch Acc: 0.0000\n",
            "Batch 35/72 | Loss: 5.6719 | Batch Acc: 0.1875\n",
            "Batch 36/72 | Loss: 7.6073 | Batch Acc: 0.0625\n",
            "Batch 37/72 | Loss: 5.8971 | Batch Acc: 0.1250\n",
            "Batch 38/72 | Loss: 5.4355 | Batch Acc: 0.1875\n",
            "Batch 39/72 | Loss: 5.1922 | Batch Acc: 0.3750\n",
            "Batch 40/72 | Loss: 6.6777 | Batch Acc: 0.0625\n",
            "Batch 41/72 | Loss: 6.0786 | Batch Acc: 0.0625\n",
            "Batch 42/72 | Loss: 6.0439 | Batch Acc: 0.0625\n",
            "Batch 43/72 | Loss: 6.1304 | Batch Acc: 0.1250\n",
            "Batch 44/72 | Loss: 5.7082 | Batch Acc: 0.2500\n",
            "Batch 45/72 | Loss: 5.1903 | Batch Acc: 0.2500\n",
            "Batch 46/72 | Loss: 6.0794 | Batch Acc: 0.3125\n",
            "Batch 47/72 | Loss: 6.3587 | Batch Acc: 0.3750\n",
            "Batch 48/72 | Loss: 6.3919 | Batch Acc: 0.0625\n",
            "Batch 49/72 | Loss: 5.4127 | Batch Acc: 0.1250\n",
            "Batch 50/72 | Loss: 5.4283 | Batch Acc: 0.1875\n",
            "Batch 51/72 | Loss: 5.0307 | Batch Acc: 0.3750\n",
            "Batch 52/72 | Loss: 5.6977 | Batch Acc: 0.2500\n",
            "Batch 53/72 | Loss: 6.4035 | Batch Acc: 0.0000\n",
            "Batch 54/72 | Loss: 6.3986 | Batch Acc: 0.3125\n",
            "Batch 55/72 | Loss: 4.7032 | Batch Acc: 0.3125\n",
            "Batch 56/72 | Loss: 6.2778 | Batch Acc: 0.1250\n",
            "Batch 57/72 | Loss: 5.5447 | Batch Acc: 0.1250\n",
            "Batch 58/72 | Loss: 5.2325 | Batch Acc: 0.2500\n",
            "Batch 59/72 | Loss: 5.0214 | Batch Acc: 0.3125\n",
            "Batch 60/72 | Loss: 5.8338 | Batch Acc: 0.1250\n",
            "Batch 61/72 | Loss: 6.1772 | Batch Acc: 0.0625\n",
            "Batch 62/72 | Loss: 5.2444 | Batch Acc: 0.3750\n",
            "Batch 63/72 | Loss: 5.2462 | Batch Acc: 0.2500\n",
            "Batch 64/72 | Loss: 5.4661 | Batch Acc: 0.1875\n",
            "Batch 65/72 | Loss: 6.7588 | Batch Acc: 0.1875\n",
            "Batch 66/72 | Loss: 6.8307 | Batch Acc: 0.1875\n",
            "Batch 67/72 | Loss: 5.7615 | Batch Acc: 0.0625\n",
            "Batch 68/72 | Loss: 5.9549 | Batch Acc: 0.1875\n",
            "Batch 69/72 | Loss: 5.7890 | Batch Acc: 0.1250\n",
            "Batch 70/72 | Loss: 5.0914 | Batch Acc: 0.3125\n",
            "Batch 71/72 | Loss: 6.1399 | Batch Acc: 0.1250\n",
            "Batch 72/72 | Loss: 7.7138 | Batch Acc: 0.0769\n",
            "Epoch Train Loss: 425.4716, Accuracy: 0.1784\n",
            "Validation Accuracy: 0.1705\n",
            "\n",
            "Epoch 9/50\n",
            "Batch 1/72 | Loss: 5.3708 | Batch Acc: 0.1250\n",
            "Batch 2/72 | Loss: 6.2824 | Batch Acc: 0.1875\n",
            "Batch 3/72 | Loss: 5.0609 | Batch Acc: 0.2500\n",
            "Batch 4/72 | Loss: 5.7696 | Batch Acc: 0.3125\n",
            "Batch 5/72 | Loss: 5.1319 | Batch Acc: 0.3125\n",
            "Batch 6/72 | Loss: 4.6865 | Batch Acc: 0.1250\n",
            "Batch 7/72 | Loss: 6.9146 | Batch Acc: 0.1250\n",
            "Batch 8/72 | Loss: 5.4591 | Batch Acc: 0.2500\n",
            "Batch 9/72 | Loss: 5.9022 | Batch Acc: 0.3125\n",
            "Batch 10/72 | Loss: 6.1377 | Batch Acc: 0.0625\n",
            "Batch 11/72 | Loss: 5.4141 | Batch Acc: 0.3125\n",
            "Batch 12/72 | Loss: 5.7611 | Batch Acc: 0.3125\n",
            "Batch 13/72 | Loss: 5.4793 | Batch Acc: 0.1250\n",
            "Batch 14/72 | Loss: 6.5311 | Batch Acc: 0.0625\n",
            "Batch 15/72 | Loss: 5.9325 | Batch Acc: 0.0625\n",
            "Batch 16/72 | Loss: 4.5608 | Batch Acc: 0.2500\n",
            "Batch 17/72 | Loss: 6.6843 | Batch Acc: 0.0000\n",
            "Batch 18/72 | Loss: 6.2828 | Batch Acc: 0.1250\n",
            "Batch 19/72 | Loss: 6.3049 | Batch Acc: 0.0625\n",
            "Batch 20/72 | Loss: 8.1831 | Batch Acc: 0.0000\n",
            "Batch 21/72 | Loss: 6.3079 | Batch Acc: 0.1250\n",
            "Batch 22/72 | Loss: 5.6479 | Batch Acc: 0.1250\n",
            "Batch 23/72 | Loss: 5.7069 | Batch Acc: 0.1875\n",
            "Batch 24/72 | Loss: 5.4342 | Batch Acc: 0.1875\n",
            "Batch 25/72 | Loss: 5.5143 | Batch Acc: 0.1875\n",
            "Batch 26/72 | Loss: 5.8796 | Batch Acc: 0.3125\n",
            "Batch 27/72 | Loss: 6.0583 | Batch Acc: 0.1875\n",
            "Batch 28/72 | Loss: 6.1633 | Batch Acc: 0.1875\n",
            "Batch 29/72 | Loss: 6.0359 | Batch Acc: 0.1875\n",
            "Batch 30/72 | Loss: 6.7161 | Batch Acc: 0.0000\n",
            "Batch 31/72 | Loss: 7.1960 | Batch Acc: 0.1250\n",
            "Batch 32/72 | Loss: 5.8490 | Batch Acc: 0.0000\n",
            "Batch 33/72 | Loss: 4.8114 | Batch Acc: 0.3750\n",
            "Batch 34/72 | Loss: 6.3554 | Batch Acc: 0.1875\n",
            "Batch 35/72 | Loss: 5.9048 | Batch Acc: 0.3750\n",
            "Batch 36/72 | Loss: 5.8366 | Batch Acc: 0.2500\n",
            "Batch 37/72 | Loss: 5.6578 | Batch Acc: 0.0625\n",
            "Batch 38/72 | Loss: 6.0985 | Batch Acc: 0.2500\n",
            "Batch 39/72 | Loss: 6.3479 | Batch Acc: 0.1875\n",
            "Batch 40/72 | Loss: 6.5811 | Batch Acc: 0.0625\n",
            "Batch 41/72 | Loss: 5.0707 | Batch Acc: 0.1250\n",
            "Batch 42/72 | Loss: 5.6811 | Batch Acc: 0.1250\n",
            "Batch 43/72 | Loss: 5.8926 | Batch Acc: 0.2500\n",
            "Batch 44/72 | Loss: 5.7136 | Batch Acc: 0.1250\n",
            "Batch 45/72 | Loss: 5.0628 | Batch Acc: 0.3125\n",
            "Batch 46/72 | Loss: 6.2107 | Batch Acc: 0.1875\n",
            "Batch 47/72 | Loss: 6.4877 | Batch Acc: 0.1875\n",
            "Batch 48/72 | Loss: 5.4232 | Batch Acc: 0.2500\n",
            "Batch 49/72 | Loss: 5.8995 | Batch Acc: 0.1250\n",
            "Batch 50/72 | Loss: 5.4448 | Batch Acc: 0.2500\n",
            "Batch 51/72 | Loss: 5.1944 | Batch Acc: 0.1875\n",
            "Batch 52/72 | Loss: 5.4348 | Batch Acc: 0.1875\n",
            "Batch 53/72 | Loss: 6.5395 | Batch Acc: 0.1250\n",
            "Batch 54/72 | Loss: 6.1920 | Batch Acc: 0.1250\n",
            "Batch 55/72 | Loss: 5.6628 | Batch Acc: 0.3125\n",
            "Batch 56/72 | Loss: 5.3092 | Batch Acc: 0.1875\n",
            "Batch 57/72 | Loss: 5.5068 | Batch Acc: 0.1250\n",
            "Batch 58/72 | Loss: 6.1140 | Batch Acc: 0.1250\n",
            "Batch 59/72 | Loss: 5.7379 | Batch Acc: 0.3125\n",
            "Batch 60/72 | Loss: 5.8697 | Batch Acc: 0.1250\n",
            "Batch 61/72 | Loss: 5.4720 | Batch Acc: 0.3750\n",
            "Batch 62/72 | Loss: 5.5938 | Batch Acc: 0.0625\n",
            "Batch 63/72 | Loss: 6.0586 | Batch Acc: 0.1875\n",
            "Batch 64/72 | Loss: 5.9300 | Batch Acc: 0.1250\n",
            "Batch 65/72 | Loss: 5.8556 | Batch Acc: 0.1250\n",
            "Batch 66/72 | Loss: 5.2414 | Batch Acc: 0.4375\n",
            "Batch 67/72 | Loss: 5.4669 | Batch Acc: 0.1250\n",
            "Batch 68/72 | Loss: 5.9159 | Batch Acc: 0.1250\n",
            "Batch 69/72 | Loss: 5.5780 | Batch Acc: 0.2500\n",
            "Batch 70/72 | Loss: 5.1611 | Batch Acc: 0.3750\n",
            "Batch 71/72 | Loss: 5.5809 | Batch Acc: 0.1250\n",
            "Batch 72/72 | Loss: 7.1148 | Batch Acc: 0.2308\n",
            "Epoch Train Loss: 421.3695, Accuracy: 0.1836\n",
            "Validation Accuracy: 0.2273\n",
            "✅ 儲存最佳模型（Acc: 0.2273）\n",
            "\n",
            "Epoch 10/50\n",
            "Batch 1/72 | Loss: 5.6718 | Batch Acc: 0.1875\n",
            "Batch 2/72 | Loss: 6.3277 | Batch Acc: 0.1250\n",
            "Batch 3/72 | Loss: 6.6690 | Batch Acc: 0.1250\n",
            "Batch 4/72 | Loss: 6.7015 | Batch Acc: 0.1250\n",
            "Batch 5/72 | Loss: 5.0412 | Batch Acc: 0.2500\n",
            "Batch 6/72 | Loss: 6.2335 | Batch Acc: 0.1250\n",
            "Batch 7/72 | Loss: 5.1203 | Batch Acc: 0.3750\n",
            "Batch 8/72 | Loss: 5.6619 | Batch Acc: 0.0625\n",
            "Batch 9/72 | Loss: 5.8985 | Batch Acc: 0.1250\n",
            "Batch 10/72 | Loss: 5.8926 | Batch Acc: 0.1250\n",
            "Batch 11/72 | Loss: 5.5267 | Batch Acc: 0.1250\n",
            "Batch 12/72 | Loss: 6.2039 | Batch Acc: 0.1250\n",
            "Batch 13/72 | Loss: 6.2206 | Batch Acc: 0.0000\n",
            "Batch 14/72 | Loss: 4.8483 | Batch Acc: 0.3125\n",
            "Batch 15/72 | Loss: 6.0428 | Batch Acc: 0.1875\n",
            "Batch 16/72 | Loss: 5.5712 | Batch Acc: 0.2500\n",
            "Batch 17/72 | Loss: 5.5791 | Batch Acc: 0.0625\n",
            "Batch 18/72 | Loss: 6.1398 | Batch Acc: 0.0000\n",
            "Batch 19/72 | Loss: 5.8308 | Batch Acc: 0.1875\n",
            "Batch 20/72 | Loss: 5.5441 | Batch Acc: 0.1875\n",
            "Batch 21/72 | Loss: 4.9091 | Batch Acc: 0.1875\n",
            "Batch 22/72 | Loss: 6.2781 | Batch Acc: 0.0000\n",
            "Batch 23/72 | Loss: 5.3872 | Batch Acc: 0.1875\n",
            "Batch 24/72 | Loss: 5.4700 | Batch Acc: 0.3125\n",
            "Batch 25/72 | Loss: 5.3829 | Batch Acc: 0.2500\n",
            "Batch 26/72 | Loss: 5.5707 | Batch Acc: 0.1250\n",
            "Batch 27/72 | Loss: 4.7240 | Batch Acc: 0.1875\n",
            "Batch 28/72 | Loss: 5.0188 | Batch Acc: 0.1250\n",
            "Batch 29/72 | Loss: 6.1831 | Batch Acc: 0.0625\n",
            "Batch 30/72 | Loss: 6.5422 | Batch Acc: 0.0625\n",
            "Batch 31/72 | Loss: 7.0687 | Batch Acc: 0.1250\n",
            "Batch 32/72 | Loss: 4.8748 | Batch Acc: 0.2500\n",
            "Batch 33/72 | Loss: 4.6911 | Batch Acc: 0.3750\n",
            "Batch 34/72 | Loss: 6.3234 | Batch Acc: 0.0625\n",
            "Batch 35/72 | Loss: 5.4125 | Batch Acc: 0.2500\n",
            "Batch 36/72 | Loss: 5.4004 | Batch Acc: 0.3125\n",
            "Batch 37/72 | Loss: 6.9146 | Batch Acc: 0.1875\n",
            "Batch 38/72 | Loss: 5.5133 | Batch Acc: 0.1250\n",
            "Batch 39/72 | Loss: 5.9210 | Batch Acc: 0.1250\n",
            "Batch 40/72 | Loss: 5.7014 | Batch Acc: 0.1875\n",
            "Batch 41/72 | Loss: 5.9433 | Batch Acc: 0.1875\n",
            "Batch 42/72 | Loss: 5.8807 | Batch Acc: 0.1875\n",
            "Batch 43/72 | Loss: 5.9747 | Batch Acc: 0.1250\n",
            "Batch 44/72 | Loss: 6.4267 | Batch Acc: 0.0625\n",
            "Batch 45/72 | Loss: 5.8187 | Batch Acc: 0.2500\n",
            "Batch 46/72 | Loss: 5.3672 | Batch Acc: 0.1250\n",
            "Batch 47/72 | Loss: 6.1772 | Batch Acc: 0.1875\n",
            "Batch 48/72 | Loss: 4.7656 | Batch Acc: 0.3125\n",
            "Batch 49/72 | Loss: 6.1143 | Batch Acc: 0.2500\n",
            "Batch 50/72 | Loss: 6.0706 | Batch Acc: 0.1875\n",
            "Batch 51/72 | Loss: 6.9722 | Batch Acc: 0.0625\n",
            "Batch 52/72 | Loss: 6.0142 | Batch Acc: 0.1875\n",
            "Batch 53/72 | Loss: 5.2838 | Batch Acc: 0.2500\n",
            "Batch 54/72 | Loss: 5.8747 | Batch Acc: 0.1250\n",
            "Batch 55/72 | Loss: 6.6862 | Batch Acc: 0.0000\n",
            "Batch 56/72 | Loss: 5.5119 | Batch Acc: 0.1875\n",
            "Batch 57/72 | Loss: 6.2972 | Batch Acc: 0.0000\n",
            "Batch 58/72 | Loss: 4.7004 | Batch Acc: 0.1875\n",
            "Batch 59/72 | Loss: 5.5014 | Batch Acc: 0.2500\n",
            "Batch 60/72 | Loss: 5.0703 | Batch Acc: 0.3125\n",
            "Batch 61/72 | Loss: 4.4784 | Batch Acc: 0.1875\n",
            "Batch 62/72 | Loss: 6.4592 | Batch Acc: 0.1250\n",
            "Batch 63/72 | Loss: 6.7499 | Batch Acc: 0.0625\n",
            "Batch 64/72 | Loss: 5.6123 | Batch Acc: 0.1250\n",
            "Batch 65/72 | Loss: 6.3770 | Batch Acc: 0.1250\n",
            "Batch 66/72 | Loss: 6.7370 | Batch Acc: 0.1250\n",
            "Batch 67/72 | Loss: 5.8292 | Batch Acc: 0.1250\n",
            "Batch 68/72 | Loss: 5.8692 | Batch Acc: 0.1875\n",
            "Batch 69/72 | Loss: 5.0114 | Batch Acc: 0.2500\n",
            "Batch 70/72 | Loss: 4.8908 | Batch Acc: 0.3750\n",
            "Batch 71/72 | Loss: 6.1040 | Batch Acc: 0.3750\n",
            "Batch 72/72 | Loss: 5.5567 | Batch Acc: 0.2308\n",
            "Epoch Train Loss: 416.1390, Accuracy: 0.1706\n",
            "Validation Accuracy: 0.1818\n",
            "\n",
            "Epoch 11/50\n",
            "Batch 1/72 | Loss: 7.1086 | Batch Acc: 0.2500\n",
            "Batch 2/72 | Loss: 6.0048 | Batch Acc: 0.1250\n",
            "Batch 3/72 | Loss: 5.1927 | Batch Acc: 0.3125\n",
            "Batch 4/72 | Loss: 5.9451 | Batch Acc: 0.2500\n",
            "Batch 5/72 | Loss: 6.4507 | Batch Acc: 0.0625\n",
            "Batch 6/72 | Loss: 6.0347 | Batch Acc: 0.1875\n",
            "Batch 7/72 | Loss: 6.2309 | Batch Acc: 0.1875\n",
            "Batch 8/72 | Loss: 5.6402 | Batch Acc: 0.0625\n",
            "Batch 9/72 | Loss: 6.5713 | Batch Acc: 0.1875\n",
            "Batch 10/72 | Loss: 5.7600 | Batch Acc: 0.3125\n",
            "Batch 11/72 | Loss: 6.6923 | Batch Acc: 0.0625\n",
            "Batch 12/72 | Loss: 4.9623 | Batch Acc: 0.2500\n",
            "Batch 13/72 | Loss: 5.7080 | Batch Acc: 0.2500\n",
            "Batch 14/72 | Loss: 5.6436 | Batch Acc: 0.3750\n",
            "Batch 15/72 | Loss: 6.0020 | Batch Acc: 0.2500\n",
            "Batch 16/72 | Loss: 6.1492 | Batch Acc: 0.0625\n",
            "Batch 17/72 | Loss: 5.7562 | Batch Acc: 0.2500\n",
            "Batch 18/72 | Loss: 5.3723 | Batch Acc: 0.1875\n",
            "Batch 19/72 | Loss: 5.5459 | Batch Acc: 0.3125\n",
            "Batch 20/72 | Loss: 5.2161 | Batch Acc: 0.2500\n",
            "Batch 21/72 | Loss: 6.1177 | Batch Acc: 0.0000\n",
            "Batch 22/72 | Loss: 6.1065 | Batch Acc: 0.1250\n",
            "Batch 23/72 | Loss: 5.2164 | Batch Acc: 0.3750\n",
            "Batch 24/72 | Loss: 6.7189 | Batch Acc: 0.1250\n",
            "Batch 25/72 | Loss: 5.8661 | Batch Acc: 0.1250\n",
            "Batch 26/72 | Loss: 5.5880 | Batch Acc: 0.3125\n",
            "Batch 27/72 | Loss: 6.6608 | Batch Acc: 0.1250\n",
            "Batch 28/72 | Loss: 5.1221 | Batch Acc: 0.2500\n",
            "Batch 29/72 | Loss: 5.4850 | Batch Acc: 0.2500\n",
            "Batch 30/72 | Loss: 6.7992 | Batch Acc: 0.0625\n",
            "Batch 31/72 | Loss: 5.8533 | Batch Acc: 0.0625\n",
            "Batch 32/72 | Loss: 5.6359 | Batch Acc: 0.3125\n",
            "Batch 33/72 | Loss: 4.5893 | Batch Acc: 0.1875\n",
            "Batch 34/72 | Loss: 6.6910 | Batch Acc: 0.1250\n",
            "Batch 35/72 | Loss: 5.7273 | Batch Acc: 0.0625\n",
            "Batch 36/72 | Loss: 6.3057 | Batch Acc: 0.1875\n",
            "Batch 37/72 | Loss: 6.4180 | Batch Acc: 0.1250\n",
            "Batch 38/72 | Loss: 5.0726 | Batch Acc: 0.1875\n",
            "Batch 39/72 | Loss: 6.2114 | Batch Acc: 0.0625\n",
            "Batch 40/72 | Loss: 6.4588 | Batch Acc: 0.2500\n",
            "Batch 41/72 | Loss: 5.2805 | Batch Acc: 0.1250\n",
            "Batch 42/72 | Loss: 5.1711 | Batch Acc: 0.2500\n",
            "Batch 43/72 | Loss: 5.1487 | Batch Acc: 0.1875\n",
            "Batch 44/72 | Loss: 5.6734 | Batch Acc: 0.0000\n",
            "Batch 45/72 | Loss: 6.7178 | Batch Acc: 0.1250\n",
            "Batch 46/72 | Loss: 5.0871 | Batch Acc: 0.2500\n",
            "Batch 47/72 | Loss: 7.9056 | Batch Acc: 0.1250\n",
            "Batch 48/72 | Loss: 6.4008 | Batch Acc: 0.0625\n",
            "Batch 49/72 | Loss: 6.6628 | Batch Acc: 0.0625\n",
            "Batch 50/72 | Loss: 5.2474 | Batch Acc: 0.1875\n",
            "Batch 51/72 | Loss: 6.0646 | Batch Acc: 0.1875\n",
            "Batch 52/72 | Loss: 5.2210 | Batch Acc: 0.2500\n",
            "Batch 53/72 | Loss: 5.3136 | Batch Acc: 0.1250\n",
            "Batch 54/72 | Loss: 5.0984 | Batch Acc: 0.2500\n",
            "Batch 55/72 | Loss: 5.6865 | Batch Acc: 0.1875\n",
            "Batch 56/72 | Loss: 5.1048 | Batch Acc: 0.3750\n",
            "Batch 57/72 | Loss: 4.8082 | Batch Acc: 0.1875\n",
            "Batch 58/72 | Loss: 5.8917 | Batch Acc: 0.1875\n",
            "Batch 59/72 | Loss: 6.0178 | Batch Acc: 0.1875\n",
            "Batch 60/72 | Loss: 5.8158 | Batch Acc: 0.1875\n",
            "Batch 61/72 | Loss: 6.5750 | Batch Acc: 0.1875\n",
            "Batch 62/72 | Loss: 6.1657 | Batch Acc: 0.1875\n",
            "Batch 63/72 | Loss: 4.8648 | Batch Acc: 0.3125\n",
            "Batch 64/72 | Loss: 5.7317 | Batch Acc: 0.2500\n",
            "Batch 65/72 | Loss: 5.3019 | Batch Acc: 0.1250\n",
            "Batch 66/72 | Loss: 6.1884 | Batch Acc: 0.1250\n",
            "Batch 67/72 | Loss: 4.8196 | Batch Acc: 0.2500\n",
            "Batch 68/72 | Loss: 5.9872 | Batch Acc: 0.1250\n",
            "Batch 69/72 | Loss: 5.3225 | Batch Acc: 0.3125\n",
            "Batch 70/72 | Loss: 6.1203 | Batch Acc: 0.1250\n",
            "Batch 71/72 | Loss: 5.9516 | Batch Acc: 0.1875\n",
            "Batch 72/72 | Loss: 6.9567 | Batch Acc: 0.1538\n",
            "Epoch Train Loss: 420.9338, Accuracy: 0.1854\n",
            "Validation Accuracy: 0.2386\n",
            "✅ 儲存最佳模型（Acc: 0.2386）\n",
            "\n",
            "Epoch 12/50\n",
            "Batch 1/72 | Loss: 5.4269 | Batch Acc: 0.2500\n",
            "Batch 2/72 | Loss: 6.6066 | Batch Acc: 0.3125\n",
            "Batch 3/72 | Loss: 6.5462 | Batch Acc: 0.0625\n",
            "Batch 4/72 | Loss: 6.4262 | Batch Acc: 0.1875\n",
            "Batch 5/72 | Loss: 5.7464 | Batch Acc: 0.0625\n",
            "Batch 6/72 | Loss: 6.4860 | Batch Acc: 0.2500\n",
            "Batch 7/72 | Loss: 4.9904 | Batch Acc: 0.1875\n",
            "Batch 8/72 | Loss: 4.9966 | Batch Acc: 0.2500\n",
            "Batch 9/72 | Loss: 6.0323 | Batch Acc: 0.1875\n",
            "Batch 10/72 | Loss: 5.7545 | Batch Acc: 0.1875\n",
            "Batch 11/72 | Loss: 5.7503 | Batch Acc: 0.1250\n",
            "Batch 12/72 | Loss: 5.2535 | Batch Acc: 0.3125\n",
            "Batch 13/72 | Loss: 5.7771 | Batch Acc: 0.1875\n",
            "Batch 14/72 | Loss: 5.7864 | Batch Acc: 0.1250\n",
            "Batch 15/72 | Loss: 5.4104 | Batch Acc: 0.1875\n",
            "Batch 16/72 | Loss: 6.0313 | Batch Acc: 0.1250\n",
            "Batch 17/72 | Loss: 7.0164 | Batch Acc: 0.0000\n",
            "Batch 18/72 | Loss: 5.1054 | Batch Acc: 0.3750\n",
            "Batch 19/72 | Loss: 6.4657 | Batch Acc: 0.1875\n",
            "Batch 20/72 | Loss: 5.2693 | Batch Acc: 0.0625\n",
            "Batch 21/72 | Loss: 5.4712 | Batch Acc: 0.2500\n",
            "Batch 22/72 | Loss: 5.9033 | Batch Acc: 0.3125\n",
            "Batch 23/72 | Loss: 6.7452 | Batch Acc: 0.1250\n",
            "Batch 24/72 | Loss: 5.9491 | Batch Acc: 0.2500\n",
            "Batch 25/72 | Loss: 6.3678 | Batch Acc: 0.1250\n",
            "Batch 26/72 | Loss: 5.7989 | Batch Acc: 0.3125\n",
            "Batch 27/72 | Loss: 5.8052 | Batch Acc: 0.1875\n",
            "Batch 28/72 | Loss: 5.3414 | Batch Acc: 0.1875\n",
            "Batch 29/72 | Loss: 5.7656 | Batch Acc: 0.1875\n",
            "Batch 30/72 | Loss: 4.8767 | Batch Acc: 0.2500\n",
            "Batch 31/72 | Loss: 6.1298 | Batch Acc: 0.1875\n",
            "Batch 32/72 | Loss: 6.2689 | Batch Acc: 0.0625\n",
            "Batch 33/72 | Loss: 5.5654 | Batch Acc: 0.1250\n",
            "Batch 34/72 | Loss: 5.6146 | Batch Acc: 0.1875\n",
            "Batch 35/72 | Loss: 5.4794 | Batch Acc: 0.1250\n",
            "Batch 36/72 | Loss: 4.8235 | Batch Acc: 0.3125\n",
            "Batch 37/72 | Loss: 6.5959 | Batch Acc: 0.1250\n",
            "Batch 38/72 | Loss: 5.6901 | Batch Acc: 0.3750\n",
            "Batch 39/72 | Loss: 6.4266 | Batch Acc: 0.1875\n",
            "Batch 40/72 | Loss: 5.5797 | Batch Acc: 0.1250\n",
            "Batch 41/72 | Loss: 5.1177 | Batch Acc: 0.3125\n",
            "Batch 42/72 | Loss: 5.5368 | Batch Acc: 0.1250\n",
            "Batch 43/72 | Loss: 5.4680 | Batch Acc: 0.2500\n",
            "Batch 44/72 | Loss: 4.8859 | Batch Acc: 0.3125\n",
            "Batch 45/72 | Loss: 6.3010 | Batch Acc: 0.1250\n",
            "Batch 46/72 | Loss: 5.7596 | Batch Acc: 0.2500\n",
            "Batch 47/72 | Loss: 5.4687 | Batch Acc: 0.1875\n",
            "Batch 48/72 | Loss: 4.9851 | Batch Acc: 0.1875\n",
            "Batch 49/72 | Loss: 5.7638 | Batch Acc: 0.1875\n",
            "Batch 50/72 | Loss: 4.6879 | Batch Acc: 0.2500\n",
            "Batch 51/72 | Loss: 6.9561 | Batch Acc: 0.0625\n",
            "Batch 52/72 | Loss: 5.3319 | Batch Acc: 0.1250\n",
            "Batch 53/72 | Loss: 6.0232 | Batch Acc: 0.0625\n",
            "Batch 54/72 | Loss: 5.9612 | Batch Acc: 0.1875\n",
            "Batch 55/72 | Loss: 6.3917 | Batch Acc: 0.0625\n",
            "Batch 56/72 | Loss: 6.9961 | Batch Acc: 0.0625\n",
            "Batch 57/72 | Loss: 5.1579 | Batch Acc: 0.3125\n",
            "Batch 58/72 | Loss: 5.8492 | Batch Acc: 0.0625\n",
            "Batch 59/72 | Loss: 6.1072 | Batch Acc: 0.1250\n",
            "Batch 60/72 | Loss: 5.4336 | Batch Acc: 0.3125\n",
            "Batch 61/72 | Loss: 5.2777 | Batch Acc: 0.3125\n",
            "Batch 62/72 | Loss: 5.5324 | Batch Acc: 0.1875\n",
            "Batch 63/72 | Loss: 5.6469 | Batch Acc: 0.1875\n",
            "Batch 64/72 | Loss: 6.5842 | Batch Acc: 0.2500\n",
            "Batch 65/72 | Loss: 7.1600 | Batch Acc: 0.0625\n",
            "Batch 66/72 | Loss: 5.7719 | Batch Acc: 0.1250\n",
            "Batch 67/72 | Loss: 6.1839 | Batch Acc: 0.1875\n",
            "Batch 68/72 | Loss: 6.5903 | Batch Acc: 0.1250\n",
            "Batch 69/72 | Loss: 5.6483 | Batch Acc: 0.2500\n",
            "Batch 70/72 | Loss: 6.3195 | Batch Acc: 0.1250\n",
            "Batch 71/72 | Loss: 5.9595 | Batch Acc: 0.1875\n",
            "Batch 72/72 | Loss: 5.5594 | Batch Acc: 0.1538\n",
            "Epoch Train Loss: 419.4928, Accuracy: 0.1854\n",
            "Validation Accuracy: 0.2557\n",
            "✅ 儲存最佳模型（Acc: 0.2557）\n",
            "\n",
            "Epoch 13/50\n",
            "Batch 1/72 | Loss: 6.3065 | Batch Acc: 0.1250\n",
            "Batch 2/72 | Loss: 6.2441 | Batch Acc: 0.1250\n",
            "Batch 3/72 | Loss: 5.6425 | Batch Acc: 0.1250\n",
            "Batch 4/72 | Loss: 6.5909 | Batch Acc: 0.2500\n",
            "Batch 5/72 | Loss: 5.7275 | Batch Acc: 0.1250\n",
            "Batch 6/72 | Loss: 5.2742 | Batch Acc: 0.3125\n",
            "Batch 7/72 | Loss: 5.7181 | Batch Acc: 0.1250\n",
            "Batch 8/72 | Loss: 5.9790 | Batch Acc: 0.1875\n",
            "Batch 9/72 | Loss: 5.0369 | Batch Acc: 0.3125\n",
            "Batch 10/72 | Loss: 5.7093 | Batch Acc: 0.2500\n",
            "Batch 11/72 | Loss: 5.9980 | Batch Acc: 0.1250\n",
            "Batch 12/72 | Loss: 5.3824 | Batch Acc: 0.4375\n",
            "Batch 13/72 | Loss: 5.1426 | Batch Acc: 0.1875\n",
            "Batch 14/72 | Loss: 5.3053 | Batch Acc: 0.2500\n",
            "Batch 15/72 | Loss: 5.2173 | Batch Acc: 0.3750\n",
            "Batch 16/72 | Loss: 6.5904 | Batch Acc: 0.1875\n",
            "Batch 17/72 | Loss: 5.2966 | Batch Acc: 0.1875\n",
            "Batch 18/72 | Loss: 5.4239 | Batch Acc: 0.2500\n",
            "Batch 19/72 | Loss: 5.6709 | Batch Acc: 0.2500\n",
            "Batch 20/72 | Loss: 6.2392 | Batch Acc: 0.0625\n",
            "Batch 21/72 | Loss: 6.1080 | Batch Acc: 0.0625\n",
            "Batch 22/72 | Loss: 6.2600 | Batch Acc: 0.2500\n",
            "Batch 23/72 | Loss: 5.9520 | Batch Acc: 0.1875\n",
            "Batch 24/72 | Loss: 5.1234 | Batch Acc: 0.1875\n",
            "Batch 25/72 | Loss: 5.4753 | Batch Acc: 0.1250\n",
            "Batch 26/72 | Loss: 6.6562 | Batch Acc: 0.1875\n",
            "Batch 27/72 | Loss: 5.6778 | Batch Acc: 0.0625\n",
            "Batch 28/72 | Loss: 5.1750 | Batch Acc: 0.3125\n",
            "Batch 29/72 | Loss: 5.3079 | Batch Acc: 0.2500\n",
            "Batch 30/72 | Loss: 5.4232 | Batch Acc: 0.3125\n",
            "Batch 31/72 | Loss: 5.9836 | Batch Acc: 0.0625\n",
            "Batch 32/72 | Loss: 5.0114 | Batch Acc: 0.3125\n",
            "Batch 33/72 | Loss: 5.0711 | Batch Acc: 0.0625\n",
            "Batch 34/72 | Loss: 6.4541 | Batch Acc: 0.3125\n",
            "Batch 35/72 | Loss: 6.7720 | Batch Acc: 0.1875\n",
            "Batch 36/72 | Loss: 6.0928 | Batch Acc: 0.1875\n",
            "Batch 37/72 | Loss: 5.4270 | Batch Acc: 0.1250\n",
            "Batch 38/72 | Loss: 7.1606 | Batch Acc: 0.1250\n",
            "Batch 39/72 | Loss: 4.9460 | Batch Acc: 0.1875\n",
            "Batch 40/72 | Loss: 6.3811 | Batch Acc: 0.1250\n",
            "Batch 41/72 | Loss: 5.3708 | Batch Acc: 0.1875\n",
            "Batch 42/72 | Loss: 5.8269 | Batch Acc: 0.1875\n",
            "Batch 43/72 | Loss: 6.5081 | Batch Acc: 0.0625\n",
            "Batch 44/72 | Loss: 5.6336 | Batch Acc: 0.1250\n",
            "Batch 45/72 | Loss: 6.7643 | Batch Acc: 0.2500\n",
            "Batch 46/72 | Loss: 5.9728 | Batch Acc: 0.2500\n",
            "Batch 47/72 | Loss: 4.9262 | Batch Acc: 0.1250\n",
            "Batch 48/72 | Loss: 4.7352 | Batch Acc: 0.1250\n",
            "Batch 49/72 | Loss: 5.6336 | Batch Acc: 0.1875\n",
            "Batch 50/72 | Loss: 5.7142 | Batch Acc: 0.1250\n",
            "Batch 51/72 | Loss: 5.4644 | Batch Acc: 0.1250\n",
            "Batch 52/72 | Loss: 5.4622 | Batch Acc: 0.3125\n",
            "Batch 53/72 | Loss: 4.3169 | Batch Acc: 0.2500\n",
            "Batch 54/72 | Loss: 4.3171 | Batch Acc: 0.5000\n",
            "Batch 55/72 | Loss: 5.2205 | Batch Acc: 0.2500\n",
            "Batch 56/72 | Loss: 6.5919 | Batch Acc: 0.1250\n",
            "Batch 57/72 | Loss: 6.5079 | Batch Acc: 0.1250\n",
            "Batch 58/72 | Loss: 6.1790 | Batch Acc: 0.1250\n",
            "Batch 59/72 | Loss: 6.5104 | Batch Acc: 0.0625\n",
            "Batch 60/72 | Loss: 5.7832 | Batch Acc: 0.2500\n",
            "Batch 61/72 | Loss: 5.1504 | Batch Acc: 0.2500\n",
            "Batch 62/72 | Loss: 4.8841 | Batch Acc: 0.3125\n",
            "Batch 63/72 | Loss: 5.5057 | Batch Acc: 0.1250\n",
            "Batch 64/72 | Loss: 5.9277 | Batch Acc: 0.1875\n",
            "Batch 65/72 | Loss: 5.5763 | Batch Acc: 0.1250\n",
            "Batch 66/72 | Loss: 5.8243 | Batch Acc: 0.1875\n",
            "Batch 67/72 | Loss: 6.2722 | Batch Acc: 0.1250\n",
            "Batch 68/72 | Loss: 5.4717 | Batch Acc: 0.1875\n",
            "Batch 69/72 | Loss: 5.5440 | Batch Acc: 0.1250\n",
            "Batch 70/72 | Loss: 7.7926 | Batch Acc: 0.1250\n",
            "Batch 71/72 | Loss: 5.1782 | Batch Acc: 0.2500\n",
            "Batch 72/72 | Loss: 4.9878 | Batch Acc: 0.3077\n",
            "Epoch Train Loss: 412.5078, Accuracy: 0.1932\n",
            "Validation Accuracy: 0.2386\n",
            "\n",
            "Epoch 14/50\n",
            "Batch 1/72 | Loss: 5.4138 | Batch Acc: 0.1250\n",
            "Batch 2/72 | Loss: 5.6237 | Batch Acc: 0.1875\n",
            "Batch 3/72 | Loss: 5.8310 | Batch Acc: 0.1875\n",
            "Batch 4/72 | Loss: 6.0023 | Batch Acc: 0.0000\n",
            "Batch 5/72 | Loss: 4.2122 | Batch Acc: 0.3750\n",
            "Batch 6/72 | Loss: 4.5382 | Batch Acc: 0.3125\n",
            "Batch 7/72 | Loss: 6.2587 | Batch Acc: 0.1875\n",
            "Batch 8/72 | Loss: 6.5162 | Batch Acc: 0.1250\n",
            "Batch 9/72 | Loss: 5.5908 | Batch Acc: 0.4375\n",
            "Batch 10/72 | Loss: 6.0556 | Batch Acc: 0.2500\n",
            "Batch 11/72 | Loss: 6.0063 | Batch Acc: 0.2500\n",
            "Batch 12/72 | Loss: 5.9969 | Batch Acc: 0.1250\n",
            "Batch 13/72 | Loss: 5.8570 | Batch Acc: 0.1250\n",
            "Batch 14/72 | Loss: 5.9321 | Batch Acc: 0.0625\n",
            "Batch 15/72 | Loss: 4.9608 | Batch Acc: 0.3750\n",
            "Batch 16/72 | Loss: 6.2795 | Batch Acc: 0.1875\n",
            "Batch 17/72 | Loss: 5.8222 | Batch Acc: 0.3125\n",
            "Batch 18/72 | Loss: 6.1083 | Batch Acc: 0.0625\n",
            "Batch 19/72 | Loss: 6.4972 | Batch Acc: 0.1875\n",
            "Batch 20/72 | Loss: 5.3739 | Batch Acc: 0.1875\n",
            "Batch 21/72 | Loss: 4.9434 | Batch Acc: 0.4375\n",
            "Batch 22/72 | Loss: 4.2164 | Batch Acc: 0.3125\n",
            "Batch 23/72 | Loss: 5.7622 | Batch Acc: 0.2500\n",
            "Batch 24/72 | Loss: 6.6306 | Batch Acc: 0.1250\n",
            "Batch 25/72 | Loss: 5.3179 | Batch Acc: 0.4375\n",
            "Batch 26/72 | Loss: 5.3858 | Batch Acc: 0.3125\n",
            "Batch 27/72 | Loss: 6.1054 | Batch Acc: 0.2500\n",
            "Batch 28/72 | Loss: 4.6976 | Batch Acc: 0.3750\n",
            "Batch 29/72 | Loss: 4.7284 | Batch Acc: 0.2500\n",
            "Batch 30/72 | Loss: 5.7392 | Batch Acc: 0.3125\n",
            "Batch 31/72 | Loss: 5.0212 | Batch Acc: 0.3125\n",
            "Batch 32/72 | Loss: 5.1910 | Batch Acc: 0.1875\n",
            "Batch 33/72 | Loss: 5.1024 | Batch Acc: 0.3125\n",
            "Batch 34/72 | Loss: 6.9930 | Batch Acc: 0.0000\n",
            "Batch 35/72 | Loss: 5.2468 | Batch Acc: 0.1875\n",
            "Batch 36/72 | Loss: 5.6211 | Batch Acc: 0.3125\n",
            "Batch 37/72 | Loss: 4.6772 | Batch Acc: 0.3750\n",
            "Batch 38/72 | Loss: 4.8110 | Batch Acc: 0.3125\n",
            "Batch 39/72 | Loss: 5.2851 | Batch Acc: 0.2500\n",
            "Batch 40/72 | Loss: 5.5622 | Batch Acc: 0.2500\n",
            "Batch 41/72 | Loss: 4.8632 | Batch Acc: 0.3125\n",
            "Batch 42/72 | Loss: 6.2785 | Batch Acc: 0.1875\n",
            "Batch 43/72 | Loss: 5.6786 | Batch Acc: 0.1250\n",
            "Batch 44/72 | Loss: 5.2105 | Batch Acc: 0.2500\n",
            "Batch 45/72 | Loss: 5.7671 | Batch Acc: 0.1875\n",
            "Batch 46/72 | Loss: 5.0519 | Batch Acc: 0.0625\n",
            "Batch 47/72 | Loss: 6.8688 | Batch Acc: 0.1875\n",
            "Batch 48/72 | Loss: 5.0258 | Batch Acc: 0.3125\n",
            "Batch 49/72 | Loss: 5.1586 | Batch Acc: 0.2500\n",
            "Batch 50/72 | Loss: 4.8887 | Batch Acc: 0.2500\n",
            "Batch 51/72 | Loss: 4.7846 | Batch Acc: 0.1875\n",
            "Batch 52/72 | Loss: 5.5048 | Batch Acc: 0.3125\n",
            "Batch 53/72 | Loss: 5.1438 | Batch Acc: 0.1875\n",
            "Batch 54/72 | Loss: 6.1448 | Batch Acc: 0.2500\n",
            "Batch 55/72 | Loss: 6.3003 | Batch Acc: 0.1875\n",
            "Batch 56/72 | Loss: 5.6209 | Batch Acc: 0.2500\n",
            "Batch 57/72 | Loss: 6.4687 | Batch Acc: 0.1250\n",
            "Batch 58/72 | Loss: 6.9209 | Batch Acc: 0.2500\n",
            "Batch 59/72 | Loss: 5.4423 | Batch Acc: 0.3125\n",
            "Batch 60/72 | Loss: 5.2070 | Batch Acc: 0.3750\n",
            "Batch 61/72 | Loss: 6.5838 | Batch Acc: 0.1250\n",
            "Batch 62/72 | Loss: 5.3829 | Batch Acc: 0.3125\n",
            "Batch 63/72 | Loss: 5.2046 | Batch Acc: 0.1875\n",
            "Batch 64/72 | Loss: 6.1780 | Batch Acc: 0.1250\n",
            "Batch 65/72 | Loss: 6.6346 | Batch Acc: 0.1875\n",
            "Batch 66/72 | Loss: 5.6147 | Batch Acc: 0.3125\n",
            "Batch 67/72 | Loss: 5.5706 | Batch Acc: 0.3125\n",
            "Batch 68/72 | Loss: 4.7335 | Batch Acc: 0.1250\n",
            "Batch 69/72 | Loss: 6.6667 | Batch Acc: 0.0625\n",
            "Batch 70/72 | Loss: 5.2506 | Batch Acc: 0.2500\n",
            "Batch 71/72 | Loss: 6.0643 | Batch Acc: 0.3125\n",
            "Batch 72/72 | Loss: 5.6098 | Batch Acc: 0.3077\n",
            "Epoch Train Loss: 403.7386, Accuracy: 0.2332\n",
            "Validation Accuracy: 0.2216\n",
            "\n",
            "Epoch 15/50\n",
            "Batch 1/72 | Loss: 7.1046 | Batch Acc: 0.1250\n",
            "Batch 2/72 | Loss: 5.1012 | Batch Acc: 0.2500\n",
            "Batch 3/72 | Loss: 5.7992 | Batch Acc: 0.0625\n",
            "Batch 4/72 | Loss: 5.7681 | Batch Acc: 0.1875\n",
            "Batch 5/72 | Loss: 6.0080 | Batch Acc: 0.1875\n",
            "Batch 6/72 | Loss: 4.8898 | Batch Acc: 0.1875\n",
            "Batch 7/72 | Loss: 6.0003 | Batch Acc: 0.1250\n",
            "Batch 8/72 | Loss: 5.0022 | Batch Acc: 0.1875\n",
            "Batch 9/72 | Loss: 5.8919 | Batch Acc: 0.1875\n",
            "Batch 10/72 | Loss: 5.1073 | Batch Acc: 0.1875\n",
            "Batch 11/72 | Loss: 7.1132 | Batch Acc: 0.0625\n",
            "Batch 12/72 | Loss: 6.1485 | Batch Acc: 0.2500\n",
            "Batch 13/72 | Loss: 5.0660 | Batch Acc: 0.3750\n",
            "Batch 14/72 | Loss: 5.9974 | Batch Acc: 0.1875\n",
            "Batch 15/72 | Loss: 4.0084 | Batch Acc: 0.3750\n",
            "Batch 16/72 | Loss: 5.0693 | Batch Acc: 0.1875\n",
            "Batch 17/72 | Loss: 4.7653 | Batch Acc: 0.3125\n",
            "Batch 18/72 | Loss: 6.0306 | Batch Acc: 0.1250\n",
            "Batch 19/72 | Loss: 4.8473 | Batch Acc: 0.4375\n",
            "Batch 20/72 | Loss: 4.8827 | Batch Acc: 0.2500\n",
            "Batch 21/72 | Loss: 4.9751 | Batch Acc: 0.3125\n",
            "Batch 22/72 | Loss: 6.0821 | Batch Acc: 0.1875\n",
            "Batch 23/72 | Loss: 5.2108 | Batch Acc: 0.1250\n",
            "Batch 24/72 | Loss: 6.4368 | Batch Acc: 0.1250\n",
            "Batch 25/72 | Loss: 5.0759 | Batch Acc: 0.2500\n",
            "Batch 26/72 | Loss: 5.2666 | Batch Acc: 0.3750\n",
            "Batch 27/72 | Loss: 4.4213 | Batch Acc: 0.2500\n",
            "Batch 28/72 | Loss: 5.6128 | Batch Acc: 0.1250\n",
            "Batch 29/72 | Loss: 5.7497 | Batch Acc: 0.2500\n",
            "Batch 30/72 | Loss: 5.7378 | Batch Acc: 0.0625\n",
            "Batch 31/72 | Loss: 4.8637 | Batch Acc: 0.3125\n",
            "Batch 32/72 | Loss: 4.8850 | Batch Acc: 0.2500\n",
            "Batch 33/72 | Loss: 5.6000 | Batch Acc: 0.3125\n",
            "Batch 34/72 | Loss: 6.6630 | Batch Acc: 0.1875\n",
            "Batch 35/72 | Loss: 6.8024 | Batch Acc: 0.0625\n",
            "Batch 36/72 | Loss: 5.9067 | Batch Acc: 0.3125\n",
            "Batch 37/72 | Loss: 5.9567 | Batch Acc: 0.1250\n",
            "Batch 38/72 | Loss: 5.4969 | Batch Acc: 0.3125\n",
            "Batch 39/72 | Loss: 4.9619 | Batch Acc: 0.2500\n",
            "Batch 40/72 | Loss: 4.7181 | Batch Acc: 0.3750\n",
            "Batch 41/72 | Loss: 4.9332 | Batch Acc: 0.1250\n",
            "Batch 42/72 | Loss: 6.4185 | Batch Acc: 0.0625\n",
            "Batch 43/72 | Loss: 5.3408 | Batch Acc: 0.1875\n",
            "Batch 44/72 | Loss: 5.8449 | Batch Acc: 0.1250\n",
            "Batch 45/72 | Loss: 5.6934 | Batch Acc: 0.1250\n",
            "Batch 46/72 | Loss: 5.0511 | Batch Acc: 0.1875\n",
            "Batch 47/72 | Loss: 5.7035 | Batch Acc: 0.1875\n",
            "Batch 48/72 | Loss: 5.6891 | Batch Acc: 0.2500\n",
            "Batch 49/72 | Loss: 6.0891 | Batch Acc: 0.1250\n",
            "Batch 50/72 | Loss: 5.3585 | Batch Acc: 0.1875\n",
            "Batch 51/72 | Loss: 5.4216 | Batch Acc: 0.3125\n",
            "Batch 52/72 | Loss: 6.0669 | Batch Acc: 0.3750\n",
            "Batch 53/72 | Loss: 5.8629 | Batch Acc: 0.2500\n",
            "Batch 54/72 | Loss: 5.4411 | Batch Acc: 0.1250\n",
            "Batch 55/72 | Loss: 5.6589 | Batch Acc: 0.1250\n",
            "Batch 56/72 | Loss: 5.6063 | Batch Acc: 0.4375\n",
            "Batch 57/72 | Loss: 6.5073 | Batch Acc: 0.1250\n",
            "Batch 58/72 | Loss: 5.6912 | Batch Acc: 0.3125\n",
            "Batch 59/72 | Loss: 5.2800 | Batch Acc: 0.3750\n",
            "Batch 60/72 | Loss: 5.4521 | Batch Acc: 0.1875\n",
            "Batch 61/72 | Loss: 5.7420 | Batch Acc: 0.1875\n",
            "Batch 62/72 | Loss: 6.0354 | Batch Acc: 0.1875\n",
            "Batch 63/72 | Loss: 5.4129 | Batch Acc: 0.3125\n",
            "Batch 64/72 | Loss: 5.8355 | Batch Acc: 0.1875\n",
            "Batch 65/72 | Loss: 5.7357 | Batch Acc: 0.1250\n",
            "Batch 66/72 | Loss: 6.5155 | Batch Acc: 0.1875\n",
            "Batch 67/72 | Loss: 6.0300 | Batch Acc: 0.3125\n",
            "Batch 68/72 | Loss: 5.9915 | Batch Acc: 0.1250\n",
            "Batch 69/72 | Loss: 4.5589 | Batch Acc: 0.2500\n",
            "Batch 70/72 | Loss: 6.8669 | Batch Acc: 0.1250\n",
            "Batch 71/72 | Loss: 6.1403 | Batch Acc: 0.0000\n",
            "Batch 72/72 | Loss: 5.4645 | Batch Acc: 0.1538\n",
            "Epoch Train Loss: 404.4639, Accuracy: 0.2106\n",
            "Validation Accuracy: 0.2614\n",
            "✅ 儲存最佳模型（Acc: 0.2614）\n",
            "\n",
            "Epoch 16/50\n",
            "Batch 1/72 | Loss: 5.7658 | Batch Acc: 0.2500\n",
            "Batch 2/72 | Loss: 5.6806 | Batch Acc: 0.3125\n",
            "Batch 3/72 | Loss: 5.1192 | Batch Acc: 0.0000\n",
            "Batch 4/72 | Loss: 5.9146 | Batch Acc: 0.1875\n",
            "Batch 5/72 | Loss: 5.4151 | Batch Acc: 0.2500\n",
            "Batch 6/72 | Loss: 4.9885 | Batch Acc: 0.2500\n",
            "Batch 7/72 | Loss: 5.8479 | Batch Acc: 0.2500\n",
            "Batch 8/72 | Loss: 7.3466 | Batch Acc: 0.1875\n",
            "Batch 9/72 | Loss: 5.7953 | Batch Acc: 0.1875\n",
            "Batch 10/72 | Loss: 5.3153 | Batch Acc: 0.1875\n",
            "Batch 11/72 | Loss: 5.8195 | Batch Acc: 0.3125\n",
            "Batch 12/72 | Loss: 5.6121 | Batch Acc: 0.1250\n",
            "Batch 13/72 | Loss: 5.1611 | Batch Acc: 0.0625\n",
            "Batch 14/72 | Loss: 6.0673 | Batch Acc: 0.2500\n",
            "Batch 15/72 | Loss: 5.5827 | Batch Acc: 0.2500\n",
            "Batch 16/72 | Loss: 5.1426 | Batch Acc: 0.2500\n",
            "Batch 17/72 | Loss: 4.6998 | Batch Acc: 0.3125\n",
            "Batch 18/72 | Loss: 6.2987 | Batch Acc: 0.2500\n",
            "Batch 19/72 | Loss: 4.2298 | Batch Acc: 0.4375\n",
            "Batch 20/72 | Loss: 5.9744 | Batch Acc: 0.1875\n",
            "Batch 21/72 | Loss: 6.4761 | Batch Acc: 0.1250\n",
            "Batch 22/72 | Loss: 5.9992 | Batch Acc: 0.1875\n",
            "Batch 23/72 | Loss: 5.0701 | Batch Acc: 0.3750\n",
            "Batch 24/72 | Loss: 5.4424 | Batch Acc: 0.3125\n",
            "Batch 25/72 | Loss: 4.9451 | Batch Acc: 0.2500\n",
            "Batch 26/72 | Loss: 5.0465 | Batch Acc: 0.2500\n",
            "Batch 27/72 | Loss: 5.6425 | Batch Acc: 0.1250\n",
            "Batch 28/72 | Loss: 5.3676 | Batch Acc: 0.3125\n",
            "Batch 29/72 | Loss: 5.4730 | Batch Acc: 0.4375\n",
            "Batch 30/72 | Loss: 6.0582 | Batch Acc: 0.1250\n",
            "Batch 31/72 | Loss: 5.3277 | Batch Acc: 0.2500\n",
            "Batch 32/72 | Loss: 5.2472 | Batch Acc: 0.3125\n",
            "Batch 33/72 | Loss: 5.0047 | Batch Acc: 0.3750\n",
            "Batch 34/72 | Loss: 5.9390 | Batch Acc: 0.1875\n",
            "Batch 35/72 | Loss: 5.9675 | Batch Acc: 0.3750\n",
            "Batch 36/72 | Loss: 5.8131 | Batch Acc: 0.1875\n",
            "Batch 37/72 | Loss: 5.2904 | Batch Acc: 0.3125\n",
            "Batch 38/72 | Loss: 5.5663 | Batch Acc: 0.1250\n",
            "Batch 39/72 | Loss: 7.0727 | Batch Acc: 0.1875\n",
            "Batch 40/72 | Loss: 5.6878 | Batch Acc: 0.1875\n",
            "Batch 41/72 | Loss: 4.9630 | Batch Acc: 0.2500\n",
            "Batch 42/72 | Loss: 5.4533 | Batch Acc: 0.1875\n",
            "Batch 43/72 | Loss: 5.2316 | Batch Acc: 0.4375\n",
            "Batch 44/72 | Loss: 6.3102 | Batch Acc: 0.2500\n",
            "Batch 45/72 | Loss: 6.9715 | Batch Acc: 0.0625\n",
            "Batch 46/72 | Loss: 6.0038 | Batch Acc: 0.0625\n",
            "Batch 47/72 | Loss: 6.2118 | Batch Acc: 0.1250\n",
            "Batch 48/72 | Loss: 5.1576 | Batch Acc: 0.1875\n",
            "Batch 49/72 | Loss: 5.2202 | Batch Acc: 0.3125\n",
            "Batch 50/72 | Loss: 5.1430 | Batch Acc: 0.2500\n",
            "Batch 51/72 | Loss: 5.0657 | Batch Acc: 0.3750\n",
            "Batch 52/72 | Loss: 5.5078 | Batch Acc: 0.1875\n",
            "Batch 53/72 | Loss: 5.7202 | Batch Acc: 0.2500\n",
            "Batch 54/72 | Loss: 6.7976 | Batch Acc: 0.1875\n",
            "Batch 55/72 | Loss: 5.3578 | Batch Acc: 0.2500\n",
            "Batch 56/72 | Loss: 5.2340 | Batch Acc: 0.3125\n",
            "Batch 57/72 | Loss: 5.2628 | Batch Acc: 0.1875\n",
            "Batch 58/72 | Loss: 5.5443 | Batch Acc: 0.0625\n",
            "Batch 59/72 | Loss: 5.5461 | Batch Acc: 0.1875\n",
            "Batch 60/72 | Loss: 4.8336 | Batch Acc: 0.3125\n",
            "Batch 61/72 | Loss: 4.9724 | Batch Acc: 0.3125\n",
            "Batch 62/72 | Loss: 5.6808 | Batch Acc: 0.0000\n",
            "Batch 63/72 | Loss: 5.2576 | Batch Acc: 0.3125\n",
            "Batch 64/72 | Loss: 4.9142 | Batch Acc: 0.1875\n",
            "Batch 65/72 | Loss: 5.0373 | Batch Acc: 0.3750\n",
            "Batch 66/72 | Loss: 5.0211 | Batch Acc: 0.3750\n",
            "Batch 67/72 | Loss: 5.5750 | Batch Acc: 0.1875\n",
            "Batch 68/72 | Loss: 5.8411 | Batch Acc: 0.1875\n",
            "Batch 69/72 | Loss: 5.0065 | Batch Acc: 0.2500\n",
            "Batch 70/72 | Loss: 5.9124 | Batch Acc: 0.2500\n",
            "Batch 71/72 | Loss: 6.5834 | Batch Acc: 0.2500\n",
            "Batch 72/72 | Loss: 5.6574 | Batch Acc: 0.2308\n",
            "Epoch Train Loss: 401.2087, Accuracy: 0.2341\n",
            "Validation Accuracy: 0.2841\n",
            "✅ 儲存最佳模型（Acc: 0.2841）\n",
            "\n",
            "Epoch 17/50\n",
            "Batch 1/72 | Loss: 5.1344 | Batch Acc: 0.2500\n",
            "Batch 2/72 | Loss: 5.6274 | Batch Acc: 0.2500\n",
            "Batch 3/72 | Loss: 4.6182 | Batch Acc: 0.3125\n",
            "Batch 4/72 | Loss: 5.1533 | Batch Acc: 0.2500\n",
            "Batch 5/72 | Loss: 6.3580 | Batch Acc: 0.3750\n",
            "Batch 6/72 | Loss: 6.1612 | Batch Acc: 0.0625\n",
            "Batch 7/72 | Loss: 5.8546 | Batch Acc: 0.0625\n",
            "Batch 8/72 | Loss: 5.2418 | Batch Acc: 0.2500\n",
            "Batch 9/72 | Loss: 5.1541 | Batch Acc: 0.3750\n",
            "Batch 10/72 | Loss: 4.9731 | Batch Acc: 0.3750\n",
            "Batch 11/72 | Loss: 5.9013 | Batch Acc: 0.0625\n",
            "Batch 12/72 | Loss: 5.3306 | Batch Acc: 0.1875\n",
            "Batch 13/72 | Loss: 5.8446 | Batch Acc: 0.1875\n",
            "Batch 14/72 | Loss: 5.1644 | Batch Acc: 0.1875\n",
            "Batch 15/72 | Loss: 5.4695 | Batch Acc: 0.3125\n",
            "Batch 16/72 | Loss: 5.9238 | Batch Acc: 0.0625\n",
            "Batch 17/72 | Loss: 5.6521 | Batch Acc: 0.1875\n",
            "Batch 18/72 | Loss: 4.5403 | Batch Acc: 0.3750\n",
            "Batch 19/72 | Loss: 4.5510 | Batch Acc: 0.3750\n",
            "Batch 20/72 | Loss: 5.3619 | Batch Acc: 0.1875\n",
            "Batch 21/72 | Loss: 5.0712 | Batch Acc: 0.1875\n",
            "Batch 22/72 | Loss: 4.0255 | Batch Acc: 0.4375\n",
            "Batch 23/72 | Loss: 6.1626 | Batch Acc: 0.1875\n",
            "Batch 24/72 | Loss: 5.6150 | Batch Acc: 0.2500\n",
            "Batch 25/72 | Loss: 5.8602 | Batch Acc: 0.0625\n",
            "Batch 26/72 | Loss: 5.0754 | Batch Acc: 0.5000\n",
            "Batch 27/72 | Loss: 5.3727 | Batch Acc: 0.3125\n",
            "Batch 28/72 | Loss: 7.2456 | Batch Acc: 0.3125\n",
            "Batch 29/72 | Loss: 6.0792 | Batch Acc: 0.1875\n",
            "Batch 30/72 | Loss: 6.1987 | Batch Acc: 0.1250\n",
            "Batch 31/72 | Loss: 5.9496 | Batch Acc: 0.1875\n",
            "Batch 32/72 | Loss: 4.8495 | Batch Acc: 0.3750\n",
            "Batch 33/72 | Loss: 4.3649 | Batch Acc: 0.3125\n",
            "Batch 34/72 | Loss: 5.7881 | Batch Acc: 0.2500\n",
            "Batch 35/72 | Loss: 5.8400 | Batch Acc: 0.1250\n",
            "Batch 36/72 | Loss: 5.4623 | Batch Acc: 0.1250\n",
            "Batch 37/72 | Loss: 5.9261 | Batch Acc: 0.1250\n",
            "Batch 38/72 | Loss: 5.8306 | Batch Acc: 0.3125\n",
            "Batch 39/72 | Loss: 5.3858 | Batch Acc: 0.1250\n",
            "Batch 40/72 | Loss: 3.8240 | Batch Acc: 0.5000\n",
            "Batch 41/72 | Loss: 5.1492 | Batch Acc: 0.1875\n",
            "Batch 42/72 | Loss: 5.1580 | Batch Acc: 0.3750\n",
            "Batch 43/72 | Loss: 5.2473 | Batch Acc: 0.2500\n",
            "Batch 44/72 | Loss: 5.6955 | Batch Acc: 0.4375\n",
            "Batch 45/72 | Loss: 4.2801 | Batch Acc: 0.3125\n",
            "Batch 46/72 | Loss: 5.8980 | Batch Acc: 0.1250\n",
            "Batch 47/72 | Loss: 5.7320 | Batch Acc: 0.1875\n",
            "Batch 48/72 | Loss: 5.4709 | Batch Acc: 0.4375\n",
            "Batch 49/72 | Loss: 4.9954 | Batch Acc: 0.1875\n",
            "Batch 50/72 | Loss: 5.1017 | Batch Acc: 0.2500\n",
            "Batch 51/72 | Loss: 6.0453 | Batch Acc: 0.2500\n",
            "Batch 52/72 | Loss: 5.0965 | Batch Acc: 0.1250\n",
            "Batch 53/72 | Loss: 5.3897 | Batch Acc: 0.1250\n",
            "Batch 54/72 | Loss: 6.0900 | Batch Acc: 0.1875\n",
            "Batch 55/72 | Loss: 5.6841 | Batch Acc: 0.3125\n",
            "Batch 56/72 | Loss: 4.9044 | Batch Acc: 0.3125\n",
            "Batch 57/72 | Loss: 6.2903 | Batch Acc: 0.0625\n",
            "Batch 58/72 | Loss: 6.2120 | Batch Acc: 0.1250\n",
            "Batch 59/72 | Loss: 5.9433 | Batch Acc: 0.1875\n",
            "Batch 60/72 | Loss: 4.4129 | Batch Acc: 0.3125\n",
            "Batch 61/72 | Loss: 5.5594 | Batch Acc: 0.1250\n",
            "Batch 62/72 | Loss: 5.7893 | Batch Acc: 0.2500\n",
            "Batch 63/72 | Loss: 4.4587 | Batch Acc: 0.2500\n",
            "Batch 64/72 | Loss: 5.5523 | Batch Acc: 0.1875\n",
            "Batch 65/72 | Loss: 4.5171 | Batch Acc: 0.3750\n",
            "Batch 66/72 | Loss: 6.5044 | Batch Acc: 0.0625\n",
            "Batch 67/72 | Loss: 4.3074 | Batch Acc: 0.3750\n",
            "Batch 68/72 | Loss: 5.2228 | Batch Acc: 0.2500\n",
            "Batch 69/72 | Loss: 4.4605 | Batch Acc: 0.3750\n",
            "Batch 70/72 | Loss: 5.8103 | Batch Acc: 0.2500\n",
            "Batch 71/72 | Loss: 4.9693 | Batch Acc: 0.1875\n",
            "Batch 72/72 | Loss: 5.5794 | Batch Acc: 0.2308\n",
            "Epoch Train Loss: 389.4700, Accuracy: 0.2411\n",
            "Validation Accuracy: 0.2898\n",
            "✅ 儲存最佳模型（Acc: 0.2898）\n",
            "\n",
            "Epoch 18/50\n",
            "Batch 1/72 | Loss: 4.5565 | Batch Acc: 0.3125\n",
            "Batch 2/72 | Loss: 6.2328 | Batch Acc: 0.1875\n",
            "Batch 3/72 | Loss: 6.0621 | Batch Acc: 0.2500\n",
            "Batch 4/72 | Loss: 6.8750 | Batch Acc: 0.0625\n",
            "Batch 5/72 | Loss: 5.5312 | Batch Acc: 0.2500\n",
            "Batch 6/72 | Loss: 7.4177 | Batch Acc: 0.0625\n",
            "Batch 7/72 | Loss: 6.2260 | Batch Acc: 0.1875\n",
            "Batch 8/72 | Loss: 5.5531 | Batch Acc: 0.1875\n",
            "Batch 9/72 | Loss: 5.2624 | Batch Acc: 0.1250\n",
            "Batch 10/72 | Loss: 5.0959 | Batch Acc: 0.2500\n",
            "Batch 11/72 | Loss: 6.2139 | Batch Acc: 0.1875\n",
            "Batch 12/72 | Loss: 6.3831 | Batch Acc: 0.1875\n",
            "Batch 13/72 | Loss: 6.3615 | Batch Acc: 0.1875\n",
            "Batch 14/72 | Loss: 5.1293 | Batch Acc: 0.2500\n",
            "Batch 15/72 | Loss: 6.0059 | Batch Acc: 0.1250\n",
            "Batch 16/72 | Loss: 5.3892 | Batch Acc: 0.1875\n",
            "Batch 17/72 | Loss: 6.2203 | Batch Acc: 0.1875\n",
            "Batch 18/72 | Loss: 5.7608 | Batch Acc: 0.0625\n",
            "Batch 19/72 | Loss: 4.4499 | Batch Acc: 0.2500\n",
            "Batch 20/72 | Loss: 5.4484 | Batch Acc: 0.2500\n",
            "Batch 21/72 | Loss: 4.6869 | Batch Acc: 0.3750\n",
            "Batch 22/72 | Loss: 5.1531 | Batch Acc: 0.2500\n",
            "Batch 23/72 | Loss: 4.7483 | Batch Acc: 0.2500\n",
            "Batch 24/72 | Loss: 5.2658 | Batch Acc: 0.2500\n",
            "Batch 25/72 | Loss: 5.6082 | Batch Acc: 0.2500\n",
            "Batch 26/72 | Loss: 5.3593 | Batch Acc: 0.3125\n",
            "Batch 27/72 | Loss: 6.8676 | Batch Acc: 0.1875\n",
            "Batch 28/72 | Loss: 5.8161 | Batch Acc: 0.1250\n",
            "Batch 29/72 | Loss: 5.4715 | Batch Acc: 0.1250\n",
            "Batch 30/72 | Loss: 4.1225 | Batch Acc: 0.2500\n",
            "Batch 31/72 | Loss: 4.0694 | Batch Acc: 0.3750\n",
            "Batch 32/72 | Loss: 4.4811 | Batch Acc: 0.2500\n",
            "Batch 33/72 | Loss: 5.6969 | Batch Acc: 0.0000\n",
            "Batch 34/72 | Loss: 6.6073 | Batch Acc: 0.1250\n",
            "Batch 35/72 | Loss: 4.5522 | Batch Acc: 0.1875\n",
            "Batch 36/72 | Loss: 5.5521 | Batch Acc: 0.2500\n",
            "Batch 37/72 | Loss: 5.8328 | Batch Acc: 0.2500\n",
            "Batch 38/72 | Loss: 5.5668 | Batch Acc: 0.3750\n",
            "Batch 39/72 | Loss: 4.8168 | Batch Acc: 0.3125\n",
            "Batch 40/72 | Loss: 6.2315 | Batch Acc: 0.1250\n",
            "Batch 41/72 | Loss: 4.7987 | Batch Acc: 0.3750\n",
            "Batch 42/72 | Loss: 6.3948 | Batch Acc: 0.1875\n",
            "Batch 43/72 | Loss: 5.4690 | Batch Acc: 0.2500\n",
            "Batch 44/72 | Loss: 6.4572 | Batch Acc: 0.1250\n",
            "Batch 45/72 | Loss: 6.3539 | Batch Acc: 0.1875\n",
            "Batch 46/72 | Loss: 4.8842 | Batch Acc: 0.2500\n",
            "Batch 47/72 | Loss: 5.2728 | Batch Acc: 0.2500\n",
            "Batch 48/72 | Loss: 5.9078 | Batch Acc: 0.1250\n",
            "Batch 49/72 | Loss: 5.9006 | Batch Acc: 0.1875\n",
            "Batch 50/72 | Loss: 5.5333 | Batch Acc: 0.1875\n",
            "Batch 51/72 | Loss: 5.5166 | Batch Acc: 0.3125\n",
            "Batch 52/72 | Loss: 5.5448 | Batch Acc: 0.2500\n",
            "Batch 53/72 | Loss: 4.1591 | Batch Acc: 0.4375\n",
            "Batch 54/72 | Loss: 5.5974 | Batch Acc: 0.1875\n",
            "Batch 55/72 | Loss: 5.1956 | Batch Acc: 0.1875\n",
            "Batch 56/72 | Loss: 5.1778 | Batch Acc: 0.1875\n",
            "Batch 57/72 | Loss: 5.6444 | Batch Acc: 0.3125\n",
            "Batch 58/72 | Loss: 5.6454 | Batch Acc: 0.2500\n",
            "Batch 59/72 | Loss: 4.6404 | Batch Acc: 0.3125\n",
            "Batch 60/72 | Loss: 7.1507 | Batch Acc: 0.1250\n",
            "Batch 61/72 | Loss: 4.5492 | Batch Acc: 0.3125\n",
            "Batch 62/72 | Loss: 5.3565 | Batch Acc: 0.3750\n",
            "Batch 63/72 | Loss: 5.1976 | Batch Acc: 0.2500\n",
            "Batch 64/72 | Loss: 4.8864 | Batch Acc: 0.3125\n",
            "Batch 65/72 | Loss: 5.1829 | Batch Acc: 0.3750\n",
            "Batch 66/72 | Loss: 5.7555 | Batch Acc: 0.2500\n",
            "Batch 67/72 | Loss: 6.2995 | Batch Acc: 0.1250\n",
            "Batch 68/72 | Loss: 5.1958 | Batch Acc: 0.2500\n",
            "Batch 69/72 | Loss: 4.9242 | Batch Acc: 0.2500\n",
            "Batch 70/72 | Loss: 6.8101 | Batch Acc: 0.1875\n",
            "Batch 71/72 | Loss: 4.1905 | Batch Acc: 0.4375\n",
            "Batch 72/72 | Loss: 6.0714 | Batch Acc: 0.1538\n",
            "Epoch Train Loss: 398.3450, Accuracy: 0.2272\n",
            "Validation Accuracy: 0.2841\n",
            "\n",
            "Epoch 19/50\n",
            "Batch 1/72 | Loss: 5.6669 | Batch Acc: 0.0625\n",
            "Batch 2/72 | Loss: 5.4083 | Batch Acc: 0.3750\n",
            "Batch 3/72 | Loss: 5.5628 | Batch Acc: 0.3750\n",
            "Batch 4/72 | Loss: 5.0947 | Batch Acc: 0.2500\n",
            "Batch 5/72 | Loss: 5.3460 | Batch Acc: 0.2500\n",
            "Batch 6/72 | Loss: 5.5301 | Batch Acc: 0.3125\n",
            "Batch 7/72 | Loss: 4.3593 | Batch Acc: 0.4375\n",
            "Batch 8/72 | Loss: 5.3831 | Batch Acc: 0.0625\n",
            "Batch 9/72 | Loss: 6.5945 | Batch Acc: 0.1875\n",
            "Batch 10/72 | Loss: 4.9818 | Batch Acc: 0.1875\n",
            "Batch 11/72 | Loss: 6.0030 | Batch Acc: 0.1875\n",
            "Batch 12/72 | Loss: 5.2207 | Batch Acc: 0.1250\n",
            "Batch 13/72 | Loss: 5.6332 | Batch Acc: 0.1875\n",
            "Batch 14/72 | Loss: 5.2967 | Batch Acc: 0.1875\n",
            "Batch 15/72 | Loss: 5.4703 | Batch Acc: 0.2500\n",
            "Batch 16/72 | Loss: 5.9014 | Batch Acc: 0.1875\n",
            "Batch 17/72 | Loss: 4.9713 | Batch Acc: 0.3125\n",
            "Batch 18/72 | Loss: 5.9976 | Batch Acc: 0.1250\n",
            "Batch 19/72 | Loss: 5.2516 | Batch Acc: 0.2500\n",
            "Batch 20/72 | Loss: 5.8413 | Batch Acc: 0.1250\n",
            "Batch 21/72 | Loss: 4.6382 | Batch Acc: 0.3750\n",
            "Batch 22/72 | Loss: 6.3884 | Batch Acc: 0.1875\n",
            "Batch 23/72 | Loss: 5.7227 | Batch Acc: 0.2500\n",
            "Batch 24/72 | Loss: 4.9292 | Batch Acc: 0.3125\n",
            "Batch 25/72 | Loss: 5.7196 | Batch Acc: 0.1875\n",
            "Batch 26/72 | Loss: 6.4203 | Batch Acc: 0.3125\n",
            "Batch 27/72 | Loss: 5.2604 | Batch Acc: 0.3125\n",
            "Batch 28/72 | Loss: 5.2137 | Batch Acc: 0.3125\n",
            "Batch 29/72 | Loss: 5.3692 | Batch Acc: 0.1250\n",
            "Batch 30/72 | Loss: 4.9872 | Batch Acc: 0.1250\n",
            "Batch 31/72 | Loss: 5.3704 | Batch Acc: 0.3125\n",
            "Batch 32/72 | Loss: 5.8720 | Batch Acc: 0.2500\n",
            "Batch 33/72 | Loss: 5.6722 | Batch Acc: 0.1250\n",
            "Batch 34/72 | Loss: 5.9797 | Batch Acc: 0.2500\n",
            "Batch 35/72 | Loss: 4.5960 | Batch Acc: 0.3750\n",
            "Batch 36/72 | Loss: 4.1012 | Batch Acc: 0.3125\n",
            "Batch 37/72 | Loss: 5.4972 | Batch Acc: 0.1250\n",
            "Batch 38/72 | Loss: 3.5875 | Batch Acc: 0.3750\n",
            "Batch 39/72 | Loss: 5.1783 | Batch Acc: 0.2500\n",
            "Batch 40/72 | Loss: 5.5865 | Batch Acc: 0.2500\n",
            "Batch 41/72 | Loss: 5.4480 | Batch Acc: 0.3125\n",
            "Batch 42/72 | Loss: 4.2915 | Batch Acc: 0.3750\n",
            "Batch 43/72 | Loss: 4.6299 | Batch Acc: 0.3750\n",
            "Batch 44/72 | Loss: 6.3754 | Batch Acc: 0.1875\n",
            "Batch 45/72 | Loss: 5.5611 | Batch Acc: 0.1250\n",
            "Batch 46/72 | Loss: 4.5743 | Batch Acc: 0.3125\n",
            "Batch 47/72 | Loss: 6.0169 | Batch Acc: 0.3750\n",
            "Batch 48/72 | Loss: 6.0524 | Batch Acc: 0.1250\n",
            "Batch 49/72 | Loss: 4.8238 | Batch Acc: 0.3125\n",
            "Batch 50/72 | Loss: 5.4875 | Batch Acc: 0.1250\n",
            "Batch 51/72 | Loss: 5.6746 | Batch Acc: 0.3750\n",
            "Batch 52/72 | Loss: 5.2395 | Batch Acc: 0.3125\n",
            "Batch 53/72 | Loss: 5.6572 | Batch Acc: 0.0625\n",
            "Batch 54/72 | Loss: 5.1598 | Batch Acc: 0.3125\n",
            "Batch 55/72 | Loss: 5.7403 | Batch Acc: 0.1875\n",
            "Batch 56/72 | Loss: 5.1486 | Batch Acc: 0.2500\n",
            "Batch 57/72 | Loss: 5.9258 | Batch Acc: 0.2500\n",
            "Batch 58/72 | Loss: 5.8680 | Batch Acc: 0.1250\n",
            "Batch 59/72 | Loss: 4.9109 | Batch Acc: 0.2500\n",
            "Batch 60/72 | Loss: 4.3838 | Batch Acc: 0.3125\n",
            "Batch 61/72 | Loss: 5.5117 | Batch Acc: 0.0625\n",
            "Batch 62/72 | Loss: 5.3240 | Batch Acc: 0.2500\n",
            "Batch 63/72 | Loss: 4.9904 | Batch Acc: 0.1875\n",
            "Batch 64/72 | Loss: 5.0520 | Batch Acc: 0.1250\n",
            "Batch 65/72 | Loss: 6.0803 | Batch Acc: 0.2500\n",
            "Batch 66/72 | Loss: 5.9339 | Batch Acc: 0.1250\n",
            "Batch 67/72 | Loss: 4.6651 | Batch Acc: 0.3750\n",
            "Batch 68/72 | Loss: 4.8124 | Batch Acc: 0.3750\n",
            "Batch 69/72 | Loss: 6.4397 | Batch Acc: 0.0625\n",
            "Batch 70/72 | Loss: 6.1314 | Batch Acc: 0.1875\n",
            "Batch 71/72 | Loss: 4.9932 | Batch Acc: 0.3750\n",
            "Batch 72/72 | Loss: 5.0165 | Batch Acc: 0.0769\n",
            "Epoch Train Loss: 387.5243, Accuracy: 0.2376\n",
            "Validation Accuracy: 0.2955\n",
            "✅ 儲存最佳模型（Acc: 0.2955）\n",
            "\n",
            "Epoch 20/50\n",
            "Batch 1/72 | Loss: 5.9142 | Batch Acc: 0.0625\n",
            "Batch 2/72 | Loss: 4.7230 | Batch Acc: 0.2500\n",
            "Batch 3/72 | Loss: 5.6089 | Batch Acc: 0.1875\n",
            "Batch 4/72 | Loss: 4.4630 | Batch Acc: 0.3750\n",
            "Batch 5/72 | Loss: 5.9689 | Batch Acc: 0.3750\n",
            "Batch 6/72 | Loss: 5.4766 | Batch Acc: 0.3125\n",
            "Batch 7/72 | Loss: 4.9408 | Batch Acc: 0.3750\n",
            "Batch 8/72 | Loss: 5.8671 | Batch Acc: 0.3125\n",
            "Batch 9/72 | Loss: 5.7926 | Batch Acc: 0.1250\n",
            "Batch 10/72 | Loss: 5.9424 | Batch Acc: 0.1875\n",
            "Batch 11/72 | Loss: 4.0786 | Batch Acc: 0.2500\n",
            "Batch 12/72 | Loss: 5.0243 | Batch Acc: 0.3125\n",
            "Batch 13/72 | Loss: 4.7177 | Batch Acc: 0.3125\n",
            "Batch 14/72 | Loss: 7.0017 | Batch Acc: 0.1875\n",
            "Batch 15/72 | Loss: 4.9066 | Batch Acc: 0.2500\n",
            "Batch 16/72 | Loss: 4.2690 | Batch Acc: 0.3125\n",
            "Batch 17/72 | Loss: 6.2807 | Batch Acc: 0.0625\n",
            "Batch 18/72 | Loss: 5.3100 | Batch Acc: 0.1875\n",
            "Batch 19/72 | Loss: 6.5919 | Batch Acc: 0.2500\n",
            "Batch 20/72 | Loss: 5.9152 | Batch Acc: 0.2500\n",
            "Batch 21/72 | Loss: 5.0467 | Batch Acc: 0.2500\n",
            "Batch 22/72 | Loss: 4.9099 | Batch Acc: 0.2500\n",
            "Batch 23/72 | Loss: 5.4199 | Batch Acc: 0.3750\n",
            "Batch 24/72 | Loss: 5.1027 | Batch Acc: 0.3125\n",
            "Batch 25/72 | Loss: 5.9237 | Batch Acc: 0.1875\n",
            "Batch 26/72 | Loss: 4.9811 | Batch Acc: 0.3125\n",
            "Batch 27/72 | Loss: 5.4476 | Batch Acc: 0.0625\n",
            "Batch 28/72 | Loss: 5.6645 | Batch Acc: 0.1250\n",
            "Batch 29/72 | Loss: 4.1629 | Batch Acc: 0.3125\n",
            "Batch 30/72 | Loss: 5.0448 | Batch Acc: 0.4375\n",
            "Batch 31/72 | Loss: 4.3752 | Batch Acc: 0.5000\n",
            "Batch 32/72 | Loss: 6.3559 | Batch Acc: 0.1875\n",
            "Batch 33/72 | Loss: 4.8592 | Batch Acc: 0.4375\n",
            "Batch 34/72 | Loss: 4.9689 | Batch Acc: 0.5000\n",
            "Batch 35/72 | Loss: 6.2009 | Batch Acc: 0.1875\n",
            "Batch 36/72 | Loss: 4.3916 | Batch Acc: 0.2500\n",
            "Batch 37/72 | Loss: 5.9673 | Batch Acc: 0.1250\n",
            "Batch 38/72 | Loss: 5.2751 | Batch Acc: 0.0625\n",
            "Batch 39/72 | Loss: 6.4632 | Batch Acc: 0.1875\n",
            "Batch 40/72 | Loss: 4.8668 | Batch Acc: 0.4375\n",
            "Batch 41/72 | Loss: 5.2461 | Batch Acc: 0.1875\n",
            "Batch 42/72 | Loss: 4.9949 | Batch Acc: 0.1875\n",
            "Batch 43/72 | Loss: 4.3288 | Batch Acc: 0.3750\n",
            "Batch 44/72 | Loss: 5.6270 | Batch Acc: 0.1250\n",
            "Batch 45/72 | Loss: 4.6237 | Batch Acc: 0.1875\n",
            "Batch 46/72 | Loss: 4.9589 | Batch Acc: 0.3125\n",
            "Batch 47/72 | Loss: 5.3796 | Batch Acc: 0.1875\n",
            "Batch 48/72 | Loss: 6.3206 | Batch Acc: 0.0625\n",
            "Batch 49/72 | Loss: 5.4510 | Batch Acc: 0.2500\n",
            "Batch 50/72 | Loss: 7.7847 | Batch Acc: 0.0000\n",
            "Batch 51/72 | Loss: 5.1428 | Batch Acc: 0.1875\n",
            "Batch 52/72 | Loss: 4.2273 | Batch Acc: 0.3750\n",
            "Batch 53/72 | Loss: 4.0223 | Batch Acc: 0.4375\n",
            "Batch 54/72 | Loss: 4.7102 | Batch Acc: 0.4375\n",
            "Batch 55/72 | Loss: 5.2921 | Batch Acc: 0.1250\n",
            "Batch 56/72 | Loss: 4.8404 | Batch Acc: 0.3750\n",
            "Batch 57/72 | Loss: 5.7907 | Batch Acc: 0.2500\n",
            "Batch 58/72 | Loss: 6.2363 | Batch Acc: 0.0625\n",
            "Batch 59/72 | Loss: 5.6480 | Batch Acc: 0.1875\n",
            "Batch 60/72 | Loss: 5.5978 | Batch Acc: 0.2500\n",
            "Batch 61/72 | Loss: 5.1013 | Batch Acc: 0.3125\n",
            "Batch 62/72 | Loss: 6.4810 | Batch Acc: 0.2500\n",
            "Batch 63/72 | Loss: 5.3357 | Batch Acc: 0.1875\n",
            "Batch 64/72 | Loss: 6.5488 | Batch Acc: 0.1250\n",
            "Batch 65/72 | Loss: 4.9725 | Batch Acc: 0.3125\n",
            "Batch 66/72 | Loss: 5.4394 | Batch Acc: 0.1875\n",
            "Batch 67/72 | Loss: 5.9113 | Batch Acc: 0.1250\n",
            "Batch 68/72 | Loss: 5.3017 | Batch Acc: 0.1875\n",
            "Batch 69/72 | Loss: 4.8542 | Batch Acc: 0.3750\n",
            "Batch 70/72 | Loss: 4.8655 | Batch Acc: 0.4375\n",
            "Batch 71/72 | Loss: 5.9402 | Batch Acc: 0.1250\n",
            "Batch 72/72 | Loss: 6.0103 | Batch Acc: 0.3846\n",
            "Epoch Train Loss: 387.2065, Accuracy: 0.2498\n",
            "Validation Accuracy: 0.2784\n",
            "\n",
            "Epoch 21/50\n",
            "Batch 1/72 | Loss: 4.6831 | Batch Acc: 0.2500\n",
            "Batch 2/72 | Loss: 5.0347 | Batch Acc: 0.2500\n",
            "Batch 3/72 | Loss: 5.2336 | Batch Acc: 0.1250\n",
            "Batch 4/72 | Loss: 5.5346 | Batch Acc: 0.1250\n",
            "Batch 5/72 | Loss: 5.6117 | Batch Acc: 0.3125\n",
            "Batch 6/72 | Loss: 5.5025 | Batch Acc: 0.0625\n",
            "Batch 7/72 | Loss: 4.9428 | Batch Acc: 0.3125\n",
            "Batch 8/72 | Loss: 4.9210 | Batch Acc: 0.3750\n",
            "Batch 9/72 | Loss: 5.8692 | Batch Acc: 0.1250\n",
            "Batch 10/72 | Loss: 5.4363 | Batch Acc: 0.2500\n",
            "Batch 11/72 | Loss: 5.5667 | Batch Acc: 0.1250\n",
            "Batch 12/72 | Loss: 6.0704 | Batch Acc: 0.0625\n",
            "Batch 13/72 | Loss: 3.9508 | Batch Acc: 0.3125\n",
            "Batch 14/72 | Loss: 5.3520 | Batch Acc: 0.2500\n",
            "Batch 15/72 | Loss: 5.0681 | Batch Acc: 0.1875\n",
            "Batch 16/72 | Loss: 4.4515 | Batch Acc: 0.1875\n",
            "Batch 17/72 | Loss: 4.4629 | Batch Acc: 0.3750\n",
            "Batch 18/72 | Loss: 7.0470 | Batch Acc: 0.0000\n",
            "Batch 19/72 | Loss: 4.2006 | Batch Acc: 0.3125\n",
            "Batch 20/72 | Loss: 5.0480 | Batch Acc: 0.1250\n",
            "Batch 21/72 | Loss: 6.7898 | Batch Acc: 0.3125\n",
            "Batch 22/72 | Loss: 5.2514 | Batch Acc: 0.3125\n",
            "Batch 23/72 | Loss: 5.2323 | Batch Acc: 0.1875\n",
            "Batch 24/72 | Loss: 5.8496 | Batch Acc: 0.2500\n",
            "Batch 25/72 | Loss: 7.6729 | Batch Acc: 0.1250\n",
            "Batch 26/72 | Loss: 6.3953 | Batch Acc: 0.1875\n",
            "Batch 27/72 | Loss: 4.7488 | Batch Acc: 0.1875\n",
            "Batch 28/72 | Loss: 5.7188 | Batch Acc: 0.2500\n",
            "Batch 29/72 | Loss: 5.1540 | Batch Acc: 0.5000\n",
            "Batch 30/72 | Loss: 4.1567 | Batch Acc: 0.3125\n",
            "Batch 31/72 | Loss: 4.9814 | Batch Acc: 0.2500\n",
            "Batch 32/72 | Loss: 4.3021 | Batch Acc: 0.3750\n",
            "Batch 33/72 | Loss: 5.4904 | Batch Acc: 0.1875\n",
            "Batch 34/72 | Loss: 4.9792 | Batch Acc: 0.3125\n",
            "Batch 35/72 | Loss: 6.0237 | Batch Acc: 0.1250\n",
            "Batch 36/72 | Loss: 6.1967 | Batch Acc: 0.2500\n",
            "Batch 37/72 | Loss: 4.9535 | Batch Acc: 0.2500\n",
            "Batch 38/72 | Loss: 5.7345 | Batch Acc: 0.1875\n",
            "Batch 39/72 | Loss: 4.7162 | Batch Acc: 0.3125\n",
            "Batch 40/72 | Loss: 5.7931 | Batch Acc: 0.0625\n",
            "Batch 41/72 | Loss: 5.0725 | Batch Acc: 0.2500\n",
            "Batch 42/72 | Loss: 4.6486 | Batch Acc: 0.3125\n",
            "Batch 43/72 | Loss: 5.5004 | Batch Acc: 0.0625\n",
            "Batch 44/72 | Loss: 5.7343 | Batch Acc: 0.1250\n",
            "Batch 45/72 | Loss: 4.7886 | Batch Acc: 0.4375\n",
            "Batch 46/72 | Loss: 5.1440 | Batch Acc: 0.0625\n",
            "Batch 47/72 | Loss: 5.4461 | Batch Acc: 0.2500\n",
            "Batch 48/72 | Loss: 5.9384 | Batch Acc: 0.1250\n",
            "Batch 49/72 | Loss: 5.3349 | Batch Acc: 0.2500\n",
            "Batch 50/72 | Loss: 5.0700 | Batch Acc: 0.3750\n",
            "Batch 51/72 | Loss: 5.1817 | Batch Acc: 0.2500\n",
            "Batch 52/72 | Loss: 6.2932 | Batch Acc: 0.1250\n",
            "Batch 53/72 | Loss: 5.7759 | Batch Acc: 0.2500\n",
            "Batch 54/72 | Loss: 6.2059 | Batch Acc: 0.3125\n",
            "Batch 55/72 | Loss: 6.9791 | Batch Acc: 0.1250\n",
            "Batch 56/72 | Loss: 7.0256 | Batch Acc: 0.2500\n",
            "Batch 57/72 | Loss: 5.9094 | Batch Acc: 0.2500\n",
            "Batch 58/72 | Loss: 5.7295 | Batch Acc: 0.3125\n",
            "Batch 59/72 | Loss: 5.6441 | Batch Acc: 0.2500\n",
            "Batch 60/72 | Loss: 6.6558 | Batch Acc: 0.1875\n",
            "Batch 61/72 | Loss: 4.1062 | Batch Acc: 0.4375\n",
            "Batch 62/72 | Loss: 6.7465 | Batch Acc: 0.1875\n",
            "Batch 63/72 | Loss: 5.5800 | Batch Acc: 0.1250\n",
            "Batch 64/72 | Loss: 5.5157 | Batch Acc: 0.0000\n",
            "Batch 65/72 | Loss: 6.1923 | Batch Acc: 0.2500\n",
            "Batch 66/72 | Loss: 5.4170 | Batch Acc: 0.2500\n",
            "Batch 67/72 | Loss: 5.2201 | Batch Acc: 0.2500\n",
            "Batch 68/72 | Loss: 5.8415 | Batch Acc: 0.3125\n",
            "Batch 69/72 | Loss: 4.9186 | Batch Acc: 0.3125\n",
            "Batch 70/72 | Loss: 4.5262 | Batch Acc: 0.3750\n",
            "Batch 71/72 | Loss: 5.4196 | Batch Acc: 0.3125\n",
            "Batch 72/72 | Loss: 4.6517 | Batch Acc: 0.3846\n",
            "Epoch Train Loss: 392.3414, Accuracy: 0.2332\n",
            "Validation Accuracy: 0.3068\n",
            "✅ 儲存最佳模型（Acc: 0.3068）\n",
            "\n",
            "Epoch 22/50\n",
            "Batch 1/72 | Loss: 5.4548 | Batch Acc: 0.2500\n",
            "Batch 2/72 | Loss: 5.0960 | Batch Acc: 0.2500\n",
            "Batch 3/72 | Loss: 4.9605 | Batch Acc: 0.2500\n",
            "Batch 4/72 | Loss: 5.1185 | Batch Acc: 0.3750\n",
            "Batch 5/72 | Loss: 4.7369 | Batch Acc: 0.2500\n",
            "Batch 6/72 | Loss: 4.4576 | Batch Acc: 0.2500\n",
            "Batch 7/72 | Loss: 4.8254 | Batch Acc: 0.2500\n",
            "Batch 8/72 | Loss: 5.2506 | Batch Acc: 0.3125\n",
            "Batch 9/72 | Loss: 5.5653 | Batch Acc: 0.1875\n",
            "Batch 10/72 | Loss: 4.5670 | Batch Acc: 0.3125\n",
            "Batch 11/72 | Loss: 6.7443 | Batch Acc: 0.3125\n",
            "Batch 12/72 | Loss: 4.3429 | Batch Acc: 0.4375\n",
            "Batch 13/72 | Loss: 4.6779 | Batch Acc: 0.3125\n",
            "Batch 14/72 | Loss: 4.4284 | Batch Acc: 0.3125\n",
            "Batch 15/72 | Loss: 5.5493 | Batch Acc: 0.1875\n",
            "Batch 16/72 | Loss: 5.6308 | Batch Acc: 0.2500\n",
            "Batch 17/72 | Loss: 5.6025 | Batch Acc: 0.3125\n",
            "Batch 18/72 | Loss: 4.5271 | Batch Acc: 0.4375\n",
            "Batch 19/72 | Loss: 4.3172 | Batch Acc: 0.3750\n",
            "Batch 20/72 | Loss: 5.6890 | Batch Acc: 0.1250\n",
            "Batch 21/72 | Loss: 6.2057 | Batch Acc: 0.1875\n",
            "Batch 22/72 | Loss: 4.5019 | Batch Acc: 0.3750\n",
            "Batch 23/72 | Loss: 4.9191 | Batch Acc: 0.3125\n",
            "Batch 24/72 | Loss: 5.1603 | Batch Acc: 0.1875\n",
            "Batch 25/72 | Loss: 4.8379 | Batch Acc: 0.1250\n",
            "Batch 26/72 | Loss: 5.9819 | Batch Acc: 0.1250\n",
            "Batch 27/72 | Loss: 4.1526 | Batch Acc: 0.4375\n",
            "Batch 28/72 | Loss: 5.7594 | Batch Acc: 0.3750\n",
            "Batch 29/72 | Loss: 5.4400 | Batch Acc: 0.1875\n",
            "Batch 30/72 | Loss: 6.2247 | Batch Acc: 0.1875\n",
            "Batch 31/72 | Loss: 4.9531 | Batch Acc: 0.2500\n",
            "Batch 32/72 | Loss: 6.3344 | Batch Acc: 0.0000\n",
            "Batch 33/72 | Loss: 5.3027 | Batch Acc: 0.3125\n",
            "Batch 34/72 | Loss: 5.0647 | Batch Acc: 0.3125\n",
            "Batch 35/72 | Loss: 4.8615 | Batch Acc: 0.3750\n",
            "Batch 36/72 | Loss: 4.7733 | Batch Acc: 0.3125\n",
            "Batch 37/72 | Loss: 6.3091 | Batch Acc: 0.1250\n",
            "Batch 38/72 | Loss: 4.8433 | Batch Acc: 0.2500\n",
            "Batch 39/72 | Loss: 5.5390 | Batch Acc: 0.1250\n",
            "Batch 40/72 | Loss: 5.1126 | Batch Acc: 0.3125\n",
            "Batch 41/72 | Loss: 4.9105 | Batch Acc: 0.3125\n",
            "Batch 42/72 | Loss: 4.4499 | Batch Acc: 0.3750\n",
            "Batch 43/72 | Loss: 5.5680 | Batch Acc: 0.1250\n",
            "Batch 44/72 | Loss: 6.2912 | Batch Acc: 0.0625\n",
            "Batch 45/72 | Loss: 5.9215 | Batch Acc: 0.1875\n",
            "Batch 46/72 | Loss: 5.9217 | Batch Acc: 0.0625\n",
            "Batch 47/72 | Loss: 6.1251 | Batch Acc: 0.3125\n",
            "Batch 48/72 | Loss: 4.7322 | Batch Acc: 0.1875\n",
            "Batch 49/72 | Loss: 4.4952 | Batch Acc: 0.2500\n",
            "Batch 50/72 | Loss: 4.3633 | Batch Acc: 0.5000\n",
            "Batch 51/72 | Loss: 5.6641 | Batch Acc: 0.2500\n",
            "Batch 52/72 | Loss: 5.9709 | Batch Acc: 0.1875\n",
            "Batch 53/72 | Loss: 6.2372 | Batch Acc: 0.1250\n",
            "Batch 54/72 | Loss: 5.7921 | Batch Acc: 0.1875\n",
            "Batch 55/72 | Loss: 5.3776 | Batch Acc: 0.3750\n",
            "Batch 56/72 | Loss: 4.9130 | Batch Acc: 0.2500\n",
            "Batch 57/72 | Loss: 5.7173 | Batch Acc: 0.0625\n",
            "Batch 58/72 | Loss: 5.5015 | Batch Acc: 0.3125\n",
            "Batch 59/72 | Loss: 4.7451 | Batch Acc: 0.1875\n",
            "Batch 60/72 | Loss: 6.2796 | Batch Acc: 0.1875\n",
            "Batch 61/72 | Loss: 5.3430 | Batch Acc: 0.0000\n",
            "Batch 62/72 | Loss: 5.1831 | Batch Acc: 0.1250\n",
            "Batch 63/72 | Loss: 6.3323 | Batch Acc: 0.2500\n",
            "Batch 64/72 | Loss: 5.3339 | Batch Acc: 0.2500\n",
            "Batch 65/72 | Loss: 4.8683 | Batch Acc: 0.1875\n",
            "Batch 66/72 | Loss: 5.6773 | Batch Acc: 0.1875\n",
            "Batch 67/72 | Loss: 5.3339 | Batch Acc: 0.1875\n",
            "Batch 68/72 | Loss: 5.5747 | Batch Acc: 0.1875\n",
            "Batch 69/72 | Loss: 6.8937 | Batch Acc: 0.0625\n",
            "Batch 70/72 | Loss: 6.3849 | Batch Acc: 0.1875\n",
            "Batch 71/72 | Loss: 4.7467 | Batch Acc: 0.2500\n",
            "Batch 72/72 | Loss: 4.8997 | Batch Acc: 0.0769\n",
            "Epoch Train Loss: 383.3677, Accuracy: 0.2376\n",
            "Validation Accuracy: 0.2670\n",
            "\n",
            "Epoch 23/50\n",
            "Batch 1/72 | Loss: 5.2369 | Batch Acc: 0.2500\n",
            "Batch 2/72 | Loss: 5.2268 | Batch Acc: 0.3125\n",
            "Batch 3/72 | Loss: 5.7738 | Batch Acc: 0.1875\n",
            "Batch 4/72 | Loss: 5.8880 | Batch Acc: 0.1250\n",
            "Batch 5/72 | Loss: 6.8489 | Batch Acc: 0.1250\n",
            "Batch 6/72 | Loss: 4.6055 | Batch Acc: 0.2500\n",
            "Batch 7/72 | Loss: 6.0221 | Batch Acc: 0.1875\n",
            "Batch 8/72 | Loss: 5.5275 | Batch Acc: 0.2500\n",
            "Batch 9/72 | Loss: 5.1517 | Batch Acc: 0.2500\n",
            "Batch 10/72 | Loss: 4.7748 | Batch Acc: 0.4375\n",
            "Batch 11/72 | Loss: 6.0174 | Batch Acc: 0.1875\n",
            "Batch 12/72 | Loss: 6.4429 | Batch Acc: 0.1875\n",
            "Batch 13/72 | Loss: 5.1529 | Batch Acc: 0.3125\n",
            "Batch 14/72 | Loss: 5.0046 | Batch Acc: 0.2500\n",
            "Batch 15/72 | Loss: 5.1503 | Batch Acc: 0.3750\n",
            "Batch 16/72 | Loss: 4.5324 | Batch Acc: 0.1875\n",
            "Batch 17/72 | Loss: 4.4620 | Batch Acc: 0.2500\n",
            "Batch 18/72 | Loss: 5.2376 | Batch Acc: 0.1875\n",
            "Batch 19/72 | Loss: 4.5599 | Batch Acc: 0.2500\n",
            "Batch 20/72 | Loss: 5.2491 | Batch Acc: 0.2500\n",
            "Batch 21/72 | Loss: 5.3475 | Batch Acc: 0.2500\n",
            "Batch 22/72 | Loss: 6.1819 | Batch Acc: 0.1250\n",
            "Batch 23/72 | Loss: 4.4776 | Batch Acc: 0.3125\n",
            "Batch 24/72 | Loss: 6.0910 | Batch Acc: 0.2500\n",
            "Batch 25/72 | Loss: 6.2254 | Batch Acc: 0.1250\n",
            "Batch 26/72 | Loss: 5.0135 | Batch Acc: 0.2500\n",
            "Batch 27/72 | Loss: 5.3642 | Batch Acc: 0.4375\n",
            "Batch 28/72 | Loss: 5.3746 | Batch Acc: 0.3125\n",
            "Batch 29/72 | Loss: 5.9011 | Batch Acc: 0.1250\n",
            "Batch 30/72 | Loss: 4.6285 | Batch Acc: 0.4375\n",
            "Batch 31/72 | Loss: 5.8380 | Batch Acc: 0.0625\n",
            "Batch 32/72 | Loss: 5.4749 | Batch Acc: 0.3125\n",
            "Batch 33/72 | Loss: 5.1471 | Batch Acc: 0.2500\n",
            "Batch 34/72 | Loss: 4.6083 | Batch Acc: 0.3125\n",
            "Batch 35/72 | Loss: 5.4295 | Batch Acc: 0.1875\n",
            "Batch 36/72 | Loss: 5.0384 | Batch Acc: 0.3750\n",
            "Batch 37/72 | Loss: 6.2253 | Batch Acc: 0.1250\n",
            "Batch 38/72 | Loss: 5.3161 | Batch Acc: 0.1875\n",
            "Batch 39/72 | Loss: 5.2934 | Batch Acc: 0.2500\n",
            "Batch 40/72 | Loss: 3.9913 | Batch Acc: 0.2500\n",
            "Batch 41/72 | Loss: 5.5165 | Batch Acc: 0.2500\n",
            "Batch 42/72 | Loss: 4.7366 | Batch Acc: 0.1875\n",
            "Batch 43/72 | Loss: 4.6570 | Batch Acc: 0.1875\n",
            "Batch 44/72 | Loss: 6.8936 | Batch Acc: 0.1250\n",
            "Batch 45/72 | Loss: 6.1959 | Batch Acc: 0.1250\n",
            "Batch 46/72 | Loss: 5.1713 | Batch Acc: 0.1250\n",
            "Batch 47/72 | Loss: 5.3530 | Batch Acc: 0.2500\n",
            "Batch 48/72 | Loss: 5.2694 | Batch Acc: 0.2500\n",
            "Batch 49/72 | Loss: 5.3152 | Batch Acc: 0.2500\n",
            "Batch 50/72 | Loss: 5.3332 | Batch Acc: 0.2500\n",
            "Batch 51/72 | Loss: 4.7621 | Batch Acc: 0.3750\n",
            "Batch 52/72 | Loss: 4.8503 | Batch Acc: 0.3125\n",
            "Batch 53/72 | Loss: 5.1469 | Batch Acc: 0.1250\n",
            "Batch 54/72 | Loss: 4.3211 | Batch Acc: 0.3750\n",
            "Batch 55/72 | Loss: 6.4812 | Batch Acc: 0.1250\n",
            "Batch 56/72 | Loss: 5.0329 | Batch Acc: 0.3750\n",
            "Batch 57/72 | Loss: 5.4024 | Batch Acc: 0.4375\n",
            "Batch 58/72 | Loss: 6.3610 | Batch Acc: 0.2500\n",
            "Batch 59/72 | Loss: 6.0793 | Batch Acc: 0.2500\n",
            "Batch 60/72 | Loss: 4.1039 | Batch Acc: 0.3750\n",
            "Batch 61/72 | Loss: 6.9265 | Batch Acc: 0.1250\n",
            "Batch 62/72 | Loss: 6.0876 | Batch Acc: 0.0625\n",
            "Batch 63/72 | Loss: 4.0478 | Batch Acc: 0.5000\n",
            "Batch 64/72 | Loss: 6.4321 | Batch Acc: 0.1875\n",
            "Batch 65/72 | Loss: 4.1114 | Batch Acc: 0.2500\n",
            "Batch 66/72 | Loss: 5.2088 | Batch Acc: 0.2500\n",
            "Batch 67/72 | Loss: 3.7323 | Batch Acc: 0.4375\n",
            "Batch 68/72 | Loss: 4.6911 | Batch Acc: 0.2500\n",
            "Batch 69/72 | Loss: 6.1138 | Batch Acc: 0.1250\n",
            "Batch 70/72 | Loss: 5.1138 | Batch Acc: 0.1875\n",
            "Batch 71/72 | Loss: 3.9051 | Batch Acc: 0.4375\n",
            "Batch 72/72 | Loss: 4.5820 | Batch Acc: 0.4615\n",
            "Epoch Train Loss: 381.7674, Accuracy: 0.2498\n",
            "Validation Accuracy: 0.3182\n",
            "✅ 儲存最佳模型（Acc: 0.3182）\n",
            "\n",
            "Epoch 24/50\n",
            "Batch 1/72 | Loss: 5.2069 | Batch Acc: 0.3125\n",
            "Batch 2/72 | Loss: 6.1189 | Batch Acc: 0.2500\n",
            "Batch 3/72 | Loss: 4.7099 | Batch Acc: 0.3750\n",
            "Batch 4/72 | Loss: 4.7821 | Batch Acc: 0.4375\n",
            "Batch 5/72 | Loss: 5.2893 | Batch Acc: 0.3750\n",
            "Batch 6/72 | Loss: 4.7410 | Batch Acc: 0.4375\n",
            "Batch 7/72 | Loss: 5.4955 | Batch Acc: 0.3125\n",
            "Batch 8/72 | Loss: 4.9455 | Batch Acc: 0.5000\n",
            "Batch 9/72 | Loss: 4.6467 | Batch Acc: 0.2500\n",
            "Batch 10/72 | Loss: 4.7876 | Batch Acc: 0.3750\n",
            "Batch 11/72 | Loss: 5.9296 | Batch Acc: 0.3125\n",
            "Batch 12/72 | Loss: 6.9869 | Batch Acc: 0.0625\n",
            "Batch 13/72 | Loss: 5.3665 | Batch Acc: 0.2500\n",
            "Batch 14/72 | Loss: 5.7638 | Batch Acc: 0.1250\n",
            "Batch 15/72 | Loss: 4.3657 | Batch Acc: 0.2500\n",
            "Batch 16/72 | Loss: 5.6767 | Batch Acc: 0.1250\n",
            "Batch 17/72 | Loss: 5.0666 | Batch Acc: 0.1875\n",
            "Batch 18/72 | Loss: 5.5100 | Batch Acc: 0.3125\n",
            "Batch 19/72 | Loss: 5.0891 | Batch Acc: 0.2500\n",
            "Batch 20/72 | Loss: 4.5802 | Batch Acc: 0.5000\n",
            "Batch 21/72 | Loss: 4.7189 | Batch Acc: 0.2500\n",
            "Batch 22/72 | Loss: 5.0654 | Batch Acc: 0.4375\n",
            "Batch 23/72 | Loss: 6.2441 | Batch Acc: 0.1875\n",
            "Batch 24/72 | Loss: 4.6989 | Batch Acc: 0.2500\n",
            "Batch 25/72 | Loss: 6.8982 | Batch Acc: 0.2500\n",
            "Batch 26/72 | Loss: 4.0485 | Batch Acc: 0.4375\n",
            "Batch 27/72 | Loss: 5.7303 | Batch Acc: 0.1250\n",
            "Batch 28/72 | Loss: 5.5357 | Batch Acc: 0.2500\n",
            "Batch 29/72 | Loss: 5.3439 | Batch Acc: 0.3125\n",
            "Batch 30/72 | Loss: 5.2363 | Batch Acc: 0.3750\n",
            "Batch 31/72 | Loss: 4.8726 | Batch Acc: 0.3125\n",
            "Batch 32/72 | Loss: 6.1104 | Batch Acc: 0.1875\n",
            "Batch 33/72 | Loss: 5.9111 | Batch Acc: 0.2500\n",
            "Batch 34/72 | Loss: 5.3363 | Batch Acc: 0.3125\n",
            "Batch 35/72 | Loss: 4.7679 | Batch Acc: 0.3125\n",
            "Batch 36/72 | Loss: 5.8090 | Batch Acc: 0.2500\n",
            "Batch 37/72 | Loss: 5.1113 | Batch Acc: 0.2500\n",
            "Batch 38/72 | Loss: 4.8350 | Batch Acc: 0.4375\n",
            "Batch 39/72 | Loss: 7.1366 | Batch Acc: 0.1250\n",
            "Batch 40/72 | Loss: 5.1898 | Batch Acc: 0.2500\n",
            "Batch 41/72 | Loss: 4.6944 | Batch Acc: 0.3125\n",
            "Batch 42/72 | Loss: 4.6109 | Batch Acc: 0.1875\n",
            "Batch 43/72 | Loss: 6.2724 | Batch Acc: 0.1875\n",
            "Batch 44/72 | Loss: 5.3364 | Batch Acc: 0.2500\n",
            "Batch 45/72 | Loss: 5.0093 | Batch Acc: 0.2500\n",
            "Batch 46/72 | Loss: 5.6517 | Batch Acc: 0.2500\n",
            "Batch 47/72 | Loss: 4.4904 | Batch Acc: 0.3125\n",
            "Batch 48/72 | Loss: 5.1810 | Batch Acc: 0.2500\n",
            "Batch 49/72 | Loss: 5.2927 | Batch Acc: 0.3125\n",
            "Batch 50/72 | Loss: 4.1173 | Batch Acc: 0.5625\n",
            "Batch 51/72 | Loss: 5.2170 | Batch Acc: 0.3125\n",
            "Batch 52/72 | Loss: 5.7002 | Batch Acc: 0.2500\n",
            "Batch 53/72 | Loss: 5.6862 | Batch Acc: 0.3125\n",
            "Batch 54/72 | Loss: 5.5294 | Batch Acc: 0.1250\n",
            "Batch 55/72 | Loss: 5.3518 | Batch Acc: 0.3750\n",
            "Batch 56/72 | Loss: 5.1519 | Batch Acc: 0.0625\n",
            "Batch 57/72 | Loss: 5.4808 | Batch Acc: 0.2500\n",
            "Batch 58/72 | Loss: 5.5654 | Batch Acc: 0.0625\n",
            "Batch 59/72 | Loss: 4.1046 | Batch Acc: 0.5625\n",
            "Batch 60/72 | Loss: 5.9606 | Batch Acc: 0.0625\n",
            "Batch 61/72 | Loss: 5.6613 | Batch Acc: 0.1875\n",
            "Batch 62/72 | Loss: 4.8250 | Batch Acc: 0.1875\n",
            "Batch 63/72 | Loss: 5.2334 | Batch Acc: 0.2500\n",
            "Batch 64/72 | Loss: 6.0237 | Batch Acc: 0.1875\n",
            "Batch 65/72 | Loss: 6.1058 | Batch Acc: 0.1250\n",
            "Batch 66/72 | Loss: 6.3229 | Batch Acc: 0.1875\n",
            "Batch 67/72 | Loss: 5.2234 | Batch Acc: 0.1875\n",
            "Batch 68/72 | Loss: 5.2448 | Batch Acc: 0.3125\n",
            "Batch 69/72 | Loss: 5.2399 | Batch Acc: 0.1250\n",
            "Batch 70/72 | Loss: 6.5386 | Batch Acc: 0.1250\n",
            "Batch 71/72 | Loss: 4.6267 | Batch Acc: 0.2500\n",
            "Batch 72/72 | Loss: 5.9032 | Batch Acc: 0.3846\n",
            "Epoch Train Loss: 385.0116, Accuracy: 0.2681\n",
            "Validation Accuracy: 0.2841\n",
            "\n",
            "Epoch 25/50\n",
            "Batch 1/72 | Loss: 5.3845 | Batch Acc: 0.3125\n",
            "Batch 2/72 | Loss: 5.5800 | Batch Acc: 0.1875\n",
            "Batch 3/72 | Loss: 3.2658 | Batch Acc: 0.6250\n",
            "Batch 4/72 | Loss: 6.5774 | Batch Acc: 0.1250\n",
            "Batch 5/72 | Loss: 6.9430 | Batch Acc: 0.1250\n",
            "Batch 6/72 | Loss: 5.5213 | Batch Acc: 0.3125\n",
            "Batch 7/72 | Loss: 5.9331 | Batch Acc: 0.2500\n",
            "Batch 8/72 | Loss: 4.3811 | Batch Acc: 0.4375\n",
            "Batch 9/72 | Loss: 5.0728 | Batch Acc: 0.3125\n",
            "Batch 10/72 | Loss: 5.7581 | Batch Acc: 0.2500\n",
            "Batch 11/72 | Loss: 5.6316 | Batch Acc: 0.2500\n",
            "Batch 12/72 | Loss: 5.2307 | Batch Acc: 0.3750\n",
            "Batch 13/72 | Loss: 5.5772 | Batch Acc: 0.1875\n",
            "Batch 14/72 | Loss: 5.6240 | Batch Acc: 0.2500\n",
            "Batch 15/72 | Loss: 5.8118 | Batch Acc: 0.1875\n",
            "Batch 16/72 | Loss: 5.5260 | Batch Acc: 0.1875\n",
            "Batch 17/72 | Loss: 5.8499 | Batch Acc: 0.2500\n",
            "Batch 18/72 | Loss: 4.6165 | Batch Acc: 0.3750\n",
            "Batch 19/72 | Loss: 5.3488 | Batch Acc: 0.3125\n",
            "Batch 20/72 | Loss: 4.9249 | Batch Acc: 0.2500\n",
            "Batch 21/72 | Loss: 5.7788 | Batch Acc: 0.1250\n",
            "Batch 22/72 | Loss: 5.6769 | Batch Acc: 0.2500\n",
            "Batch 23/72 | Loss: 4.3811 | Batch Acc: 0.2500\n",
            "Batch 24/72 | Loss: 5.2482 | Batch Acc: 0.2500\n",
            "Batch 25/72 | Loss: 4.1980 | Batch Acc: 0.1875\n",
            "Batch 26/72 | Loss: 5.3496 | Batch Acc: 0.1875\n",
            "Batch 27/72 | Loss: 5.0194 | Batch Acc: 0.3125\n",
            "Batch 28/72 | Loss: 5.3333 | Batch Acc: 0.3125\n",
            "Batch 29/72 | Loss: 5.9819 | Batch Acc: 0.1250\n",
            "Batch 30/72 | Loss: 6.1272 | Batch Acc: 0.1250\n",
            "Batch 31/72 | Loss: 6.0726 | Batch Acc: 0.2500\n",
            "Batch 32/72 | Loss: 4.9920 | Batch Acc: 0.3125\n",
            "Batch 33/72 | Loss: 5.5521 | Batch Acc: 0.1875\n",
            "Batch 34/72 | Loss: 4.5426 | Batch Acc: 0.3125\n",
            "Batch 35/72 | Loss: 5.4002 | Batch Acc: 0.2500\n",
            "Batch 36/72 | Loss: 5.6074 | Batch Acc: 0.2500\n",
            "Batch 37/72 | Loss: 5.4611 | Batch Acc: 0.2500\n",
            "Batch 38/72 | Loss: 4.3488 | Batch Acc: 0.3750\n",
            "Batch 39/72 | Loss: 5.7080 | Batch Acc: 0.1875\n",
            "Batch 40/72 | Loss: 5.5635 | Batch Acc: 0.3125\n",
            "Batch 41/72 | Loss: 4.3304 | Batch Acc: 0.3750\n",
            "Batch 42/72 | Loss: 4.8463 | Batch Acc: 0.3125\n",
            "Batch 43/72 | Loss: 5.0003 | Batch Acc: 0.1250\n",
            "Batch 44/72 | Loss: 5.6398 | Batch Acc: 0.3125\n",
            "Batch 45/72 | Loss: 4.4399 | Batch Acc: 0.3750\n",
            "Batch 46/72 | Loss: 4.5517 | Batch Acc: 0.4375\n",
            "Batch 47/72 | Loss: 4.7545 | Batch Acc: 0.3750\n",
            "Batch 48/72 | Loss: 5.1897 | Batch Acc: 0.1250\n",
            "Batch 49/72 | Loss: 5.3819 | Batch Acc: 0.1875\n",
            "Batch 50/72 | Loss: 3.9417 | Batch Acc: 0.6250\n",
            "Batch 51/72 | Loss: 5.5608 | Batch Acc: 0.1875\n",
            "Batch 52/72 | Loss: 4.9649 | Batch Acc: 0.2500\n",
            "Batch 53/72 | Loss: 5.3047 | Batch Acc: 0.1875\n",
            "Batch 54/72 | Loss: 4.6583 | Batch Acc: 0.1875\n",
            "Batch 55/72 | Loss: 4.4412 | Batch Acc: 0.3125\n",
            "Batch 56/72 | Loss: 4.4929 | Batch Acc: 0.4375\n",
            "Batch 57/72 | Loss: 6.4807 | Batch Acc: 0.1875\n",
            "Batch 58/72 | Loss: 5.6548 | Batch Acc: 0.0625\n",
            "Batch 59/72 | Loss: 3.9669 | Batch Acc: 0.3750\n",
            "Batch 60/72 | Loss: 5.9088 | Batch Acc: 0.3750\n",
            "Batch 61/72 | Loss: 6.5150 | Batch Acc: 0.1875\n",
            "Batch 62/72 | Loss: 5.9924 | Batch Acc: 0.0625\n",
            "Batch 63/72 | Loss: 4.9574 | Batch Acc: 0.3125\n",
            "Batch 64/72 | Loss: 4.5329 | Batch Acc: 0.3750\n",
            "Batch 65/72 | Loss: 5.8234 | Batch Acc: 0.1875\n",
            "Batch 66/72 | Loss: 5.6194 | Batch Acc: 0.3125\n",
            "Batch 67/72 | Loss: 5.8230 | Batch Acc: 0.3125\n",
            "Batch 68/72 | Loss: 4.6208 | Batch Acc: 0.4375\n",
            "Batch 69/72 | Loss: 4.9937 | Batch Acc: 0.4375\n",
            "Batch 70/72 | Loss: 5.2631 | Batch Acc: 0.1875\n",
            "Batch 71/72 | Loss: 5.8467 | Batch Acc: 0.0625\n",
            "Batch 72/72 | Loss: 5.6941 | Batch Acc: 0.3077\n",
            "Epoch Train Loss: 380.0933, Accuracy: 0.2689\n",
            "Validation Accuracy: 0.3239\n",
            "✅ 儲存最佳模型（Acc: 0.3239）\n",
            "\n",
            "Epoch 26/50\n",
            "Batch 1/72 | Loss: 4.3250 | Batch Acc: 0.5000\n",
            "Batch 2/72 | Loss: 5.0151 | Batch Acc: 0.3750\n",
            "Batch 3/72 | Loss: 5.9073 | Batch Acc: 0.1250\n",
            "Batch 4/72 | Loss: 5.3872 | Batch Acc: 0.2500\n",
            "Batch 5/72 | Loss: 5.7758 | Batch Acc: 0.1250\n",
            "Batch 6/72 | Loss: 5.7382 | Batch Acc: 0.1250\n",
            "Batch 7/72 | Loss: 7.1233 | Batch Acc: 0.0000\n",
            "Batch 8/72 | Loss: 5.1681 | Batch Acc: 0.2500\n",
            "Batch 9/72 | Loss: 5.0314 | Batch Acc: 0.3125\n",
            "Batch 10/72 | Loss: 5.0628 | Batch Acc: 0.3750\n",
            "Batch 11/72 | Loss: 5.0316 | Batch Acc: 0.3125\n",
            "Batch 12/72 | Loss: 4.8932 | Batch Acc: 0.3750\n",
            "Batch 13/72 | Loss: 6.6802 | Batch Acc: 0.1875\n",
            "Batch 14/72 | Loss: 4.0442 | Batch Acc: 0.3125\n",
            "Batch 15/72 | Loss: 5.4102 | Batch Acc: 0.3125\n",
            "Batch 16/72 | Loss: 4.9234 | Batch Acc: 0.3125\n",
            "Batch 17/72 | Loss: 5.9333 | Batch Acc: 0.0625\n",
            "Batch 18/72 | Loss: 6.1992 | Batch Acc: 0.3750\n",
            "Batch 19/72 | Loss: 5.3125 | Batch Acc: 0.1875\n",
            "Batch 20/72 | Loss: 4.8800 | Batch Acc: 0.3750\n",
            "Batch 21/72 | Loss: 6.0286 | Batch Acc: 0.0625\n",
            "Batch 22/72 | Loss: 5.7033 | Batch Acc: 0.1250\n",
            "Batch 23/72 | Loss: 4.3791 | Batch Acc: 0.4375\n",
            "Batch 24/72 | Loss: 4.8569 | Batch Acc: 0.3750\n",
            "Batch 25/72 | Loss: 5.2682 | Batch Acc: 0.2500\n",
            "Batch 26/72 | Loss: 5.9889 | Batch Acc: 0.1250\n",
            "Batch 27/72 | Loss: 5.8223 | Batch Acc: 0.1875\n",
            "Batch 28/72 | Loss: 6.7164 | Batch Acc: 0.1875\n",
            "Batch 29/72 | Loss: 4.9948 | Batch Acc: 0.3125\n",
            "Batch 30/72 | Loss: 5.2982 | Batch Acc: 0.3750\n",
            "Batch 31/72 | Loss: 5.3922 | Batch Acc: 0.2500\n",
            "Batch 32/72 | Loss: 4.4054 | Batch Acc: 0.3750\n",
            "Batch 33/72 | Loss: 5.0755 | Batch Acc: 0.2500\n",
            "Batch 34/72 | Loss: 4.8102 | Batch Acc: 0.3125\n",
            "Batch 35/72 | Loss: 5.3664 | Batch Acc: 0.2500\n",
            "Batch 36/72 | Loss: 5.4256 | Batch Acc: 0.0625\n",
            "Batch 37/72 | Loss: 5.1986 | Batch Acc: 0.2500\n",
            "Batch 38/72 | Loss: 4.5773 | Batch Acc: 0.1875\n",
            "Batch 39/72 | Loss: 4.8383 | Batch Acc: 0.1875\n",
            "Batch 40/72 | Loss: 4.1744 | Batch Acc: 0.2500\n",
            "Batch 41/72 | Loss: 5.8304 | Batch Acc: 0.1875\n",
            "Batch 42/72 | Loss: 5.8773 | Batch Acc: 0.2500\n",
            "Batch 43/72 | Loss: 5.3279 | Batch Acc: 0.1250\n",
            "Batch 44/72 | Loss: 3.9788 | Batch Acc: 0.3125\n",
            "Batch 45/72 | Loss: 5.2885 | Batch Acc: 0.3125\n",
            "Batch 46/72 | Loss: 4.7724 | Batch Acc: 0.1875\n",
            "Batch 47/72 | Loss: 6.1721 | Batch Acc: 0.1250\n",
            "Batch 48/72 | Loss: 6.5666 | Batch Acc: 0.3125\n",
            "Batch 49/72 | Loss: 5.1974 | Batch Acc: 0.1250\n",
            "Batch 50/72 | Loss: 6.2950 | Batch Acc: 0.1875\n",
            "Batch 51/72 | Loss: 5.4239 | Batch Acc: 0.1250\n",
            "Batch 52/72 | Loss: 5.0380 | Batch Acc: 0.2500\n",
            "Batch 53/72 | Loss: 4.8445 | Batch Acc: 0.2500\n",
            "Batch 54/72 | Loss: 5.5061 | Batch Acc: 0.3125\n",
            "Batch 55/72 | Loss: 4.8958 | Batch Acc: 0.4375\n",
            "Batch 56/72 | Loss: 5.0438 | Batch Acc: 0.4375\n",
            "Batch 57/72 | Loss: 6.1740 | Batch Acc: 0.2500\n",
            "Batch 58/72 | Loss: 4.4057 | Batch Acc: 0.6250\n",
            "Batch 59/72 | Loss: 5.9133 | Batch Acc: 0.2500\n",
            "Batch 60/72 | Loss: 5.0112 | Batch Acc: 0.3125\n",
            "Batch 61/72 | Loss: 4.2201 | Batch Acc: 0.3750\n",
            "Batch 62/72 | Loss: 4.9851 | Batch Acc: 0.4375\n",
            "Batch 63/72 | Loss: 4.5435 | Batch Acc: 0.4375\n",
            "Batch 64/72 | Loss: 5.8938 | Batch Acc: 0.2500\n",
            "Batch 65/72 | Loss: 6.2849 | Batch Acc: 0.2500\n",
            "Batch 66/72 | Loss: 5.9938 | Batch Acc: 0.1250\n",
            "Batch 67/72 | Loss: 5.2980 | Batch Acc: 0.2500\n",
            "Batch 68/72 | Loss: 5.1429 | Batch Acc: 0.0625\n",
            "Batch 69/72 | Loss: 4.8321 | Batch Acc: 0.3125\n",
            "Batch 70/72 | Loss: 5.5148 | Batch Acc: 0.1250\n",
            "Batch 71/72 | Loss: 5.2904 | Batch Acc: 0.3125\n",
            "Batch 72/72 | Loss: 4.8010 | Batch Acc: 0.3077\n",
            "Epoch Train Loss: 382.5255, Accuracy: 0.2594\n",
            "Validation Accuracy: 0.3068\n",
            "\n",
            "Epoch 27/50\n",
            "Batch 1/72 | Loss: 4.9272 | Batch Acc: 0.3750\n",
            "Batch 2/72 | Loss: 4.6096 | Batch Acc: 0.2500\n",
            "Batch 3/72 | Loss: 4.4985 | Batch Acc: 0.6250\n",
            "Batch 4/72 | Loss: 4.8310 | Batch Acc: 0.3125\n",
            "Batch 5/72 | Loss: 5.9023 | Batch Acc: 0.2500\n",
            "Batch 6/72 | Loss: 4.7589 | Batch Acc: 0.3125\n",
            "Batch 7/72 | Loss: 5.6457 | Batch Acc: 0.2500\n",
            "Batch 8/72 | Loss: 3.9556 | Batch Acc: 0.3125\n",
            "Batch 9/72 | Loss: 4.7796 | Batch Acc: 0.2500\n",
            "Batch 10/72 | Loss: 5.5542 | Batch Acc: 0.1875\n",
            "Batch 11/72 | Loss: 4.6334 | Batch Acc: 0.1250\n",
            "Batch 12/72 | Loss: 4.3297 | Batch Acc: 0.3750\n",
            "Batch 13/72 | Loss: 5.2396 | Batch Acc: 0.3125\n",
            "Batch 14/72 | Loss: 4.0445 | Batch Acc: 0.1875\n",
            "Batch 15/72 | Loss: 4.8065 | Batch Acc: 0.1875\n",
            "Batch 16/72 | Loss: 4.6008 | Batch Acc: 0.1875\n",
            "Batch 17/72 | Loss: 5.6431 | Batch Acc: 0.1875\n",
            "Batch 18/72 | Loss: 5.4044 | Batch Acc: 0.1875\n",
            "Batch 19/72 | Loss: 4.6868 | Batch Acc: 0.3750\n",
            "Batch 20/72 | Loss: 4.8180 | Batch Acc: 0.2500\n",
            "Batch 21/72 | Loss: 4.1961 | Batch Acc: 0.4375\n",
            "Batch 22/72 | Loss: 5.2915 | Batch Acc: 0.3125\n",
            "Batch 23/72 | Loss: 4.4229 | Batch Acc: 0.3750\n",
            "Batch 24/72 | Loss: 5.7701 | Batch Acc: 0.0625\n",
            "Batch 25/72 | Loss: 6.6154 | Batch Acc: 0.1250\n",
            "Batch 26/72 | Loss: 5.5126 | Batch Acc: 0.4375\n",
            "Batch 27/72 | Loss: 4.6634 | Batch Acc: 0.1875\n",
            "Batch 28/72 | Loss: 4.9311 | Batch Acc: 0.1250\n",
            "Batch 29/72 | Loss: 4.4336 | Batch Acc: 0.4375\n",
            "Batch 30/72 | Loss: 5.4671 | Batch Acc: 0.3125\n",
            "Batch 31/72 | Loss: 5.5289 | Batch Acc: 0.1250\n",
            "Batch 32/72 | Loss: 4.9157 | Batch Acc: 0.3125\n",
            "Batch 33/72 | Loss: 5.4117 | Batch Acc: 0.3125\n",
            "Batch 34/72 | Loss: 5.3930 | Batch Acc: 0.2500\n",
            "Batch 35/72 | Loss: 5.0847 | Batch Acc: 0.2500\n",
            "Batch 36/72 | Loss: 5.3911 | Batch Acc: 0.3125\n",
            "Batch 37/72 | Loss: 5.1842 | Batch Acc: 0.3750\n",
            "Batch 38/72 | Loss: 5.9152 | Batch Acc: 0.2500\n",
            "Batch 39/72 | Loss: 6.1856 | Batch Acc: 0.0000\n",
            "Batch 40/72 | Loss: 5.3098 | Batch Acc: 0.1875\n",
            "Batch 41/72 | Loss: 5.3932 | Batch Acc: 0.3125\n",
            "Batch 42/72 | Loss: 6.8902 | Batch Acc: 0.2500\n",
            "Batch 43/72 | Loss: 4.0339 | Batch Acc: 0.3125\n",
            "Batch 44/72 | Loss: 5.8800 | Batch Acc: 0.3125\n",
            "Batch 45/72 | Loss: 5.7597 | Batch Acc: 0.0625\n",
            "Batch 46/72 | Loss: 6.1920 | Batch Acc: 0.1250\n",
            "Batch 47/72 | Loss: 7.0764 | Batch Acc: 0.2500\n",
            "Batch 48/72 | Loss: 5.7295 | Batch Acc: 0.2500\n",
            "Batch 49/72 | Loss: 5.1074 | Batch Acc: 0.1250\n",
            "Batch 50/72 | Loss: 4.2941 | Batch Acc: 0.2500\n",
            "Batch 51/72 | Loss: 4.0224 | Batch Acc: 0.5000\n",
            "Batch 52/72 | Loss: 5.3419 | Batch Acc: 0.2500\n",
            "Batch 53/72 | Loss: 4.6793 | Batch Acc: 0.4375\n",
            "Batch 54/72 | Loss: 4.1311 | Batch Acc: 0.2500\n",
            "Batch 55/72 | Loss: 4.3400 | Batch Acc: 0.4375\n",
            "Batch 56/72 | Loss: 5.3275 | Batch Acc: 0.1250\n",
            "Batch 57/72 | Loss: 5.3501 | Batch Acc: 0.3750\n",
            "Batch 58/72 | Loss: 5.9276 | Batch Acc: 0.1875\n",
            "Batch 59/72 | Loss: 5.8622 | Batch Acc: 0.1250\n",
            "Batch 60/72 | Loss: 5.3275 | Batch Acc: 0.1250\n",
            "Batch 61/72 | Loss: 5.5013 | Batch Acc: 0.1875\n",
            "Batch 62/72 | Loss: 4.9410 | Batch Acc: 0.4375\n",
            "Batch 63/72 | Loss: 3.8153 | Batch Acc: 0.5000\n",
            "Batch 64/72 | Loss: 3.8402 | Batch Acc: 0.5000\n",
            "Batch 65/72 | Loss: 6.0964 | Batch Acc: 0.1250\n",
            "Batch 66/72 | Loss: 5.6769 | Batch Acc: 0.3125\n",
            "Batch 67/72 | Loss: 6.5253 | Batch Acc: 0.1250\n",
            "Batch 68/72 | Loss: 5.2346 | Batch Acc: 0.3750\n",
            "Batch 69/72 | Loss: 5.5948 | Batch Acc: 0.1875\n",
            "Batch 70/72 | Loss: 6.2049 | Batch Acc: 0.3125\n",
            "Batch 71/72 | Loss: 5.8498 | Batch Acc: 0.0625\n",
            "Batch 72/72 | Loss: 7.9284 | Batch Acc: 0.1538\n",
            "Epoch Train Loss: 377.1679, Accuracy: 0.2646\n",
            "Validation Accuracy: 0.3580\n",
            "✅ 儲存最佳模型（Acc: 0.3580）\n",
            "\n",
            "Epoch 28/50\n",
            "Batch 1/72 | Loss: 5.2999 | Batch Acc: 0.3750\n",
            "Batch 2/72 | Loss: 5.4257 | Batch Acc: 0.1250\n",
            "Batch 3/72 | Loss: 5.0319 | Batch Acc: 0.2500\n",
            "Batch 4/72 | Loss: 4.3432 | Batch Acc: 0.5625\n",
            "Batch 5/72 | Loss: 6.1482 | Batch Acc: 0.1875\n",
            "Batch 6/72 | Loss: 4.7154 | Batch Acc: 0.4375\n",
            "Batch 7/72 | Loss: 4.7781 | Batch Acc: 0.3750\n",
            "Batch 8/72 | Loss: 4.9594 | Batch Acc: 0.1875\n",
            "Batch 9/72 | Loss: 4.6448 | Batch Acc: 0.2500\n",
            "Batch 10/72 | Loss: 5.3732 | Batch Acc: 0.1875\n",
            "Batch 11/72 | Loss: 3.7164 | Batch Acc: 0.6875\n",
            "Batch 12/72 | Loss: 6.8149 | Batch Acc: 0.1875\n",
            "Batch 13/72 | Loss: 6.2727 | Batch Acc: 0.1875\n",
            "Batch 14/72 | Loss: 6.1517 | Batch Acc: 0.0000\n",
            "Batch 15/72 | Loss: 5.4524 | Batch Acc: 0.3125\n",
            "Batch 16/72 | Loss: 4.4300 | Batch Acc: 0.5000\n",
            "Batch 17/72 | Loss: 6.0340 | Batch Acc: 0.1875\n",
            "Batch 18/72 | Loss: 4.8460 | Batch Acc: 0.4375\n",
            "Batch 19/72 | Loss: 4.0983 | Batch Acc: 0.3750\n",
            "Batch 20/72 | Loss: 4.1143 | Batch Acc: 0.3125\n",
            "Batch 21/72 | Loss: 4.3243 | Batch Acc: 0.2500\n",
            "Batch 22/72 | Loss: 4.1834 | Batch Acc: 0.3750\n",
            "Batch 23/72 | Loss: 6.1726 | Batch Acc: 0.2500\n",
            "Batch 24/72 | Loss: 4.8951 | Batch Acc: 0.2500\n",
            "Batch 25/72 | Loss: 5.9958 | Batch Acc: 0.1250\n",
            "Batch 26/72 | Loss: 5.7379 | Batch Acc: 0.3125\n",
            "Batch 27/72 | Loss: 4.2110 | Batch Acc: 0.3125\n",
            "Batch 28/72 | Loss: 4.8915 | Batch Acc: 0.2500\n",
            "Batch 29/72 | Loss: 4.1780 | Batch Acc: 0.3125\n",
            "Batch 30/72 | Loss: 6.1149 | Batch Acc: 0.3125\n",
            "Batch 31/72 | Loss: 3.9598 | Batch Acc: 0.2500\n",
            "Batch 32/72 | Loss: 5.2706 | Batch Acc: 0.4375\n",
            "Batch 33/72 | Loss: 5.3348 | Batch Acc: 0.3750\n",
            "Batch 34/72 | Loss: 4.9732 | Batch Acc: 0.3750\n",
            "Batch 35/72 | Loss: 5.6992 | Batch Acc: 0.4375\n",
            "Batch 36/72 | Loss: 5.8094 | Batch Acc: 0.3125\n",
            "Batch 37/72 | Loss: 5.6298 | Batch Acc: 0.1875\n",
            "Batch 38/72 | Loss: 5.5690 | Batch Acc: 0.3125\n",
            "Batch 39/72 | Loss: 6.2143 | Batch Acc: 0.1250\n",
            "Batch 40/72 | Loss: 4.8385 | Batch Acc: 0.2500\n",
            "Batch 41/72 | Loss: 4.7278 | Batch Acc: 0.3750\n",
            "Batch 42/72 | Loss: 4.5498 | Batch Acc: 0.3125\n",
            "Batch 43/72 | Loss: 5.2737 | Batch Acc: 0.3125\n",
            "Batch 44/72 | Loss: 3.9801 | Batch Acc: 0.3125\n",
            "Batch 45/72 | Loss: 5.0684 | Batch Acc: 0.3125\n",
            "Batch 46/72 | Loss: 6.9601 | Batch Acc: 0.1250\n",
            "Batch 47/72 | Loss: 3.9312 | Batch Acc: 0.4375\n",
            "Batch 48/72 | Loss: 4.9474 | Batch Acc: 0.2500\n",
            "Batch 49/72 | Loss: 5.4923 | Batch Acc: 0.1875\n",
            "Batch 50/72 | Loss: 5.2412 | Batch Acc: 0.1875\n",
            "Batch 51/72 | Loss: 5.2954 | Batch Acc: 0.1875\n",
            "Batch 52/72 | Loss: 5.3773 | Batch Acc: 0.3125\n",
            "Batch 53/72 | Loss: 5.9366 | Batch Acc: 0.1875\n",
            "Batch 54/72 | Loss: 5.3031 | Batch Acc: 0.2500\n",
            "Batch 55/72 | Loss: 5.5330 | Batch Acc: 0.3750\n",
            "Batch 56/72 | Loss: 4.4631 | Batch Acc: 0.2500\n",
            "Batch 57/72 | Loss: 6.2970 | Batch Acc: 0.1875\n",
            "Batch 58/72 | Loss: 4.9867 | Batch Acc: 0.3750\n",
            "Batch 59/72 | Loss: 5.3077 | Batch Acc: 0.1875\n",
            "Batch 60/72 | Loss: 5.5434 | Batch Acc: 0.2500\n",
            "Batch 61/72 | Loss: 4.9996 | Batch Acc: 0.3125\n",
            "Batch 62/72 | Loss: 4.9302 | Batch Acc: 0.1875\n",
            "Batch 63/72 | Loss: 4.9156 | Batch Acc: 0.2500\n",
            "Batch 64/72 | Loss: 5.5005 | Batch Acc: 0.2500\n",
            "Batch 65/72 | Loss: 4.5307 | Batch Acc: 0.2500\n",
            "Batch 66/72 | Loss: 5.4397 | Batch Acc: 0.1875\n",
            "Batch 67/72 | Loss: 5.4780 | Batch Acc: 0.4375\n",
            "Batch 68/72 | Loss: 4.0395 | Batch Acc: 0.2500\n",
            "Batch 69/72 | Loss: 5.5852 | Batch Acc: 0.2500\n",
            "Batch 70/72 | Loss: 5.1806 | Batch Acc: 0.1875\n",
            "Batch 71/72 | Loss: 5.5673 | Batch Acc: 0.3125\n",
            "Batch 72/72 | Loss: 6.3807 | Batch Acc: 0.3077\n",
            "Epoch Train Loss: 373.4166, Accuracy: 0.2846\n",
            "Validation Accuracy: 0.3295\n",
            "\n",
            "Epoch 29/50\n",
            "Batch 1/72 | Loss: 5.1029 | Batch Acc: 0.3125\n",
            "Batch 2/72 | Loss: 5.3005 | Batch Acc: 0.4375\n",
            "Batch 3/72 | Loss: 4.6756 | Batch Acc: 0.5000\n",
            "Batch 4/72 | Loss: 4.8337 | Batch Acc: 0.1875\n",
            "Batch 5/72 | Loss: 4.6406 | Batch Acc: 0.1875\n",
            "Batch 6/72 | Loss: 5.2412 | Batch Acc: 0.1250\n",
            "Batch 7/72 | Loss: 4.7264 | Batch Acc: 0.3125\n",
            "Batch 8/72 | Loss: 5.0822 | Batch Acc: 0.3125\n",
            "Batch 9/72 | Loss: 3.8612 | Batch Acc: 0.3750\n",
            "Batch 10/72 | Loss: 5.5565 | Batch Acc: 0.4375\n",
            "Batch 11/72 | Loss: 3.8247 | Batch Acc: 0.4375\n",
            "Batch 12/72 | Loss: 4.5624 | Batch Acc: 0.3125\n",
            "Batch 13/72 | Loss: 6.1503 | Batch Acc: 0.0625\n",
            "Batch 14/72 | Loss: 5.4178 | Batch Acc: 0.2500\n",
            "Batch 15/72 | Loss: 4.9309 | Batch Acc: 0.2500\n",
            "Batch 16/72 | Loss: 7.8695 | Batch Acc: 0.1875\n",
            "Batch 17/72 | Loss: 5.6551 | Batch Acc: 0.2500\n",
            "Batch 18/72 | Loss: 5.2759 | Batch Acc: 0.4375\n",
            "Batch 19/72 | Loss: 5.6606 | Batch Acc: 0.1875\n",
            "Batch 20/72 | Loss: 5.4222 | Batch Acc: 0.3750\n",
            "Batch 21/72 | Loss: 5.0063 | Batch Acc: 0.3750\n",
            "Batch 22/72 | Loss: 4.4424 | Batch Acc: 0.4375\n",
            "Batch 23/72 | Loss: 5.3808 | Batch Acc: 0.1875\n",
            "Batch 24/72 | Loss: 6.4835 | Batch Acc: 0.1875\n",
            "Batch 25/72 | Loss: 4.7482 | Batch Acc: 0.3750\n",
            "Batch 26/72 | Loss: 4.5175 | Batch Acc: 0.2500\n",
            "Batch 27/72 | Loss: 4.1346 | Batch Acc: 0.4375\n",
            "Batch 28/72 | Loss: 4.3595 | Batch Acc: 0.3750\n",
            "Batch 29/72 | Loss: 5.5064 | Batch Acc: 0.1250\n",
            "Batch 30/72 | Loss: 4.4958 | Batch Acc: 0.3750\n",
            "Batch 31/72 | Loss: 4.3686 | Batch Acc: 0.2500\n",
            "Batch 32/72 | Loss: 5.9916 | Batch Acc: 0.1250\n",
            "Batch 33/72 | Loss: 4.5712 | Batch Acc: 0.1250\n",
            "Batch 34/72 | Loss: 5.9288 | Batch Acc: 0.3125\n",
            "Batch 35/72 | Loss: 5.9948 | Batch Acc: 0.2500\n",
            "Batch 36/72 | Loss: 4.4168 | Batch Acc: 0.1875\n",
            "Batch 37/72 | Loss: 4.0181 | Batch Acc: 0.3750\n",
            "Batch 38/72 | Loss: 5.6440 | Batch Acc: 0.3125\n",
            "Batch 39/72 | Loss: 5.6951 | Batch Acc: 0.2500\n",
            "Batch 40/72 | Loss: 6.2421 | Batch Acc: 0.1875\n",
            "Batch 41/72 | Loss: 5.1648 | Batch Acc: 0.1250\n",
            "Batch 42/72 | Loss: 5.0731 | Batch Acc: 0.2500\n",
            "Batch 43/72 | Loss: 5.4162 | Batch Acc: 0.1250\n",
            "Batch 44/72 | Loss: 5.5382 | Batch Acc: 0.1875\n",
            "Batch 45/72 | Loss: 5.5064 | Batch Acc: 0.2500\n",
            "Batch 46/72 | Loss: 4.4616 | Batch Acc: 0.1875\n",
            "Batch 47/72 | Loss: 4.9345 | Batch Acc: 0.5000\n",
            "Batch 48/72 | Loss: 5.2863 | Batch Acc: 0.2500\n",
            "Batch 49/72 | Loss: 5.0561 | Batch Acc: 0.1250\n",
            "Batch 50/72 | Loss: 6.0235 | Batch Acc: 0.1875\n",
            "Batch 51/72 | Loss: 5.8411 | Batch Acc: 0.3750\n",
            "Batch 52/72 | Loss: 4.7962 | Batch Acc: 0.4375\n",
            "Batch 53/72 | Loss: 5.0021 | Batch Acc: 0.1875\n",
            "Batch 54/72 | Loss: 6.5665 | Batch Acc: 0.0625\n",
            "Batch 55/72 | Loss: 4.2659 | Batch Acc: 0.1875\n",
            "Batch 56/72 | Loss: 4.3318 | Batch Acc: 0.1875\n",
            "Batch 57/72 | Loss: 5.9732 | Batch Acc: 0.2500\n",
            "Batch 58/72 | Loss: 4.8287 | Batch Acc: 0.3125\n",
            "Batch 59/72 | Loss: 4.0757 | Batch Acc: 0.3750\n",
            "Batch 60/72 | Loss: 6.8478 | Batch Acc: 0.0000\n",
            "Batch 61/72 | Loss: 4.7759 | Batch Acc: 0.3750\n",
            "Batch 62/72 | Loss: 5.4383 | Batch Acc: 0.2500\n",
            "Batch 63/72 | Loss: 5.5659 | Batch Acc: 0.2500\n",
            "Batch 64/72 | Loss: 5.5789 | Batch Acc: 0.1875\n",
            "Batch 65/72 | Loss: 5.4572 | Batch Acc: 0.3125\n",
            "Batch 66/72 | Loss: 5.9668 | Batch Acc: 0.1875\n",
            "Batch 67/72 | Loss: 4.4872 | Batch Acc: 0.3750\n",
            "Batch 68/72 | Loss: 6.3777 | Batch Acc: 0.1250\n",
            "Batch 69/72 | Loss: 4.7889 | Batch Acc: 0.2500\n",
            "Batch 70/72 | Loss: 5.7105 | Batch Acc: 0.3125\n",
            "Batch 71/72 | Loss: 6.3550 | Batch Acc: 0.3125\n",
            "Batch 72/72 | Loss: 3.6024 | Batch Acc: 0.4615\n",
            "Epoch Train Loss: 374.8308, Accuracy: 0.2689\n",
            "Validation Accuracy: 0.3523\n",
            "\n",
            "Epoch 30/50\n",
            "Batch 1/72 | Loss: 5.1696 | Batch Acc: 0.3125\n",
            "Batch 2/72 | Loss: 3.7034 | Batch Acc: 0.3125\n",
            "Batch 3/72 | Loss: 4.5011 | Batch Acc: 0.3750\n",
            "Batch 4/72 | Loss: 5.5121 | Batch Acc: 0.1875\n",
            "Batch 5/72 | Loss: 5.2503 | Batch Acc: 0.1875\n",
            "Batch 6/72 | Loss: 5.6257 | Batch Acc: 0.0625\n",
            "Batch 7/72 | Loss: 4.6388 | Batch Acc: 0.2500\n",
            "Batch 8/72 | Loss: 4.6101 | Batch Acc: 0.2500\n",
            "Batch 9/72 | Loss: 5.2196 | Batch Acc: 0.1250\n",
            "Batch 10/72 | Loss: 5.3225 | Batch Acc: 0.1250\n",
            "Batch 11/72 | Loss: 4.5721 | Batch Acc: 0.2500\n",
            "Batch 12/72 | Loss: 4.8423 | Batch Acc: 0.3750\n",
            "Batch 13/72 | Loss: 6.0179 | Batch Acc: 0.2500\n",
            "Batch 14/72 | Loss: 6.3484 | Batch Acc: 0.1875\n",
            "Batch 15/72 | Loss: 5.6977 | Batch Acc: 0.1875\n",
            "Batch 16/72 | Loss: 5.4065 | Batch Acc: 0.1875\n",
            "Batch 17/72 | Loss: 4.9637 | Batch Acc: 0.1875\n",
            "Batch 18/72 | Loss: 4.3571 | Batch Acc: 0.5000\n",
            "Batch 19/72 | Loss: 5.9923 | Batch Acc: 0.3125\n",
            "Batch 20/72 | Loss: 7.0070 | Batch Acc: 0.1250\n",
            "Batch 21/72 | Loss: 4.4325 | Batch Acc: 0.3750\n",
            "Batch 22/72 | Loss: 4.5999 | Batch Acc: 0.4375\n",
            "Batch 23/72 | Loss: 4.9243 | Batch Acc: 0.3750\n",
            "Batch 24/72 | Loss: 4.9121 | Batch Acc: 0.3750\n",
            "Batch 25/72 | Loss: 5.4875 | Batch Acc: 0.1875\n",
            "Batch 26/72 | Loss: 5.3089 | Batch Acc: 0.2500\n",
            "Batch 27/72 | Loss: 4.8704 | Batch Acc: 0.3125\n",
            "Batch 28/72 | Loss: 4.8076 | Batch Acc: 0.3125\n",
            "Batch 29/72 | Loss: 4.0307 | Batch Acc: 0.4375\n",
            "Batch 30/72 | Loss: 4.4265 | Batch Acc: 0.5625\n",
            "Batch 31/72 | Loss: 4.8719 | Batch Acc: 0.3125\n",
            "Batch 32/72 | Loss: 3.7388 | Batch Acc: 0.5625\n",
            "Batch 33/72 | Loss: 4.4896 | Batch Acc: 0.3125\n",
            "Batch 34/72 | Loss: 4.4527 | Batch Acc: 0.4375\n",
            "Batch 35/72 | Loss: 6.4511 | Batch Acc: 0.0625\n",
            "Batch 36/72 | Loss: 5.8004 | Batch Acc: 0.3125\n",
            "Batch 37/72 | Loss: 4.3357 | Batch Acc: 0.2500\n",
            "Batch 38/72 | Loss: 5.5809 | Batch Acc: 0.3750\n",
            "Batch 39/72 | Loss: 5.1138 | Batch Acc: 0.3750\n",
            "Batch 40/72 | Loss: 6.1498 | Batch Acc: 0.2500\n",
            "Batch 41/72 | Loss: 4.0042 | Batch Acc: 0.4375\n",
            "Batch 42/72 | Loss: 4.9474 | Batch Acc: 0.4375\n",
            "Batch 43/72 | Loss: 4.0859 | Batch Acc: 0.3750\n",
            "Batch 44/72 | Loss: 4.7495 | Batch Acc: 0.2500\n",
            "Batch 45/72 | Loss: 6.9471 | Batch Acc: 0.1250\n",
            "Batch 46/72 | Loss: 5.6136 | Batch Acc: 0.2500\n",
            "Batch 47/72 | Loss: 4.7818 | Batch Acc: 0.3750\n",
            "Batch 48/72 | Loss: 4.7793 | Batch Acc: 0.2500\n",
            "Batch 49/72 | Loss: 5.5652 | Batch Acc: 0.1250\n",
            "Batch 50/72 | Loss: 5.4174 | Batch Acc: 0.2500\n",
            "Batch 51/72 | Loss: 6.3154 | Batch Acc: 0.3125\n",
            "Batch 52/72 | Loss: 4.3134 | Batch Acc: 0.3750\n",
            "Batch 53/72 | Loss: 4.9249 | Batch Acc: 0.3125\n",
            "Batch 54/72 | Loss: 6.1828 | Batch Acc: 0.1250\n",
            "Batch 55/72 | Loss: 5.2708 | Batch Acc: 0.3125\n",
            "Batch 56/72 | Loss: 5.7501 | Batch Acc: 0.1875\n",
            "Batch 57/72 | Loss: 5.1842 | Batch Acc: 0.3125\n",
            "Batch 58/72 | Loss: 4.8712 | Batch Acc: 0.4375\n",
            "Batch 59/72 | Loss: 4.0289 | Batch Acc: 0.5000\n",
            "Batch 60/72 | Loss: 6.5724 | Batch Acc: 0.1250\n",
            "Batch 61/72 | Loss: 4.7233 | Batch Acc: 0.2500\n",
            "Batch 62/72 | Loss: 5.9624 | Batch Acc: 0.2500\n",
            "Batch 63/72 | Loss: 4.8843 | Batch Acc: 0.4375\n",
            "Batch 64/72 | Loss: 5.4962 | Batch Acc: 0.0000\n",
            "Batch 65/72 | Loss: 5.1428 | Batch Acc: 0.1250\n",
            "Batch 66/72 | Loss: 5.4845 | Batch Acc: 0.3125\n",
            "Batch 67/72 | Loss: 5.3581 | Batch Acc: 0.1250\n",
            "Batch 68/72 | Loss: 6.5469 | Batch Acc: 0.1875\n",
            "Batch 69/72 | Loss: 4.6475 | Batch Acc: 0.4375\n",
            "Batch 70/72 | Loss: 5.4105 | Batch Acc: 0.1250\n",
            "Batch 71/72 | Loss: 5.6715 | Batch Acc: 0.1875\n",
            "Batch 72/72 | Loss: 5.3843 | Batch Acc: 0.2308\n",
            "Epoch Train Loss: 372.5611, Accuracy: 0.2802\n",
            "Validation Accuracy: 0.3466\n",
            "\n",
            "Epoch 31/50\n",
            "Batch 1/72 | Loss: 5.0880 | Batch Acc: 0.3125\n",
            "Batch 2/72 | Loss: 5.4288 | Batch Acc: 0.3125\n",
            "Batch 3/72 | Loss: 5.1182 | Batch Acc: 0.2500\n",
            "Batch 4/72 | Loss: 5.0814 | Batch Acc: 0.2500\n",
            "Batch 5/72 | Loss: 5.0698 | Batch Acc: 0.3750\n",
            "Batch 6/72 | Loss: 5.4774 | Batch Acc: 0.3125\n",
            "Batch 7/72 | Loss: 5.1234 | Batch Acc: 0.2500\n",
            "Batch 8/72 | Loss: 5.1518 | Batch Acc: 0.1875\n",
            "Batch 9/72 | Loss: 5.9021 | Batch Acc: 0.2500\n",
            "Batch 10/72 | Loss: 5.5476 | Batch Acc: 0.3125\n",
            "Batch 11/72 | Loss: 5.3765 | Batch Acc: 0.3750\n",
            "Batch 12/72 | Loss: 5.3776 | Batch Acc: 0.0625\n",
            "Batch 13/72 | Loss: 5.7647 | Batch Acc: 0.1250\n",
            "Batch 14/72 | Loss: 4.4002 | Batch Acc: 0.5000\n",
            "Batch 15/72 | Loss: 5.5646 | Batch Acc: 0.1875\n",
            "Batch 16/72 | Loss: 4.9810 | Batch Acc: 0.3750\n",
            "Batch 17/72 | Loss: 6.0550 | Batch Acc: 0.2500\n",
            "Batch 18/72 | Loss: 4.6985 | Batch Acc: 0.2500\n",
            "Batch 19/72 | Loss: 5.4091 | Batch Acc: 0.1250\n",
            "Batch 20/72 | Loss: 5.2869 | Batch Acc: 0.1250\n",
            "Batch 21/72 | Loss: 5.8377 | Batch Acc: 0.1875\n",
            "Batch 22/72 | Loss: 6.4349 | Batch Acc: 0.3750\n",
            "Batch 23/72 | Loss: 4.9186 | Batch Acc: 0.1875\n",
            "Batch 24/72 | Loss: 5.6520 | Batch Acc: 0.2500\n",
            "Batch 25/72 | Loss: 4.1119 | Batch Acc: 0.3750\n",
            "Batch 26/72 | Loss: 4.7683 | Batch Acc: 0.2500\n",
            "Batch 27/72 | Loss: 5.3503 | Batch Acc: 0.2500\n",
            "Batch 28/72 | Loss: 5.3636 | Batch Acc: 0.3125\n",
            "Batch 29/72 | Loss: 5.2673 | Batch Acc: 0.1875\n",
            "Batch 30/72 | Loss: 3.8532 | Batch Acc: 0.4375\n",
            "Batch 31/72 | Loss: 6.6246 | Batch Acc: 0.1250\n",
            "Batch 32/72 | Loss: 5.0854 | Batch Acc: 0.3750\n",
            "Batch 33/72 | Loss: 4.3057 | Batch Acc: 0.3750\n",
            "Batch 34/72 | Loss: 4.7664 | Batch Acc: 0.1875\n",
            "Batch 35/72 | Loss: 5.4470 | Batch Acc: 0.3750\n",
            "Batch 36/72 | Loss: 5.4789 | Batch Acc: 0.1875\n",
            "Batch 37/72 | Loss: 5.4592 | Batch Acc: 0.1250\n",
            "Batch 38/72 | Loss: 5.3962 | Batch Acc: 0.1875\n",
            "Batch 39/72 | Loss: 4.3322 | Batch Acc: 0.3125\n",
            "Batch 40/72 | Loss: 5.5211 | Batch Acc: 0.3125\n",
            "Batch 41/72 | Loss: 5.0644 | Batch Acc: 0.2500\n",
            "Batch 42/72 | Loss: 6.3300 | Batch Acc: 0.0625\n",
            "Batch 43/72 | Loss: 5.2929 | Batch Acc: 0.3750\n",
            "Batch 44/72 | Loss: 3.9499 | Batch Acc: 0.3750\n",
            "Batch 45/72 | Loss: 4.9615 | Batch Acc: 0.2500\n",
            "Batch 46/72 | Loss: 5.5194 | Batch Acc: 0.4375\n",
            "Batch 47/72 | Loss: 5.3180 | Batch Acc: 0.1875\n",
            "Batch 48/72 | Loss: 4.2572 | Batch Acc: 0.4375\n",
            "Batch 49/72 | Loss: 4.6700 | Batch Acc: 0.3750\n",
            "Batch 50/72 | Loss: 4.8228 | Batch Acc: 0.3750\n",
            "Batch 51/72 | Loss: 5.2118 | Batch Acc: 0.3125\n",
            "Batch 52/72 | Loss: 5.2243 | Batch Acc: 0.3125\n",
            "Batch 53/72 | Loss: 4.8549 | Batch Acc: 0.5000\n",
            "Batch 54/72 | Loss: 4.9794 | Batch Acc: 0.4375\n",
            "Batch 55/72 | Loss: 4.2578 | Batch Acc: 0.3125\n",
            "Batch 56/72 | Loss: 4.8138 | Batch Acc: 0.3125\n",
            "Batch 57/72 | Loss: 6.5483 | Batch Acc: 0.1250\n",
            "Batch 58/72 | Loss: 5.6690 | Batch Acc: 0.1250\n",
            "Batch 59/72 | Loss: 6.0792 | Batch Acc: 0.3125\n",
            "Batch 60/72 | Loss: 5.1162 | Batch Acc: 0.3125\n",
            "Batch 61/72 | Loss: 5.2278 | Batch Acc: 0.3125\n",
            "Batch 62/72 | Loss: 4.4037 | Batch Acc: 0.3125\n",
            "Batch 63/72 | Loss: 4.3818 | Batch Acc: 0.1875\n",
            "Batch 64/72 | Loss: 6.0346 | Batch Acc: 0.0625\n",
            "Batch 65/72 | Loss: 5.7600 | Batch Acc: 0.0625\n",
            "Batch 66/72 | Loss: 5.4102 | Batch Acc: 0.3125\n",
            "Batch 67/72 | Loss: 5.0621 | Batch Acc: 0.3750\n",
            "Batch 68/72 | Loss: 5.3371 | Batch Acc: 0.1875\n",
            "Batch 69/72 | Loss: 5.1086 | Batch Acc: 0.2500\n",
            "Batch 70/72 | Loss: 5.1491 | Batch Acc: 0.3125\n",
            "Batch 71/72 | Loss: 3.9326 | Batch Acc: 0.6250\n",
            "Batch 72/72 | Loss: 4.6934 | Batch Acc: 0.4615\n",
            "Epoch Train Loss: 373.3190, Accuracy: 0.2802\n",
            "Validation Accuracy: 0.2955\n",
            "⏹️ 觸發 Early Stopping\n",
            "Validation Accuracy: 0.3295\n",
            "\n",
            "Final Test Accuracy: 0.3295\n"
          ]
        }
      ],
      "source": [
        "# Step 7: 訓練流程\n",
        "best_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "    train_one_epoch_amp(model, train_loader)\n",
        "    val_acc = evaluate(model, val_loader)\n",
        "    # print(\"validation accuracy: \", val_acc)\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        print(f\"✅ 儲存最佳模型（Acc: {best_acc:.4f}）\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= early_stop_patience:\n",
        "            print(\"⏹️ 觸發 Early Stopping\")\n",
        "            break\n",
        "\n",
        "test_acc = evaluate(model, test_loader)\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRZynCkG5A_c",
        "outputId": "cece6b91-206d-497a-c458-68a3664983b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.3295\n",
            "\n",
            "Final Test Accuracy: 0.3295\n"
          ]
        }
      ],
      "source": [
        "test_acc = evaluate(model, test_loader)\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcFN_o-nhoXj",
        "outputId": "fcc60241-98c4-4818-ae1f-fc16a99be3ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PatchGNNNet(\n",
              "  (stem): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): Mish()\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): Mish()\n",
              "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (9): Mish()\n",
              "    (10): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (12): Mish()\n",
              "  )\n",
              "  (gc_in): Linear(in_features=128, out_features=256, bias=False)\n",
              "  (gc_nei): Linear(in_features=128, out_features=256, bias=False)\n",
              "  (gc_act): Mish()\n",
              "  (mlp_head): Sequential(\n",
              "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (1): Mish()\n",
              "    (2): Dropout(p=0.3, inplace=False)\n",
              "    (3): Linear(in_features=256, out_features=22, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Step 8: 載入最佳模型（推論前）\n",
        "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSdGpwm95A_d",
        "outputId": "e8742d7c-84f9-4fb7-814c-26307fb0ab02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node 1: 0.0444\n",
            "Node 2: 0.1711\n",
            "Node 3: 0.0841\n",
            "Node 4: 0.0410\n",
            "Node 5: 0.0562\n",
            "Node 6: 0.0062\n",
            "Node 7: 0.1582\n",
            "Node 8: 0.0930\n",
            "Node 9: 0.0484\n",
            "Node 10: 0.0333\n",
            "Node 11: 0.0445\n",
            "Node 12: 0.0031\n",
            "Node 13: 0.0734\n",
            "Node 14: 0.0353\n",
            "Node 15: 0.0077\n",
            "Node 16: 0.0038\n",
            "Node 17: 0.0027\n",
            "Node 18: 0.0017\n",
            "Node 19: 0.0045\n",
            "Node 20: 0.0068\n",
            "Node 21: 0.0030\n",
            "Node 22: 0.0776\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Load model\n",
        "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# Load and preprocess image\n",
        "img_path = \"/content/drive/MyDrive/DLA_term_project_data/classified_data/test/node_15/IMG_4393.jpg\"\n",
        "img = Image.open(img_path).convert(\"RGB\")\n",
        "input_tensor = test_transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "# Predict\n",
        "with torch.no_grad():\n",
        "    logits = model(input_tensor)\n",
        "    probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "\n",
        "# Print probabilities\n",
        "for idx, prob in enumerate(probs):\n",
        "    print(f\"Node {idx+1}: {prob:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMyafEEN5A_d",
        "outputId": "f900a6e9-11d4-4574-940b-11cc2e3ae07c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 1: True Node=1, Predicted Node=14 ❌\n",
            "Sample 2: True Node=1, Predicted Node=2 ❌\n",
            "Sample 3: True Node=1, Predicted Node=14 ❌\n",
            "Sample 4: True Node=1, Predicted Node=16 ❌\n",
            "Sample 5: True Node=1, Predicted Node=14 ❌\n",
            "Sample 6: True Node=1, Predicted Node=14 ❌\n",
            "Sample 7: True Node=1, Predicted Node=14 ❌\n",
            "Sample 8: True Node=1, Predicted Node=14 ❌\n",
            "Sample 9: True Node=2, Predicted Node=2 ✅\n",
            "Sample 10: True Node=2, Predicted Node=2 ✅\n",
            "Sample 11: True Node=2, Predicted Node=19 ❌\n",
            "Sample 12: True Node=2, Predicted Node=20 ❌\n",
            "Sample 13: True Node=2, Predicted Node=2 ✅\n",
            "Sample 14: True Node=2, Predicted Node=2 ✅\n",
            "Sample 15: True Node=2, Predicted Node=2 ✅\n",
            "Sample 16: True Node=2, Predicted Node=2 ✅\n",
            "Sample 17: True Node=3, Predicted Node=2 ❌\n",
            "Sample 18: True Node=3, Predicted Node=8 ❌\n",
            "Sample 19: True Node=3, Predicted Node=19 ❌\n",
            "Sample 20: True Node=3, Predicted Node=15 ❌\n",
            "Sample 21: True Node=3, Predicted Node=9 ❌\n",
            "Sample 22: True Node=3, Predicted Node=5 ❌\n",
            "Sample 23: True Node=3, Predicted Node=8 ❌\n",
            "Sample 24: True Node=3, Predicted Node=4 ❌\n",
            "Sample 25: True Node=4, Predicted Node=10 ❌\n",
            "Sample 26: True Node=4, Predicted Node=4 ✅\n",
            "Sample 27: True Node=4, Predicted Node=7 ❌\n",
            "Sample 28: True Node=4, Predicted Node=6 ❌\n",
            "Sample 29: True Node=4, Predicted Node=19 ❌\n",
            "Sample 30: True Node=4, Predicted Node=4 ✅\n",
            "Sample 31: True Node=4, Predicted Node=10 ❌\n",
            "Sample 32: True Node=4, Predicted Node=4 ✅\n",
            "Sample 33: True Node=5, Predicted Node=4 ❌\n",
            "Sample 34: True Node=5, Predicted Node=4 ❌\n",
            "Sample 35: True Node=5, Predicted Node=5 ✅\n",
            "Sample 36: True Node=5, Predicted Node=5 ✅\n",
            "Sample 37: True Node=5, Predicted Node=5 ✅\n",
            "Sample 38: True Node=5, Predicted Node=4 ❌\n",
            "Sample 39: True Node=5, Predicted Node=5 ✅\n",
            "Sample 40: True Node=5, Predicted Node=4 ❌\n",
            "Sample 41: True Node=6, Predicted Node=21 ❌\n",
            "Sample 42: True Node=6, Predicted Node=18 ❌\n",
            "Sample 43: True Node=6, Predicted Node=21 ❌\n",
            "Sample 44: True Node=6, Predicted Node=21 ❌\n",
            "Sample 45: True Node=6, Predicted Node=6 ✅\n",
            "Sample 46: True Node=6, Predicted Node=21 ❌\n",
            "Sample 47: True Node=6, Predicted Node=21 ❌\n",
            "Sample 48: True Node=6, Predicted Node=21 ❌\n",
            "Sample 49: True Node=7, Predicted Node=7 ✅\n",
            "Sample 50: True Node=7, Predicted Node=9 ❌\n",
            "Sample 51: True Node=7, Predicted Node=5 ❌\n",
            "Sample 52: True Node=7, Predicted Node=2 ❌\n",
            "Sample 53: True Node=7, Predicted Node=7 ✅\n",
            "Sample 54: True Node=7, Predicted Node=9 ❌\n",
            "Sample 55: True Node=7, Predicted Node=7 ✅\n",
            "Sample 56: True Node=7, Predicted Node=8 ❌\n",
            "Sample 57: True Node=8, Predicted Node=8 ✅\n",
            "Sample 58: True Node=8, Predicted Node=8 ✅\n",
            "Sample 59: True Node=8, Predicted Node=8 ✅\n",
            "Sample 60: True Node=8, Predicted Node=8 ✅\n",
            "Sample 61: True Node=8, Predicted Node=5 ❌\n",
            "Sample 62: True Node=8, Predicted Node=8 ✅\n",
            "Sample 63: True Node=8, Predicted Node=8 ✅\n",
            "Sample 64: True Node=8, Predicted Node=8 ✅\n",
            "Sample 65: True Node=9, Predicted Node=2 ❌\n",
            "Sample 66: True Node=9, Predicted Node=9 ✅\n",
            "Sample 67: True Node=9, Predicted Node=7 ❌\n",
            "Sample 68: True Node=9, Predicted Node=2 ❌\n",
            "Sample 69: True Node=9, Predicted Node=2 ❌\n",
            "Sample 70: True Node=9, Predicted Node=14 ❌\n",
            "Sample 71: True Node=9, Predicted Node=2 ❌\n",
            "Sample 72: True Node=9, Predicted Node=2 ❌\n",
            "Sample 73: True Node=10, Predicted Node=10 ✅\n",
            "Sample 74: True Node=10, Predicted Node=10 ✅\n",
            "Sample 75: True Node=10, Predicted Node=10 ✅\n",
            "Sample 76: True Node=10, Predicted Node=19 ❌\n",
            "Sample 77: True Node=10, Predicted Node=14 ❌\n",
            "Sample 78: True Node=10, Predicted Node=10 ✅\n",
            "Sample 79: True Node=10, Predicted Node=14 ❌\n",
            "Sample 80: True Node=10, Predicted Node=14 ❌\n",
            "Sample 81: True Node=11, Predicted Node=10 ❌\n",
            "Sample 82: True Node=11, Predicted Node=10 ❌\n",
            "Sample 83: True Node=11, Predicted Node=2 ❌\n",
            "Sample 84: True Node=11, Predicted Node=13 ❌\n",
            "Sample 85: True Node=11, Predicted Node=10 ❌\n",
            "Sample 86: True Node=11, Predicted Node=10 ❌\n",
            "Sample 87: True Node=11, Predicted Node=10 ❌\n",
            "Sample 88: True Node=11, Predicted Node=13 ❌\n",
            "Sample 89: True Node=12, Predicted Node=18 ❌\n",
            "Sample 90: True Node=12, Predicted Node=18 ❌\n",
            "Sample 91: True Node=12, Predicted Node=18 ❌\n",
            "Sample 92: True Node=12, Predicted Node=19 ❌\n",
            "Sample 93: True Node=12, Predicted Node=18 ❌\n",
            "Sample 94: True Node=12, Predicted Node=18 ❌\n",
            "Sample 95: True Node=12, Predicted Node=18 ❌\n",
            "Sample 96: True Node=12, Predicted Node=18 ❌\n",
            "Sample 97: True Node=13, Predicted Node=9 ❌\n",
            "Sample 98: True Node=13, Predicted Node=18 ❌\n",
            "Sample 99: True Node=13, Predicted Node=20 ❌\n",
            "Sample 100: True Node=13, Predicted Node=2 ❌\n",
            "Sample 101: True Node=13, Predicted Node=18 ❌\n",
            "Sample 102: True Node=13, Predicted Node=14 ❌\n",
            "Sample 103: True Node=13, Predicted Node=2 ❌\n",
            "Sample 104: True Node=13, Predicted Node=13 ✅\n",
            "Sample 105: True Node=14, Predicted Node=2 ❌\n",
            "Sample 106: True Node=14, Predicted Node=14 ✅\n",
            "Sample 107: True Node=14, Predicted Node=14 ✅\n",
            "Sample 108: True Node=14, Predicted Node=14 ✅\n",
            "Sample 109: True Node=14, Predicted Node=14 ✅\n",
            "Sample 110: True Node=14, Predicted Node=14 ✅\n",
            "Sample 111: True Node=14, Predicted Node=14 ✅\n",
            "Sample 112: True Node=14, Predicted Node=14 ✅\n",
            "Sample 113: True Node=15, Predicted Node=19 ❌\n",
            "Sample 114: True Node=15, Predicted Node=15 ✅\n",
            "Sample 115: True Node=15, Predicted Node=6 ❌\n",
            "Sample 116: True Node=15, Predicted Node=15 ✅\n",
            "Sample 117: True Node=15, Predicted Node=6 ❌\n",
            "Sample 118: True Node=15, Predicted Node=22 ❌\n",
            "Sample 119: True Node=15, Predicted Node=5 ❌\n",
            "Sample 120: True Node=15, Predicted Node=22 ❌\n",
            "Sample 121: True Node=16, Predicted Node=21 ❌\n",
            "Sample 122: True Node=16, Predicted Node=18 ❌\n",
            "Sample 123: True Node=16, Predicted Node=20 ❌\n",
            "Sample 124: True Node=16, Predicted Node=19 ❌\n",
            "Sample 125: True Node=16, Predicted Node=18 ❌\n",
            "Sample 126: True Node=16, Predicted Node=18 ❌\n",
            "Sample 127: True Node=16, Predicted Node=18 ❌\n",
            "Sample 128: True Node=16, Predicted Node=18 ❌\n",
            "Sample 129: True Node=17, Predicted Node=18 ❌\n",
            "Sample 130: True Node=17, Predicted Node=18 ❌\n",
            "Sample 131: True Node=17, Predicted Node=19 ❌\n",
            "Sample 132: True Node=17, Predicted Node=19 ❌\n",
            "Sample 133: True Node=17, Predicted Node=18 ❌\n",
            "Sample 134: True Node=17, Predicted Node=18 ❌\n",
            "Sample 135: True Node=17, Predicted Node=18 ❌\n",
            "Sample 136: True Node=17, Predicted Node=19 ❌\n",
            "Sample 137: True Node=18, Predicted Node=18 ✅\n",
            "Sample 138: True Node=18, Predicted Node=18 ✅\n",
            "Sample 139: True Node=18, Predicted Node=19 ❌\n",
            "Sample 140: True Node=18, Predicted Node=18 ✅\n",
            "Sample 141: True Node=18, Predicted Node=18 ✅\n",
            "Sample 142: True Node=18, Predicted Node=18 ✅\n",
            "Sample 143: True Node=18, Predicted Node=18 ✅\n",
            "Sample 144: True Node=18, Predicted Node=19 ❌\n",
            "Sample 145: True Node=19, Predicted Node=18 ❌\n",
            "Sample 146: True Node=19, Predicted Node=18 ❌\n",
            "Sample 147: True Node=19, Predicted Node=19 ✅\n",
            "Sample 148: True Node=19, Predicted Node=19 ✅\n",
            "Sample 149: True Node=19, Predicted Node=18 ❌\n",
            "Sample 150: True Node=19, Predicted Node=18 ❌\n",
            "Sample 151: True Node=19, Predicted Node=18 ❌\n",
            "Sample 152: True Node=19, Predicted Node=18 ❌\n",
            "Sample 153: True Node=20, Predicted Node=19 ❌\n",
            "Sample 154: True Node=20, Predicted Node=18 ❌\n",
            "Sample 155: True Node=20, Predicted Node=19 ❌\n",
            "Sample 156: True Node=20, Predicted Node=19 ❌\n",
            "Sample 157: True Node=20, Predicted Node=18 ❌\n",
            "Sample 158: True Node=20, Predicted Node=18 ❌\n",
            "Sample 159: True Node=20, Predicted Node=18 ❌\n",
            "Sample 160: True Node=20, Predicted Node=18 ❌\n",
            "Sample 161: True Node=21, Predicted Node=21 ✅\n",
            "Sample 162: True Node=21, Predicted Node=21 ✅\n",
            "Sample 163: True Node=21, Predicted Node=21 ✅\n",
            "Sample 164: True Node=21, Predicted Node=21 ✅\n",
            "Sample 165: True Node=21, Predicted Node=22 ❌\n",
            "Sample 166: True Node=21, Predicted Node=21 ✅\n",
            "Sample 167: True Node=21, Predicted Node=21 ✅\n",
            "Sample 168: True Node=21, Predicted Node=21 ✅\n",
            "Sample 169: True Node=22, Predicted Node=14 ❌\n",
            "Sample 170: True Node=22, Predicted Node=22 ✅\n",
            "Sample 171: True Node=22, Predicted Node=21 ❌\n",
            "Sample 172: True Node=22, Predicted Node=14 ❌\n",
            "Sample 173: True Node=22, Predicted Node=19 ❌\n",
            "Sample 174: True Node=22, Predicted Node=2 ❌\n",
            "Sample 175: True Node=22, Predicted Node=14 ❌\n",
            "Sample 176: True Node=22, Predicted Node=19 ❌\n",
            "\n",
            "Misclassification count per node (True label):\n",
            "Node 1: 8\n",
            "Node 2: 2\n",
            "Node 3: 8\n",
            "Node 4: 5\n",
            "Node 5: 4\n",
            "Node 6: 7\n",
            "Node 7: 5\n",
            "Node 8: 1\n",
            "Node 9: 7\n",
            "Node 10: 4\n",
            "Node 11: 8\n",
            "Node 12: 8\n",
            "Node 13: 7\n",
            "Node 14: 1\n",
            "Node 15: 6\n",
            "Node 16: 8\n",
            "Node 17: 8\n",
            "Node 18: 2\n",
            "Node 19: 6\n",
            "Node 20: 8\n",
            "Node 21: 1\n",
            "Node 22: 7\n",
            "\n",
            "Node most often misclassified: Node 1 (8 times)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        preds = torch.argmax(probs, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Print predictions and ground truths\n",
        "for i, (pred, label) in enumerate(zip(all_preds, all_labels)):\n",
        "    correct = \"✅\" if pred == label else \"❌\"\n",
        "    print(f\"Sample {i+1}: True Node={label+1}, Predicted Node={pred+1} {correct}\")\n",
        "\n",
        "# Count misclassifications per node\n",
        "misclass_counts = Counter()\n",
        "for pred, label in zip(all_preds, all_labels):\n",
        "    if pred != label:\n",
        "        misclass_counts[label+1] += 1  # +1 for 1-based node index\n",
        "\n",
        "print(\"\\nMisclassification count per node (True label):\")\n",
        "for node in range(1, len(test_loader.dataset.classes)+1):\n",
        "    print(f\"Node {node}: {misclass_counts[node]}\")\n",
        "\n",
        "# Find the most misclassified node(s)\n",
        "if misclass_counts:\n",
        "    most_misclassified = misclass_counts.most_common(1)[0]\n",
        "    print(f\"\\nNode most often misclassified: Node {most_misclassified[0]} ({most_misclassified[1]} times)\")\n",
        "else:\n",
        "    print(\"\\nNo misclassifications!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
