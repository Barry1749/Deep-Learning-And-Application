{"cells":[{"cell_type":"markdown","metadata":{"id":"1YSHKi41hoXZ"},"source":[]},{"cell_type":"code","source":["#在Colab用\n","!pip install -Uq timm==0.9.12\n"],"metadata":{"id":"pau5RAGu5JFd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749206178045,"user_tz":-480,"elapsed":67894,"user":{"displayName":"Chaha","userId":"17365212891346415618"}},"outputId":"914870fb-c90d-46d6-bdc4-96b14994dbfc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["#授權\n","from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"id":"tbxBw8HI5RzE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749206202555,"user_tz":-480,"elapsed":24506,"user":{"displayName":"Chaha","userId":"17365212891346415618"}},"outputId":"b7e69258-8a56-4d8f-a73e-b7af379497d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9EKp_bKGhoXg"},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","from torch.cuda.amp import GradScaler, autocast\n","import torch.nn.functional as F\n","import timm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"58oc3dAUQQAB"},"outputs":[],"source":["num_epochs = 50\n","batch_size = 16\n","early_stop_patience = 4\n","patience_counter = 0\n","loss_alpha=1.0\n","loss_beta=0.5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bS_YldbqhoXg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749206226200,"user_tz":-480,"elapsed":12641,"user":{"displayName":"Chaha","userId":"17365212891346415618"}},"outputId":"cd0e4d08-83fd-4a8a-9e00-cb3af5f58c52"},"outputs":[{"output_type":"stream","name":"stdout","text":["類別數量： 22\n"]}],"source":["base_dir = \"/content/drive/MyDrive/DLA_term_project_data/classified_data\"\n","\n","train_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                         std=[0.229, 0.224, 0.225])\n","])\n","imagenet_stats = [(0.485, 0.456, 0.406), (0.229, 0.224, 0.225)]\n","train_transform = transforms.Compose([\n","    transforms.RandomResizedCrop(224),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(imagenet_stats[0], imagenet_stats[1])\n","])\n","\n","transform_val = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                         std=[0.229, 0.224, 0.225])\n","])\n","\n","validation_transform = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(imagenet_stats[0], imagenet_stats[1])\n","])\n","\n","test_transform = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(imagenet_stats[0], imagenet_stats[1])\n","])\n","\n","train_dataset = datasets.ImageFolder(os.path.join(base_dir, 'train'), transform=train_transform)\n","val_dataset = datasets.ImageFolder(os.path.join(base_dir, 'validation'), transform=validation_transform)\n","test_data = datasets.ImageFolder(os.path.join(base_dir, \"test\"), transform=test_transform)\n","\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","print(\"類別數量：\", len(train_dataset.classes))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a1xpCePFPJef"},"outputs":[],"source":["# customed loss function\n","# ----- 1. Node-to-Node Distance Matrix -----\n","node_distance_matrix = torch.tensor([\n","    [0,1,2,2,1,2,2,2.5,3.5,4,5.5,5,4,2.5,3.5,4,5,3.5,4.5,3.5,2.5,2.5],\n","    [1,0,1,3,2,3,1,1.5,2.5,3,4.5,4,3,1.5,4.5,5,6,4.5,5.5,4.5,3.5,3.5],\n","    [2,1,0,2,3,4,2,2.5,3.5,4,5.5,3,2,0.5,3.5,4,5,3.5,4.5,5.5,4.5,2.5],\n","    [2,3,2,0,1,2,4,4.5,5.5,6,7.5,5,4,2.5,1.5,2,3,1.5,2.5,3.5,2.5,0.5],\n","    [1,2,3,1,0,1,3,3.5,4.5,5,6.5,6,5,3.5,2.5,3,4,2.5,3.5,2.5,1.5,1.5],\n","    [2,3,4,2,1,0,2,2.5,3.5,4,5.5,7,6,4.5,3.5,4,5,3.5,2.5,1.5,0.5,2.5],\n","    [2,1,2,4,3,2,0,0.5,1.5,2,3.5,5,4,2.5,5.5,6,7,5.5,4.5,3.5,2.5,4.5],\n","    [2.5,1.5,2.5,4.5,3.5,2.5,0.5,0,1,1.5,3,3.5,4.5,3,6,6.5,7.5,6,5,4,3,5],\n","    [3.5,2.5,3.5,5.5,4.5,3.5,1.5,1,0,0.5,2,2.5,3.5,4,7,7.5,8.5,7,6,5,4,6],\n","    [4,3,4,6,5,4,2,1.5,0.5,0,1.5,2,3,4.5,7.5,8,9,7.5,6.5,5.5,4.5,6.5],\n","    [5.5,4.5,5.5,7.5,6.5,5.5,3.5,3,2,1.5,0,0.5,1.5,3,7,7.5,8.5,7,8,7,6,6.5],\n","    [5,4,3,5,6,7,5,3.5,2.5,2,0.5,0,1,2.5,6.5,7,8,6.5,7.5,7.5,6.5,5.5],\n","    [4,3,2,4,5,6,4,4.5,3.5,3,1.5,1,0,1.5,5.5,6,7,5.5,6.5,7.5,7,4.5],\n","    [2.5,1.5,0.5,2.5,3.5,4.5,2.5,3,4,4.5,3,2.5,1.5,0,4,4.5,5.5,4,5,6,5.5,3],\n","    [3.5,4.5,3.5,1.5,2.5,3.5,5.5,6,7,7.5,7,6.5,5.5,4,0,0.5,1.5,0.5,1.5,2.5,3.5,1],\n","    [4,5,4,2,3,4,6,6.5,7.5,8,7.5,7,6,4.5,0.5,0,1,1,2,3,4,1.5],\n","    [5,6,5,3,4,5,7,7.5,8.5,9,8.5,8,7,5.5,1.5,1,0,1.5,2.5,3.5,4.5,2.5],\n","    [3.5,4.5,3.5,1.5,2.5,3.5,5.5,6,7,7.5,7,6.5,5.5,4,0.5,1,1.5,0,1,2,3,1],\n","    [4.5,5.5,4.5,2.5,3.5,2.5,4.5,5,6,6.5,8,7.5,6.5,5,1.5,2,2.5,1,0,1,2,2],\n","    [3.5,4.5,5.5,3.5,2.5,1.5,3.5,4,5,5.5,7,7.5,7.5,6,2.5,3,3.5,2,1,0,1,3],\n","    [2.5,3.5,4.5,2.5,1.5,0.5,2.5,3,4,4.5,6,6.5,7,5.5,3.5,4,4.5,3,2,1,0,3],\n","    [2.5,3.5,2.5,0.5,1.5,2.5,4.5,5,6,6.5,6.5,5.5,4.5,3,1,1.5,2.5,1,2,3,3,0]\n","], dtype=torch.float)\n","\n","# ----- 2. Area-to-Area Distance Matrix -----\n","area_distance_matrix = torch.tensor([\n","    [0, 1, 2, 2, 2, 2],\n","    [1, 0, 1, 1, 1, 1],\n","    [2, 1, 0, 1, 3, 3],\n","    [2, 1, 1, 0, 3, 3],\n","    [2, 1, 3, 3, 0, 1],\n","    [2, 1, 3, 3, 1, 0]\n","], dtype=torch.float)\n","\n","# ----- 3. Node-to-Area Mapping -----\n","node_to_area = torch.tensor([\n","    0, 0, 0, 0, 0, 0, 0,    # 1~7 → area 0\n","    1,                      # 8  → area 1\n","    2, 2,                   # 9~10 → area 2\n","    3, 3, 3,                # 11~13 → area 3\n","    1,                      # 14 → area 1\n","    4, 4, 4,                # 15~17 → area 4\n","    5, 5, 5,                # 18~20 → area 5\n","    1,                      # 21 → area 1\n","    1                       # 22 → area 1\n","], dtype=torch.long)\n","\n","# ----- 4. Custom Loss Function -----\n","class DistancePenaltyLoss(nn.Module):\n","    def __init__(self, node_distance_matrix, area_distance_matrix, node_to_area, alpha=1.0, beta=1.0):\n","        super().__init__()\n","        self.node_distance_matrix = node_distance_matrix\n","        self.area_distance_matrix = area_distance_matrix\n","        self.node_to_area = node_to_area\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.ce = nn.CrossEntropyLoss()\n","\n","    def forward(self, logits, targets):\n","        \"\"\"\n","        logits: (batch_size, 22)\n","        targets: (batch_size,) - ground truth class indices\n","        \"\"\"\n","        ce_loss = self.ce(logits, targets)\n","\n","        probs = F.softmax(logits, dim=1)  # (B, 22)\n","        batch_size = targets.size(0)\n","\n","        node_penalty = 0.0\n","        area_penalty = 0.0\n","\n","        for i in range(batch_size):\n","            target = targets[i]\n","            prob = probs[i]  # shape (22,)\n","\n","            # Node distance penalty\n","            node_distances = self.node_distance_matrix[target]  # (22,)\n","            node_penalty += (prob * node_distances).sum()\n","\n","            # Area distance penalty\n","            target_area = self.node_to_area[target]\n","            pred_areas = self.node_to_area  # shape (22,)\n","            area_dists = self.area_distance_matrix[target_area][pred_areas]  # shape (22,)\n","            area_penalty += (prob * area_dists).sum()\n","\n","        node_penalty /= batch_size\n","        area_penalty /= batch_size\n","\n","        total_loss = ce_loss + self.alpha * node_penalty + self.beta * area_penalty\n","        return total_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k5mafh1TQ0pA"},"outputs":[],"source":["# timm.list_models('convnext*')\n","# timm.list_models('convnext*', pretrained=True)\n","# timm.list_models('efficient*', pretrained=True)\n","# timm.list_models('res*', pretrained=True)\n","# timm.list_models('swin*', pretrained=True)\n","\n","# convnextv2_tiny.fcmae\n","# efficientvit_b0.r224_in1k\n","# efficientvit_b1.r224_in1k\n","# efficientvit_b2.r224_in1k\n","# efficientvit_b3.r224_in1k\n","# efficientvit_m0.r224_in1k\n","# efficientvit_m1.r224_in1k\n","# efficientvit_m2.r224_in1k\n","# efficientvit_m3.r224_in1k\n","# efficientvit_m4.r224_in1k\n","# efficientvit_m5.r224_in1k\n","# efficientformerv2_s0.snap_dist_in1k\n","# efficientformerv2_s1.snap_dist_in1k\n","# efficientformerv2_s2.snap_dist_in1k\n","# resnet50.a1_in1k\n","# resnet50d.ra2_in1k\n","# resnet50.fb_swsl_ig1b_ft_in1k\n","# resnet50.tv_in1k\n","# resnet50.ra_in1k\n","# res2net50d.in1k\n","# res2net50_14w_8s.in1k\n","# mobilevitv2_100.cvnets_in1k\n","# mobilevitv2_150.cvnets_in22k_ft_in1k\n","# mobilenetv3_large_100.ra_in1k\n","# mobileone_s1.apple_in1k\n","# mobilenetv2_100.ra_in1k\n","# swin_tiny_patch4_window7_224.ms_in1k\n","# swinv2_tiny_window8_256.ms_in1k\n","# swinv2_cr_tiny_ns_224.sw_in1k\n","\n","\n","\n","# ✅ Overall Best Choices (Balanced: Accuracy, Speed, Size)\n","# mobilevitv2_100.cvnets_in1k\t       Transformer-CNN hybrid, small but very accurate\n","# efficientnet_b0\t             Lightweight, easy to train, great accuracy/size tradeoff\n","# mobilenetv3_large_100.ra_in1k       Very fast, widely supported, good baseline for small datasets\n","# res2net50d.in1k\t             Strong feature extractor, better than plain ResNet50\n","# resnet50d.ra2_in1k             Modern ResNet variant with strong augmentations\n","# swinv2_tiny_window8_256.ms_in1k\t     Lightweight transformer, good if you want to try Swin family\n","\n","# 🧠 If You Want Transfer Learning Strength\n","# mobilevitv2_150.cvnets_in22k_ft_in1k\t  Pretrained on ImageNet-22K → fine-tuned on IN1K\n","# resnet50.fb_swsl_ig1b_ft_in1k\t       Trained on IG-1B dataset — great features even for few-shot\n","\n","# 🚀 If You Want Ultra-Fast Training\n","# mobileone_s1.apple_in1k          \tTiny, deployable, optimized for fast inference\n","# mobilenetv2_100.ra_in1k         \tClassic, efficient, and still performs well"]},{"cell_type":"code","source":["# ---------- 100% MLP baseline：Patch-MLP ----------\n","\n","#希望是好的baseline哈哈 或是你們如果有其他想法也可以試試看ㄛ\n","#大概的想法：把影像切成 (patch_size x patch_size) 小塊 → 線性投影 → 全域平均 → MLP 分類\n","\n","\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class PureMLP(nn.Module):\n","\n","    def __init__(self, num_classes=22, patch_size=16, proj_dim=256,\n","                 mlp_hidden=[512, 256], dropout=0.2):\n","        super().__init__()\n","        self.patch_size = patch_size\n","        patch_dim = 3 * patch_size * patch_size          # 每個 patch 的展開向量長度\n","        self.unfold = nn.Unfold(kernel_size=patch_size,\n","                                stride=patch_size)       # 切 patch\n","\n","        # ① patch projector（把每個 patch 投影到 proj_dim 維度）\n","        self.proj = nn.Sequential(\n","            nn.Linear(patch_dim, proj_dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout)\n","        )\n","\n","        # ② 圖像層級全域特徵 = 所有 patch 向量的平均\n","        # ③ MLP 分類頭\n","        mlp_layers = []\n","        in_dim = proj_dim\n","        for h in mlp_hidden:\n","            mlp_layers += [nn.Linear(in_dim, h), nn.GELU(), nn.Dropout(dropout)]\n","            in_dim = h\n","        mlp_layers.append(nn.Linear(in_dim, num_classes))\n","        self.mlp_head = nn.Sequential(*mlp_layers)\n","\n","    def forward(self, x):\n","        # x: (B, 3, H, W)  →  unfold → (B, patch_dim, N_patches)\n","        patches = self.unfold(x)                         # 切 patch\n","        patches = patches.transpose(1, 2)                # (B, N, patch_dim)\n","        patches = self.proj(patches)                     # (B, N, proj_dim)\n","        feat = patches.mean(dim=1)                       # (B, proj_dim)  全域平均\n","        logits = self.mlp_head(feat)                     # (B, num_classes)\n","        return logits\n"],"metadata":{"id":"w4G_OmJF-J1M"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uoGafpvlhoXh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749206226267,"user_tz":-480,"elapsed":23,"user":{"displayName":"Chaha","userId":"17365212891346415618"}},"outputId":"12786259-7a0d-4294-d253-68adb38bf66e"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-9-c8ba2027748c>:22: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n"]}],"source":["# Step 5: 初始化模型與優化器\n","#模型換成 PureMLP baseline\n","device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n","\n","model = PureMLP(\n","    num_classes=len(train_dataset.classes),\n","    patch_size=16,       # 與 PureMLP 內預設一致\n","    proj_dim=256,\n","    mlp_hidden=[512, 256],\n","    dropout=0.2\n",").to(device)\n","\n","criterion = DistancePenaltyLoss(\n","    node_distance_matrix.to(device),\n","    area_distance_matrix.to(device),\n","    node_to_area.to(device),\n","    alpha=loss_alpha,\n","    beta=loss_beta\n",")\n","\n","optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n","scaler = GradScaler()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EQtCaXoWhoXi"},"outputs":[],"source":["def train_one_epoch_amp(model, loader):\n","    model.train()\n","    total_loss, correct, total = 0, 0, 0\n","    for batch_idx, (images, labels) in enumerate(loader):\n","        images, labels = images.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        with torch.autocast(device.type if device.type != \"mps\" else \"cpu\", enabled=(device.type != \"cpu\")):\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        total_loss += loss.item()\n","        _, predicted = torch.max(outputs, 1)\n","        correct += (predicted == labels).sum().item()\n","        total += labels.size(0)\n","        # Print batch details\n","        print(f\"Batch {batch_idx+1}/{len(loader)} | Loss: {loss.item():.4f} | Batch Acc: {(predicted == labels).float().mean().item():.4f}\")\n","    acc = correct / total\n","    print(f\"Epoch Train Loss: {total_loss:.4f}, Accuracy: {acc:.4f}\")\n","\n","def evaluate(model, loader):\n","    model.eval()\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for images, labels in loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs, 1)\n","            correct += (predicted == labels).sum().item()\n","            total += labels.size(0)\n","    acc = correct / total\n","    print(f\"Validation Accuracy: {acc:.4f}\")\n","    return acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MhmyI6EhhoXj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749210097976,"user_tz":-480,"elapsed":3871686,"user":{"displayName":"Chaha","userId":"17365212891346415618"}},"outputId":"f5d96315-8727-41ee-a869-5d13d2a4b3b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Epoch 1/50\n","Batch 1/72 | Loss: 7.3525 | Batch Acc: 0.0625\n","Batch 2/72 | Loss: 7.4870 | Batch Acc: 0.1250\n","Batch 3/72 | Loss: 7.4404 | Batch Acc: 0.0000\n","Batch 4/72 | Loss: 7.3996 | Batch Acc: 0.0000\n","Batch 5/72 | Loss: 7.1873 | Batch Acc: 0.1250\n","Batch 6/72 | Loss: 7.6169 | Batch Acc: 0.0000\n","Batch 7/72 | Loss: 7.3677 | Batch Acc: 0.1250\n","Batch 8/72 | Loss: 7.0879 | Batch Acc: 0.0625\n","Batch 9/72 | Loss: 7.4478 | Batch Acc: 0.0625\n","Batch 10/72 | Loss: 7.1644 | Batch Acc: 0.1250\n","Batch 11/72 | Loss: 7.3401 | Batch Acc: 0.0000\n","Batch 12/72 | Loss: 7.3415 | Batch Acc: 0.0000\n","Batch 13/72 | Loss: 7.5210 | Batch Acc: 0.0625\n","Batch 14/72 | Loss: 7.5104 | Batch Acc: 0.0625\n","Batch 15/72 | Loss: 7.6776 | Batch Acc: 0.0000\n","Batch 16/72 | Loss: 7.4344 | Batch Acc: 0.0625\n","Batch 17/72 | Loss: 7.2264 | Batch Acc: 0.0625\n","Batch 18/72 | Loss: 7.3690 | Batch Acc: 0.0625\n","Batch 19/72 | Loss: 6.9677 | Batch Acc: 0.0000\n","Batch 20/72 | Loss: 7.6319 | Batch Acc: 0.0625\n","Batch 21/72 | Loss: 7.1979 | Batch Acc: 0.0625\n","Batch 22/72 | Loss: 7.7836 | Batch Acc: 0.0000\n","Batch 23/72 | Loss: 7.7219 | Batch Acc: 0.0000\n","Batch 24/72 | Loss: 7.5402 | Batch Acc: 0.0625\n","Batch 25/72 | Loss: 7.3876 | Batch Acc: 0.0625\n","Batch 26/72 | Loss: 7.3641 | Batch Acc: 0.0625\n","Batch 27/72 | Loss: 7.2002 | Batch Acc: 0.1250\n","Batch 28/72 | Loss: 7.4831 | Batch Acc: 0.0000\n","Batch 29/72 | Loss: 7.6301 | Batch Acc: 0.0000\n","Batch 30/72 | Loss: 7.1242 | Batch Acc: 0.1250\n","Batch 31/72 | Loss: 7.5937 | Batch Acc: 0.0000\n","Batch 32/72 | Loss: 7.3214 | Batch Acc: 0.0000\n","Batch 33/72 | Loss: 6.9903 | Batch Acc: 0.0625\n","Batch 34/72 | Loss: 7.5445 | Batch Acc: 0.0625\n","Batch 35/72 | Loss: 7.6375 | Batch Acc: 0.0625\n","Batch 36/72 | Loss: 7.2474 | Batch Acc: 0.0625\n","Batch 37/72 | Loss: 7.8633 | Batch Acc: 0.0000\n","Batch 38/72 | Loss: 7.3307 | Batch Acc: 0.0625\n","Batch 39/72 | Loss: 7.2187 | Batch Acc: 0.1250\n","Batch 40/72 | Loss: 7.7868 | Batch Acc: 0.0000\n","Batch 41/72 | Loss: 7.0456 | Batch Acc: 0.1250\n","Batch 42/72 | Loss: 6.9571 | Batch Acc: 0.0625\n","Batch 43/72 | Loss: 7.5723 | Batch Acc: 0.0000\n","Batch 44/72 | Loss: 7.1829 | Batch Acc: 0.0625\n","Batch 45/72 | Loss: 7.2544 | Batch Acc: 0.0625\n","Batch 46/72 | Loss: 7.0701 | Batch Acc: 0.0625\n","Batch 47/72 | Loss: 7.0227 | Batch Acc: 0.0625\n","Batch 48/72 | Loss: 7.6201 | Batch Acc: 0.0000\n","Batch 49/72 | Loss: 7.0191 | Batch Acc: 0.0625\n","Batch 50/72 | Loss: 7.1584 | Batch Acc: 0.0000\n","Batch 51/72 | Loss: 6.9752 | Batch Acc: 0.0000\n","Batch 52/72 | Loss: 7.4450 | Batch Acc: 0.1250\n","Batch 53/72 | Loss: 6.9090 | Batch Acc: 0.0625\n","Batch 54/72 | Loss: 7.0341 | Batch Acc: 0.0625\n","Batch 55/72 | Loss: 7.4363 | Batch Acc: 0.0000\n","Batch 56/72 | Loss: 7.2521 | Batch Acc: 0.0000\n","Batch 57/72 | Loss: 7.1128 | Batch Acc: 0.0000\n","Batch 58/72 | Loss: 7.0630 | Batch Acc: 0.0000\n","Batch 59/72 | Loss: 7.8484 | Batch Acc: 0.0000\n","Batch 60/72 | Loss: 7.3100 | Batch Acc: 0.1875\n","Batch 61/72 | Loss: 7.5489 | Batch Acc: 0.0625\n","Batch 62/72 | Loss: 6.9670 | Batch Acc: 0.1875\n","Batch 63/72 | Loss: 7.2727 | Batch Acc: 0.0625\n","Batch 64/72 | Loss: 7.6107 | Batch Acc: 0.0000\n","Batch 65/72 | Loss: 7.3450 | Batch Acc: 0.0000\n","Batch 66/72 | Loss: 7.2170 | Batch Acc: 0.1875\n","Batch 67/72 | Loss: 7.3198 | Batch Acc: 0.1250\n","Batch 68/72 | Loss: 7.3996 | Batch Acc: 0.0625\n","Batch 69/72 | Loss: 6.7295 | Batch Acc: 0.0000\n","Batch 70/72 | Loss: 6.0876 | Batch Acc: 0.1250\n","Batch 71/72 | Loss: 6.6736 | Batch Acc: 0.1875\n","Batch 72/72 | Loss: 6.9121 | Batch Acc: 0.0769\n","Epoch Train Loss: 525.9111, Accuracy: 0.0557\n","Validation Accuracy: 0.0568\n","✅ 儲存最佳模型（Acc: 0.0568）\n","\n","Epoch 2/50\n","Batch 1/72 | Loss: 7.0292 | Batch Acc: 0.0000\n","Batch 2/72 | Loss: 6.7935 | Batch Acc: 0.1250\n","Batch 3/72 | Loss: 7.4142 | Batch Acc: 0.1250\n","Batch 4/72 | Loss: 7.1329 | Batch Acc: 0.0625\n","Batch 5/72 | Loss: 7.4162 | Batch Acc: 0.0625\n","Batch 6/72 | Loss: 7.7496 | Batch Acc: 0.0000\n","Batch 7/72 | Loss: 7.6365 | Batch Acc: 0.0000\n","Batch 8/72 | Loss: 7.7641 | Batch Acc: 0.1250\n","Batch 9/72 | Loss: 7.3440 | Batch Acc: 0.0625\n","Batch 10/72 | Loss: 6.3885 | Batch Acc: 0.1250\n","Batch 11/72 | Loss: 7.6948 | Batch Acc: 0.0625\n","Batch 12/72 | Loss: 7.4124 | Batch Acc: 0.0625\n","Batch 13/72 | Loss: 6.9900 | Batch Acc: 0.0625\n","Batch 14/72 | Loss: 6.6145 | Batch Acc: 0.0625\n","Batch 15/72 | Loss: 7.1179 | Batch Acc: 0.0000\n","Batch 16/72 | Loss: 7.2294 | Batch Acc: 0.0625\n","Batch 17/72 | Loss: 6.9904 | Batch Acc: 0.0000\n","Batch 18/72 | Loss: 7.5358 | Batch Acc: 0.0625\n","Batch 19/72 | Loss: 7.3941 | Batch Acc: 0.0000\n","Batch 20/72 | Loss: 7.9064 | Batch Acc: 0.0625\n","Batch 21/72 | Loss: 6.0343 | Batch Acc: 0.1250\n","Batch 22/72 | Loss: 7.3868 | Batch Acc: 0.0000\n","Batch 23/72 | Loss: 7.0527 | Batch Acc: 0.0625\n","Batch 24/72 | Loss: 6.6546 | Batch Acc: 0.1250\n","Batch 25/72 | Loss: 6.9567 | Batch Acc: 0.0625\n","Batch 26/72 | Loss: 7.0226 | Batch Acc: 0.0625\n","Batch 27/72 | Loss: 7.4289 | Batch Acc: 0.0000\n","Batch 28/72 | Loss: 7.6559 | Batch Acc: 0.0000\n","Batch 29/72 | Loss: 7.3350 | Batch Acc: 0.1875\n","Batch 30/72 | Loss: 7.5466 | Batch Acc: 0.1875\n","Batch 31/72 | Loss: 6.5901 | Batch Acc: 0.1250\n","Batch 32/72 | Loss: 6.8879 | Batch Acc: 0.0625\n","Batch 33/72 | Loss: 7.0524 | Batch Acc: 0.1250\n","Batch 34/72 | Loss: 7.4973 | Batch Acc: 0.0625\n","Batch 35/72 | Loss: 7.2185 | Batch Acc: 0.0000\n","Batch 36/72 | Loss: 7.3879 | Batch Acc: 0.0000\n","Batch 37/72 | Loss: 8.0622 | Batch Acc: 0.0000\n","Batch 38/72 | Loss: 7.3128 | Batch Acc: 0.0000\n","Batch 39/72 | Loss: 7.0955 | Batch Acc: 0.1875\n","Batch 40/72 | Loss: 6.6507 | Batch Acc: 0.1250\n","Batch 41/72 | Loss: 7.4894 | Batch Acc: 0.0000\n","Batch 42/72 | Loss: 7.3336 | Batch Acc: 0.0000\n","Batch 43/72 | Loss: 7.6552 | Batch Acc: 0.0000\n","Batch 44/72 | Loss: 7.1996 | Batch Acc: 0.0625\n","Batch 45/72 | Loss: 7.2332 | Batch Acc: 0.0000\n","Batch 46/72 | Loss: 6.9115 | Batch Acc: 0.0625\n","Batch 47/72 | Loss: 7.5156 | Batch Acc: 0.0625\n","Batch 48/72 | Loss: 6.8957 | Batch Acc: 0.0625\n","Batch 49/72 | Loss: 6.9847 | Batch Acc: 0.1250\n","Batch 50/72 | Loss: 7.3760 | Batch Acc: 0.0000\n","Batch 51/72 | Loss: 7.2432 | Batch Acc: 0.0625\n","Batch 52/72 | Loss: 7.3638 | Batch Acc: 0.0000\n","Batch 53/72 | Loss: 7.4074 | Batch Acc: 0.0625\n","Batch 54/72 | Loss: 6.8481 | Batch Acc: 0.0625\n","Batch 55/72 | Loss: 6.1133 | Batch Acc: 0.1250\n","Batch 56/72 | Loss: 6.7515 | Batch Acc: 0.0625\n","Batch 57/72 | Loss: 7.1848 | Batch Acc: 0.0625\n","Batch 58/72 | Loss: 7.1530 | Batch Acc: 0.0000\n","Batch 59/72 | Loss: 6.6710 | Batch Acc: 0.0625\n","Batch 60/72 | Loss: 7.1950 | Batch Acc: 0.0000\n","Batch 61/72 | Loss: 6.2716 | Batch Acc: 0.0000\n","Batch 62/72 | Loss: 7.6798 | Batch Acc: 0.0000\n","Batch 63/72 | Loss: 7.2136 | Batch Acc: 0.0000\n","Batch 64/72 | Loss: 7.0552 | Batch Acc: 0.0625\n","Batch 65/72 | Loss: 6.8595 | Batch Acc: 0.0625\n","Batch 66/72 | Loss: 6.7427 | Batch Acc: 0.0625\n","Batch 67/72 | Loss: 7.4269 | Batch Acc: 0.0625\n","Batch 68/72 | Loss: 7.5705 | Batch Acc: 0.0625\n","Batch 69/72 | Loss: 7.1492 | Batch Acc: 0.0625\n","Batch 70/72 | Loss: 7.3791 | Batch Acc: 0.0625\n","Batch 71/72 | Loss: 6.8669 | Batch Acc: 0.0000\n","Batch 72/72 | Loss: 6.9063 | Batch Acc: 0.0000\n","Epoch Train Loss: 516.0307, Accuracy: 0.0548\n","Validation Accuracy: 0.0852\n","✅ 儲存最佳模型（Acc: 0.0852）\n","\n","Epoch 3/50\n","Batch 1/72 | Loss: 6.5630 | Batch Acc: 0.1875\n","Batch 2/72 | Loss: 7.1692 | Batch Acc: 0.0625\n","Batch 3/72 | Loss: 6.7498 | Batch Acc: 0.1250\n","Batch 4/72 | Loss: 7.3541 | Batch Acc: 0.0000\n","Batch 5/72 | Loss: 6.6035 | Batch Acc: 0.1250\n","Batch 6/72 | Loss: 6.5044 | Batch Acc: 0.0625\n","Batch 7/72 | Loss: 6.7962 | Batch Acc: 0.0000\n","Batch 8/72 | Loss: 6.9341 | Batch Acc: 0.1250\n","Batch 9/72 | Loss: 6.9233 | Batch Acc: 0.1250\n","Batch 10/72 | Loss: 7.8418 | Batch Acc: 0.0625\n","Batch 11/72 | Loss: 6.9965 | Batch Acc: 0.1250\n","Batch 12/72 | Loss: 7.4402 | Batch Acc: 0.1875\n","Batch 13/72 | Loss: 6.7835 | Batch Acc: 0.1250\n","Batch 14/72 | Loss: 8.1332 | Batch Acc: 0.1250\n","Batch 15/72 | Loss: 7.2157 | Batch Acc: 0.1250\n","Batch 16/72 | Loss: 7.0684 | Batch Acc: 0.0625\n","Batch 17/72 | Loss: 6.5799 | Batch Acc: 0.1250\n","Batch 18/72 | Loss: 7.0835 | Batch Acc: 0.0625\n","Batch 19/72 | Loss: 7.3404 | Batch Acc: 0.0625\n","Batch 20/72 | Loss: 6.9273 | Batch Acc: 0.1250\n","Batch 21/72 | Loss: 7.3214 | Batch Acc: 0.0625\n","Batch 22/72 | Loss: 6.7975 | Batch Acc: 0.1875\n","Batch 23/72 | Loss: 6.9191 | Batch Acc: 0.0625\n","Batch 24/72 | Loss: 6.9015 | Batch Acc: 0.0000\n","Batch 25/72 | Loss: 7.2574 | Batch Acc: 0.0625\n","Batch 26/72 | Loss: 7.3541 | Batch Acc: 0.0625\n","Batch 27/72 | Loss: 6.9458 | Batch Acc: 0.1250\n","Batch 28/72 | Loss: 6.5856 | Batch Acc: 0.0625\n","Batch 29/72 | Loss: 6.2116 | Batch Acc: 0.1250\n","Batch 30/72 | Loss: 6.1063 | Batch Acc: 0.2500\n","Batch 31/72 | Loss: 7.1598 | Batch Acc: 0.0000\n","Batch 32/72 | Loss: 6.3759 | Batch Acc: 0.1250\n","Batch 33/72 | Loss: 6.8801 | Batch Acc: 0.0000\n","Batch 34/72 | Loss: 6.3429 | Batch Acc: 0.0625\n","Batch 35/72 | Loss: 6.8965 | Batch Acc: 0.0625\n","Batch 36/72 | Loss: 8.1557 | Batch Acc: 0.0000\n","Batch 37/72 | Loss: 6.7778 | Batch Acc: 0.1875\n","Batch 38/72 | Loss: 7.4071 | Batch Acc: 0.0625\n","Batch 39/72 | Loss: 7.7221 | Batch Acc: 0.0625\n","Batch 40/72 | Loss: 7.0895 | Batch Acc: 0.0625\n","Batch 41/72 | Loss: 7.0222 | Batch Acc: 0.0000\n","Batch 42/72 | Loss: 7.2059 | Batch Acc: 0.0000\n","Batch 43/72 | Loss: 7.0323 | Batch Acc: 0.1875\n","Batch 44/72 | Loss: 7.0131 | Batch Acc: 0.0000\n","Batch 45/72 | Loss: 7.6332 | Batch Acc: 0.1250\n","Batch 46/72 | Loss: 6.2997 | Batch Acc: 0.0625\n","Batch 47/72 | Loss: 8.0639 | Batch Acc: 0.0000\n","Batch 48/72 | Loss: 7.4846 | Batch Acc: 0.1250\n","Batch 49/72 | Loss: 7.4752 | Batch Acc: 0.1875\n","Batch 50/72 | Loss: 7.4398 | Batch Acc: 0.1875\n","Batch 51/72 | Loss: 6.9670 | Batch Acc: 0.1250\n","Batch 52/72 | Loss: 7.6155 | Batch Acc: 0.0000\n","Batch 53/72 | Loss: 7.0091 | Batch Acc: 0.1875\n","Batch 54/72 | Loss: 7.6764 | Batch Acc: 0.0625\n","Batch 55/72 | Loss: 7.4421 | Batch Acc: 0.0000\n","Batch 56/72 | Loss: 6.7785 | Batch Acc: 0.0625\n","Batch 57/72 | Loss: 7.3706 | Batch Acc: 0.1875\n","Batch 58/72 | Loss: 6.5761 | Batch Acc: 0.0625\n","Batch 59/72 | Loss: 7.2227 | Batch Acc: 0.0000\n","Batch 60/72 | Loss: 7.6940 | Batch Acc: 0.0625\n","Batch 61/72 | Loss: 6.8169 | Batch Acc: 0.0625\n","Batch 62/72 | Loss: 6.5863 | Batch Acc: 0.0625\n","Batch 63/72 | Loss: 7.6859 | Batch Acc: 0.1250\n","Batch 64/72 | Loss: 7.0528 | Batch Acc: 0.1250\n","Batch 65/72 | Loss: 7.1072 | Batch Acc: 0.0000\n","Batch 66/72 | Loss: 7.4664 | Batch Acc: 0.0000\n","Batch 67/72 | Loss: 6.6370 | Batch Acc: 0.0625\n","Batch 68/72 | Loss: 7.8334 | Batch Acc: 0.0000\n","Batch 69/72 | Loss: 6.2013 | Batch Acc: 0.1875\n","Batch 70/72 | Loss: 7.9680 | Batch Acc: 0.0000\n","Batch 71/72 | Loss: 7.3453 | Batch Acc: 0.0000\n","Batch 72/72 | Loss: 7.3740 | Batch Acc: 0.0769\n","Epoch Train Loss: 511.3121, Accuracy: 0.0827\n","Validation Accuracy: 0.0852\n","\n","Epoch 4/50\n","Batch 1/72 | Loss: 7.5877 | Batch Acc: 0.0000\n","Batch 2/72 | Loss: 7.0152 | Batch Acc: 0.0000\n","Batch 3/72 | Loss: 6.8964 | Batch Acc: 0.1250\n","Batch 4/72 | Loss: 6.5543 | Batch Acc: 0.0625\n","Batch 5/72 | Loss: 6.6642 | Batch Acc: 0.1250\n","Batch 6/72 | Loss: 7.2400 | Batch Acc: 0.0625\n","Batch 7/72 | Loss: 7.2288 | Batch Acc: 0.1250\n","Batch 8/72 | Loss: 6.3982 | Batch Acc: 0.1250\n","Batch 9/72 | Loss: 6.8416 | Batch Acc: 0.0000\n","Batch 10/72 | Loss: 7.2659 | Batch Acc: 0.0625\n","Batch 11/72 | Loss: 6.6878 | Batch Acc: 0.1250\n","Batch 12/72 | Loss: 6.4443 | Batch Acc: 0.1250\n","Batch 13/72 | Loss: 7.9243 | Batch Acc: 0.0625\n","Batch 14/72 | Loss: 6.8701 | Batch Acc: 0.1250\n","Batch 15/72 | Loss: 7.2571 | Batch Acc: 0.0625\n","Batch 16/72 | Loss: 6.6676 | Batch Acc: 0.0625\n","Batch 17/72 | Loss: 7.2744 | Batch Acc: 0.0000\n","Batch 18/72 | Loss: 6.7723 | Batch Acc: 0.0000\n","Batch 19/72 | Loss: 6.5363 | Batch Acc: 0.1250\n","Batch 20/72 | Loss: 6.9696 | Batch Acc: 0.1875\n","Batch 21/72 | Loss: 7.4192 | Batch Acc: 0.0000\n","Batch 22/72 | Loss: 6.6873 | Batch Acc: 0.0625\n","Batch 23/72 | Loss: 6.0758 | Batch Acc: 0.1250\n","Batch 24/72 | Loss: 8.0557 | Batch Acc: 0.0000\n","Batch 25/72 | Loss: 7.6312 | Batch Acc: 0.0625\n","Batch 26/72 | Loss: 6.4239 | Batch Acc: 0.0000\n","Batch 27/72 | Loss: 7.0297 | Batch Acc: 0.1250\n","Batch 28/72 | Loss: 7.3190 | Batch Acc: 0.1250\n","Batch 29/72 | Loss: 7.8705 | Batch Acc: 0.0625\n","Batch 30/72 | Loss: 7.8365 | Batch Acc: 0.1250\n","Batch 31/72 | Loss: 8.0600 | Batch Acc: 0.0000\n","Batch 32/72 | Loss: 7.3444 | Batch Acc: 0.0625\n","Batch 33/72 | Loss: 6.4166 | Batch Acc: 0.0000\n","Batch 34/72 | Loss: 6.6555 | Batch Acc: 0.0625\n","Batch 35/72 | Loss: 7.6321 | Batch Acc: 0.0625\n","Batch 36/72 | Loss: 6.6934 | Batch Acc: 0.1250\n","Batch 37/72 | Loss: 7.2547 | Batch Acc: 0.0625\n","Batch 38/72 | Loss: 6.6143 | Batch Acc: 0.1250\n","Batch 39/72 | Loss: 7.3814 | Batch Acc: 0.0000\n","Batch 40/72 | Loss: 6.4456 | Batch Acc: 0.0625\n","Batch 41/72 | Loss: 7.1047 | Batch Acc: 0.0625\n","Batch 42/72 | Loss: 8.1002 | Batch Acc: 0.1250\n","Batch 43/72 | Loss: 7.7755 | Batch Acc: 0.0625\n","Batch 44/72 | Loss: 6.5452 | Batch Acc: 0.1250\n","Batch 45/72 | Loss: 6.7279 | Batch Acc: 0.0625\n","Batch 46/72 | Loss: 6.3865 | Batch Acc: 0.2500\n","Batch 47/72 | Loss: 7.5996 | Batch Acc: 0.0625\n","Batch 48/72 | Loss: 7.2136 | Batch Acc: 0.0625\n","Batch 49/72 | Loss: 7.0650 | Batch Acc: 0.0000\n","Batch 50/72 | Loss: 6.7705 | Batch Acc: 0.0625\n","Batch 51/72 | Loss: 6.9701 | Batch Acc: 0.1250\n","Batch 52/72 | Loss: 7.0032 | Batch Acc: 0.0625\n","Batch 53/72 | Loss: 7.0535 | Batch Acc: 0.0000\n","Batch 54/72 | Loss: 6.9168 | Batch Acc: 0.1250\n","Batch 55/72 | Loss: 7.3689 | Batch Acc: 0.0000\n","Batch 56/72 | Loss: 6.5191 | Batch Acc: 0.0625\n","Batch 57/72 | Loss: 7.4220 | Batch Acc: 0.0625\n","Batch 58/72 | Loss: 6.8770 | Batch Acc: 0.0625\n","Batch 59/72 | Loss: 6.7217 | Batch Acc: 0.1250\n","Batch 60/72 | Loss: 7.4638 | Batch Acc: 0.0625\n","Batch 61/72 | Loss: 7.2057 | Batch Acc: 0.1875\n","Batch 62/72 | Loss: 7.5668 | Batch Acc: 0.0625\n","Batch 63/72 | Loss: 7.1591 | Batch Acc: 0.0625\n","Batch 64/72 | Loss: 7.7152 | Batch Acc: 0.0000\n","Batch 65/72 | Loss: 7.4336 | Batch Acc: 0.0000\n","Batch 66/72 | Loss: 7.3814 | Batch Acc: 0.0625\n","Batch 67/72 | Loss: 6.1715 | Batch Acc: 0.1875\n","Batch 68/72 | Loss: 7.1349 | Batch Acc: 0.0000\n","Batch 69/72 | Loss: 6.9931 | Batch Acc: 0.1250\n","Batch 70/72 | Loss: 7.1120 | Batch Acc: 0.0625\n","Batch 71/72 | Loss: 6.6129 | Batch Acc: 0.0625\n","Batch 72/72 | Loss: 7.1722 | Batch Acc: 0.0769\n","Epoch Train Loss: 509.2059, Accuracy: 0.0731\n","Validation Accuracy: 0.1080\n","✅ 儲存最佳模型（Acc: 0.1080）\n","\n","Epoch 5/50\n","Batch 1/72 | Loss: 7.3199 | Batch Acc: 0.0000\n","Batch 2/72 | Loss: 6.8602 | Batch Acc: 0.1250\n","Batch 3/72 | Loss: 6.3004 | Batch Acc: 0.1875\n","Batch 4/72 | Loss: 6.8118 | Batch Acc: 0.0625\n","Batch 5/72 | Loss: 6.5761 | Batch Acc: 0.0625\n","Batch 6/72 | Loss: 7.3220 | Batch Acc: 0.0625\n","Batch 7/72 | Loss: 7.5772 | Batch Acc: 0.0625\n","Batch 8/72 | Loss: 6.3720 | Batch Acc: 0.2500\n","Batch 9/72 | Loss: 8.1044 | Batch Acc: 0.0625\n","Batch 10/72 | Loss: 6.4849 | Batch Acc: 0.1875\n","Batch 11/72 | Loss: 7.2629 | Batch Acc: 0.0625\n","Batch 12/72 | Loss: 7.0689 | Batch Acc: 0.1250\n","Batch 13/72 | Loss: 6.3156 | Batch Acc: 0.0625\n","Batch 14/72 | Loss: 7.0776 | Batch Acc: 0.0000\n","Batch 15/72 | Loss: 7.3828 | Batch Acc: 0.0625\n","Batch 16/72 | Loss: 6.5329 | Batch Acc: 0.1250\n","Batch 17/72 | Loss: 7.0168 | Batch Acc: 0.1250\n","Batch 18/72 | Loss: 6.4902 | Batch Acc: 0.0000\n","Batch 19/72 | Loss: 6.6941 | Batch Acc: 0.0000\n","Batch 20/72 | Loss: 7.5239 | Batch Acc: 0.0625\n","Batch 21/72 | Loss: 7.0971 | Batch Acc: 0.0625\n","Batch 22/72 | Loss: 7.2880 | Batch Acc: 0.0000\n","Batch 23/72 | Loss: 6.1956 | Batch Acc: 0.1250\n","Batch 24/72 | Loss: 6.5814 | Batch Acc: 0.0625\n","Batch 25/72 | Loss: 5.9329 | Batch Acc: 0.3125\n","Batch 26/72 | Loss: 7.4076 | Batch Acc: 0.0625\n","Batch 27/72 | Loss: 7.0346 | Batch Acc: 0.0000\n","Batch 28/72 | Loss: 6.9374 | Batch Acc: 0.0000\n","Batch 29/72 | Loss: 7.1989 | Batch Acc: 0.0625\n","Batch 30/72 | Loss: 7.2418 | Batch Acc: 0.0625\n","Batch 31/72 | Loss: 6.8959 | Batch Acc: 0.0625\n","Batch 32/72 | Loss: 6.9797 | Batch Acc: 0.0625\n","Batch 33/72 | Loss: 7.2040 | Batch Acc: 0.0625\n","Batch 34/72 | Loss: 7.2126 | Batch Acc: 0.0625\n","Batch 35/72 | Loss: 8.0589 | Batch Acc: 0.1250\n","Batch 36/72 | Loss: 6.8287 | Batch Acc: 0.0625\n","Batch 37/72 | Loss: 6.9522 | Batch Acc: 0.0625\n","Batch 38/72 | Loss: 6.8571 | Batch Acc: 0.0625\n","Batch 39/72 | Loss: 7.7947 | Batch Acc: 0.0000\n","Batch 40/72 | Loss: 7.4230 | Batch Acc: 0.1250\n","Batch 41/72 | Loss: 6.9854 | Batch Acc: 0.1875\n","Batch 42/72 | Loss: 7.2796 | Batch Acc: 0.0625\n","Batch 43/72 | Loss: 6.9621 | Batch Acc: 0.1875\n","Batch 44/72 | Loss: 6.3214 | Batch Acc: 0.0625\n","Batch 45/72 | Loss: 7.0056 | Batch Acc: 0.0625\n","Batch 46/72 | Loss: 7.4169 | Batch Acc: 0.0625\n","Batch 47/72 | Loss: 6.8450 | Batch Acc: 0.0625\n","Batch 48/72 | Loss: 6.6899 | Batch Acc: 0.1250\n","Batch 49/72 | Loss: 6.9992 | Batch Acc: 0.1875\n","Batch 50/72 | Loss: 6.6484 | Batch Acc: 0.0625\n","Batch 51/72 | Loss: 6.4341 | Batch Acc: 0.2500\n","Batch 52/72 | Loss: 6.9845 | Batch Acc: 0.1875\n","Batch 53/72 | Loss: 6.4318 | Batch Acc: 0.1875\n","Batch 54/72 | Loss: 6.6975 | Batch Acc: 0.0625\n","Batch 55/72 | Loss: 6.8249 | Batch Acc: 0.1250\n","Batch 56/72 | Loss: 7.0759 | Batch Acc: 0.0625\n","Batch 57/72 | Loss: 7.3734 | Batch Acc: 0.0000\n","Batch 58/72 | Loss: 7.6916 | Batch Acc: 0.0000\n","Batch 59/72 | Loss: 6.1070 | Batch Acc: 0.1875\n","Batch 60/72 | Loss: 6.7890 | Batch Acc: 0.0000\n","Batch 61/72 | Loss: 7.2250 | Batch Acc: 0.0000\n","Batch 62/72 | Loss: 7.4828 | Batch Acc: 0.0625\n","Batch 63/72 | Loss: 6.3702 | Batch Acc: 0.1250\n","Batch 64/72 | Loss: 6.2485 | Batch Acc: 0.1875\n","Batch 65/72 | Loss: 7.8627 | Batch Acc: 0.0000\n","Batch 66/72 | Loss: 7.4266 | Batch Acc: 0.0000\n","Batch 67/72 | Loss: 8.0106 | Batch Acc: 0.0000\n","Batch 68/72 | Loss: 7.0650 | Batch Acc: 0.1250\n","Batch 69/72 | Loss: 7.4987 | Batch Acc: 0.0625\n","Batch 70/72 | Loss: 6.4328 | Batch Acc: 0.1250\n","Batch 71/72 | Loss: 6.8209 | Batch Acc: 0.0625\n","Batch 72/72 | Loss: 7.0877 | Batch Acc: 0.1538\n","Epoch Train Loss: 503.3115, Accuracy: 0.0853\n","Validation Accuracy: 0.0966\n","\n","Epoch 6/50\n","Batch 1/72 | Loss: 7.1766 | Batch Acc: 0.0625\n","Batch 2/72 | Loss: 6.5704 | Batch Acc: 0.1250\n","Batch 3/72 | Loss: 6.8320 | Batch Acc: 0.1250\n","Batch 4/72 | Loss: 6.6492 | Batch Acc: 0.1250\n","Batch 5/72 | Loss: 7.1726 | Batch Acc: 0.0000\n","Batch 6/72 | Loss: 8.3917 | Batch Acc: 0.0000\n","Batch 7/72 | Loss: 6.6128 | Batch Acc: 0.1250\n","Batch 8/72 | Loss: 6.6693 | Batch Acc: 0.0625\n","Batch 9/72 | Loss: 6.2897 | Batch Acc: 0.2500\n","Batch 10/72 | Loss: 6.9154 | Batch Acc: 0.0625\n","Batch 11/72 | Loss: 6.8712 | Batch Acc: 0.0625\n","Batch 12/72 | Loss: 5.9343 | Batch Acc: 0.1250\n","Batch 13/72 | Loss: 7.2156 | Batch Acc: 0.0625\n","Batch 14/72 | Loss: 6.8507 | Batch Acc: 0.0625\n","Batch 15/72 | Loss: 6.7830 | Batch Acc: 0.0000\n","Batch 16/72 | Loss: 6.3931 | Batch Acc: 0.1875\n","Batch 17/72 | Loss: 6.8616 | Batch Acc: 0.0000\n","Batch 18/72 | Loss: 6.7473 | Batch Acc: 0.0625\n","Batch 19/72 | Loss: 6.8376 | Batch Acc: 0.0625\n","Batch 20/72 | Loss: 6.8981 | Batch Acc: 0.0625\n","Batch 21/72 | Loss: 6.6822 | Batch Acc: 0.0000\n","Batch 22/72 | Loss: 6.5352 | Batch Acc: 0.0625\n","Batch 23/72 | Loss: 7.7773 | Batch Acc: 0.0625\n","Batch 24/72 | Loss: 7.0031 | Batch Acc: 0.0000\n","Batch 25/72 | Loss: 6.1488 | Batch Acc: 0.0625\n","Batch 26/72 | Loss: 7.0279 | Batch Acc: 0.1875\n","Batch 27/72 | Loss: 6.9521 | Batch Acc: 0.0625\n","Batch 28/72 | Loss: 7.2473 | Batch Acc: 0.0625\n","Batch 29/72 | Loss: 7.3153 | Batch Acc: 0.0000\n","Batch 30/72 | Loss: 6.1516 | Batch Acc: 0.0000\n","Batch 31/72 | Loss: 7.0413 | Batch Acc: 0.1250\n","Batch 32/72 | Loss: 6.8955 | Batch Acc: 0.1875\n","Batch 33/72 | Loss: 6.3378 | Batch Acc: 0.1875\n","Batch 34/72 | Loss: 6.6126 | Batch Acc: 0.1250\n","Batch 35/72 | Loss: 6.4693 | Batch Acc: 0.1250\n","Batch 36/72 | Loss: 7.6745 | Batch Acc: 0.1250\n","Batch 37/72 | Loss: 7.0596 | Batch Acc: 0.0625\n","Batch 38/72 | Loss: 7.2597 | Batch Acc: 0.0625\n","Batch 39/72 | Loss: 7.7543 | Batch Acc: 0.0625\n","Batch 40/72 | Loss: 6.3644 | Batch Acc: 0.0625\n","Batch 41/72 | Loss: 5.7587 | Batch Acc: 0.1250\n","Batch 42/72 | Loss: 7.2998 | Batch Acc: 0.0625\n","Batch 43/72 | Loss: 7.6224 | Batch Acc: 0.0625\n","Batch 44/72 | Loss: 6.2959 | Batch Acc: 0.1250\n","Batch 45/72 | Loss: 6.6961 | Batch Acc: 0.0625\n","Batch 46/72 | Loss: 6.6906 | Batch Acc: 0.1875\n","Batch 47/72 | Loss: 6.6124 | Batch Acc: 0.0625\n","Batch 48/72 | Loss: 6.3616 | Batch Acc: 0.1875\n","Batch 49/72 | Loss: 6.9888 | Batch Acc: 0.0625\n","Batch 50/72 | Loss: 7.1616 | Batch Acc: 0.1875\n","Batch 51/72 | Loss: 8.0567 | Batch Acc: 0.0625\n","Batch 52/72 | Loss: 5.9710 | Batch Acc: 0.1875\n","Batch 53/72 | Loss: 6.4310 | Batch Acc: 0.1875\n","Batch 54/72 | Loss: 7.5320 | Batch Acc: 0.0000\n","Batch 55/72 | Loss: 6.6507 | Batch Acc: 0.0000\n","Batch 56/72 | Loss: 5.6338 | Batch Acc: 0.2500\n","Batch 57/72 | Loss: 6.4962 | Batch Acc: 0.1250\n","Batch 58/72 | Loss: 6.7920 | Batch Acc: 0.1250\n","Batch 59/72 | Loss: 7.3426 | Batch Acc: 0.0625\n","Batch 60/72 | Loss: 6.5537 | Batch Acc: 0.0625\n","Batch 61/72 | Loss: 6.5624 | Batch Acc: 0.0625\n","Batch 62/72 | Loss: 6.7728 | Batch Acc: 0.1875\n","Batch 63/72 | Loss: 6.9832 | Batch Acc: 0.1250\n","Batch 64/72 | Loss: 6.1903 | Batch Acc: 0.0625\n","Batch 65/72 | Loss: 7.4151 | Batch Acc: 0.0625\n","Batch 66/72 | Loss: 7.0535 | Batch Acc: 0.1250\n","Batch 67/72 | Loss: 6.9403 | Batch Acc: 0.0625\n","Batch 68/72 | Loss: 6.7469 | Batch Acc: 0.0000\n","Batch 69/72 | Loss: 7.1344 | Batch Acc: 0.0625\n","Batch 70/72 | Loss: 7.3224 | Batch Acc: 0.1250\n","Batch 71/72 | Loss: 6.8103 | Batch Acc: 0.0000\n","Batch 72/72 | Loss: 7.3952 | Batch Acc: 0.0769\n","Epoch Train Loss: 493.2560, Accuracy: 0.0888\n","Validation Accuracy: 0.1080\n","\n","Epoch 7/50\n","Batch 1/72 | Loss: 5.9772 | Batch Acc: 0.1875\n","Batch 2/72 | Loss: 7.1077 | Batch Acc: 0.0000\n","Batch 3/72 | Loss: 6.9623 | Batch Acc: 0.0000\n","Batch 4/72 | Loss: 6.2680 | Batch Acc: 0.1875\n","Batch 5/72 | Loss: 7.1214 | Batch Acc: 0.0625\n","Batch 6/72 | Loss: 7.0505 | Batch Acc: 0.1250\n","Batch 7/72 | Loss: 7.4245 | Batch Acc: 0.0625\n","Batch 8/72 | Loss: 7.2290 | Batch Acc: 0.0000\n","Batch 9/72 | Loss: 6.9980 | Batch Acc: 0.0000\n","Batch 10/72 | Loss: 6.1489 | Batch Acc: 0.0625\n","Batch 11/72 | Loss: 6.9369 | Batch Acc: 0.1250\n","Batch 12/72 | Loss: 6.5323 | Batch Acc: 0.1250\n","Batch 13/72 | Loss: 6.7560 | Batch Acc: 0.0625\n","Batch 14/72 | Loss: 7.4547 | Batch Acc: 0.0625\n","Batch 15/72 | Loss: 6.6318 | Batch Acc: 0.1875\n","Batch 16/72 | Loss: 6.7418 | Batch Acc: 0.2500\n","Batch 17/72 | Loss: 7.2716 | Batch Acc: 0.0000\n","Batch 18/72 | Loss: 6.2848 | Batch Acc: 0.1875\n","Batch 19/72 | Loss: 6.4597 | Batch Acc: 0.1875\n","Batch 20/72 | Loss: 6.7701 | Batch Acc: 0.1250\n","Batch 21/72 | Loss: 6.8373 | Batch Acc: 0.1250\n","Batch 22/72 | Loss: 7.3723 | Batch Acc: 0.0000\n","Batch 23/72 | Loss: 6.9874 | Batch Acc: 0.0625\n","Batch 24/72 | Loss: 7.4959 | Batch Acc: 0.1250\n","Batch 25/72 | Loss: 7.0293 | Batch Acc: 0.1250\n","Batch 26/72 | Loss: 6.3909 | Batch Acc: 0.1875\n","Batch 27/72 | Loss: 5.9865 | Batch Acc: 0.1875\n","Batch 28/72 | Loss: 6.7125 | Batch Acc: 0.0000\n","Batch 29/72 | Loss: 6.5130 | Batch Acc: 0.0625\n","Batch 30/72 | Loss: 7.2226 | Batch Acc: 0.0625\n","Batch 31/72 | Loss: 6.3319 | Batch Acc: 0.0625\n","Batch 32/72 | Loss: 7.5432 | Batch Acc: 0.0625\n","Batch 33/72 | Loss: 7.6170 | Batch Acc: 0.0625\n","Batch 34/72 | Loss: 6.7607 | Batch Acc: 0.0000\n","Batch 35/72 | Loss: 6.5932 | Batch Acc: 0.1250\n","Batch 36/72 | Loss: 6.5290 | Batch Acc: 0.0625\n","Batch 37/72 | Loss: 6.5853 | Batch Acc: 0.0625\n","Batch 38/72 | Loss: 6.8061 | Batch Acc: 0.0625\n","Batch 39/72 | Loss: 6.3880 | Batch Acc: 0.1250\n","Batch 40/72 | Loss: 6.5575 | Batch Acc: 0.1250\n","Batch 41/72 | Loss: 6.2620 | Batch Acc: 0.0625\n","Batch 42/72 | Loss: 6.1246 | Batch Acc: 0.1250\n","Batch 43/72 | Loss: 6.9820 | Batch Acc: 0.0625\n","Batch 44/72 | Loss: 6.7346 | Batch Acc: 0.0000\n","Batch 45/72 | Loss: 7.2772 | Batch Acc: 0.1250\n","Batch 46/72 | Loss: 5.9543 | Batch Acc: 0.1250\n","Batch 47/72 | Loss: 6.7023 | Batch Acc: 0.1250\n","Batch 48/72 | Loss: 6.8035 | Batch Acc: 0.0625\n","Batch 49/72 | Loss: 6.9121 | Batch Acc: 0.0000\n","Batch 50/72 | Loss: 7.2721 | Batch Acc: 0.1250\n","Batch 51/72 | Loss: 6.4883 | Batch Acc: 0.1875\n","Batch 52/72 | Loss: 6.7464 | Batch Acc: 0.0625\n","Batch 53/72 | Loss: 7.2434 | Batch Acc: 0.0625\n","Batch 54/72 | Loss: 7.7948 | Batch Acc: 0.1250\n","Batch 55/72 | Loss: 6.9647 | Batch Acc: 0.1875\n","Batch 56/72 | Loss: 6.4328 | Batch Acc: 0.0625\n","Batch 57/72 | Loss: 7.4846 | Batch Acc: 0.0625\n","Batch 58/72 | Loss: 6.3676 | Batch Acc: 0.1250\n","Batch 59/72 | Loss: 6.9890 | Batch Acc: 0.0625\n","Batch 60/72 | Loss: 7.2583 | Batch Acc: 0.0000\n","Batch 61/72 | Loss: 6.3419 | Batch Acc: 0.3125\n","Batch 62/72 | Loss: 7.0397 | Batch Acc: 0.0000\n","Batch 63/72 | Loss: 6.7738 | Batch Acc: 0.1250\n","Batch 64/72 | Loss: 7.2161 | Batch Acc: 0.0625\n","Batch 65/72 | Loss: 7.0975 | Batch Acc: 0.0000\n","Batch 66/72 | Loss: 6.2219 | Batch Acc: 0.0625\n","Batch 67/72 | Loss: 6.7099 | Batch Acc: 0.1250\n","Batch 68/72 | Loss: 6.4573 | Batch Acc: 0.0625\n","Batch 69/72 | Loss: 6.8507 | Batch Acc: 0.2500\n","Batch 70/72 | Loss: 7.1592 | Batch Acc: 0.0625\n","Batch 71/72 | Loss: 6.6569 | Batch Acc: 0.0625\n","Batch 72/72 | Loss: 6.0430 | Batch Acc: 0.2308\n","Epoch Train Loss: 489.7492, Accuracy: 0.0940\n","Validation Accuracy: 0.1023\n","\n","Epoch 8/50\n","Batch 1/72 | Loss: 7.1246 | Batch Acc: 0.0625\n","Batch 2/72 | Loss: 6.6525 | Batch Acc: 0.0625\n","Batch 3/72 | Loss: 6.8639 | Batch Acc: 0.1250\n","Batch 4/72 | Loss: 6.3474 | Batch Acc: 0.0625\n","Batch 5/72 | Loss: 6.4458 | Batch Acc: 0.2500\n","Batch 6/72 | Loss: 7.0782 | Batch Acc: 0.1875\n","Batch 7/72 | Loss: 7.1523 | Batch Acc: 0.0625\n","Batch 8/72 | Loss: 6.5435 | Batch Acc: 0.0000\n","Batch 9/72 | Loss: 6.3216 | Batch Acc: 0.1875\n","Batch 10/72 | Loss: 6.4725 | Batch Acc: 0.1875\n","Batch 11/72 | Loss: 6.9502 | Batch Acc: 0.1875\n","Batch 12/72 | Loss: 5.8959 | Batch Acc: 0.2500\n","Batch 13/72 | Loss: 6.7085 | Batch Acc: 0.2500\n","Batch 14/72 | Loss: 6.3215 | Batch Acc: 0.1250\n","Batch 15/72 | Loss: 5.9612 | Batch Acc: 0.1250\n","Batch 16/72 | Loss: 6.3139 | Batch Acc: 0.1250\n","Batch 17/72 | Loss: 6.4146 | Batch Acc: 0.0625\n","Batch 18/72 | Loss: 6.1509 | Batch Acc: 0.1250\n","Batch 19/72 | Loss: 7.1397 | Batch Acc: 0.0625\n","Batch 20/72 | Loss: 6.9792 | Batch Acc: 0.0000\n","Batch 21/72 | Loss: 5.4091 | Batch Acc: 0.1250\n","Batch 22/72 | Loss: 6.4578 | Batch Acc: 0.0625\n","Batch 23/72 | Loss: 7.6948 | Batch Acc: 0.0625\n","Batch 24/72 | Loss: 6.3974 | Batch Acc: 0.1875\n","Batch 25/72 | Loss: 5.7046 | Batch Acc: 0.2500\n","Batch 26/72 | Loss: 6.5223 | Batch Acc: 0.2500\n","Batch 27/72 | Loss: 7.0140 | Batch Acc: 0.0000\n","Batch 28/72 | Loss: 6.5850 | Batch Acc: 0.1875\n","Batch 29/72 | Loss: 6.0950 | Batch Acc: 0.1250\n","Batch 30/72 | Loss: 7.0702 | Batch Acc: 0.1250\n","Batch 31/72 | Loss: 6.6510 | Batch Acc: 0.0625\n","Batch 32/72 | Loss: 6.8707 | Batch Acc: 0.0625\n","Batch 33/72 | Loss: 6.7386 | Batch Acc: 0.0000\n","Batch 34/72 | Loss: 6.6467 | Batch Acc: 0.0625\n","Batch 35/72 | Loss: 6.7548 | Batch Acc: 0.1250\n","Batch 36/72 | Loss: 6.4560 | Batch Acc: 0.0000\n","Batch 37/72 | Loss: 6.0010 | Batch Acc: 0.1250\n","Batch 38/72 | Loss: 7.0945 | Batch Acc: 0.0625\n","Batch 39/72 | Loss: 6.5120 | Batch Acc: 0.1250\n","Batch 40/72 | Loss: 6.6098 | Batch Acc: 0.1250\n","Batch 41/72 | Loss: 6.0708 | Batch Acc: 0.0625\n","Batch 42/72 | Loss: 6.0874 | Batch Acc: 0.0625\n","Batch 43/72 | Loss: 6.6058 | Batch Acc: 0.0625\n","Batch 44/72 | Loss: 7.0248 | Batch Acc: 0.1250\n","Batch 45/72 | Loss: 6.4176 | Batch Acc: 0.1250\n","Batch 46/72 | Loss: 6.8072 | Batch Acc: 0.0625\n","Batch 47/72 | Loss: 6.6027 | Batch Acc: 0.1250\n","Batch 48/72 | Loss: 6.3325 | Batch Acc: 0.0625\n","Batch 49/72 | Loss: 6.7813 | Batch Acc: 0.0625\n","Batch 50/72 | Loss: 5.9904 | Batch Acc: 0.0000\n","Batch 51/72 | Loss: 6.7938 | Batch Acc: 0.0625\n","Batch 52/72 | Loss: 7.5746 | Batch Acc: 0.0000\n","Batch 53/72 | Loss: 6.1012 | Batch Acc: 0.0625\n","Batch 54/72 | Loss: 5.8669 | Batch Acc: 0.1875\n","Batch 55/72 | Loss: 7.3669 | Batch Acc: 0.0625\n","Batch 56/72 | Loss: 7.1824 | Batch Acc: 0.0625\n","Batch 57/72 | Loss: 7.5007 | Batch Acc: 0.0625\n","Batch 58/72 | Loss: 6.4650 | Batch Acc: 0.1250\n","Batch 59/72 | Loss: 7.5383 | Batch Acc: 0.0625\n","Batch 60/72 | Loss: 7.2442 | Batch Acc: 0.0625\n","Batch 61/72 | Loss: 6.4614 | Batch Acc: 0.1250\n","Batch 62/72 | Loss: 7.2718 | Batch Acc: 0.0625\n","Batch 63/72 | Loss: 6.5312 | Batch Acc: 0.0625\n","Batch 64/72 | Loss: 6.4562 | Batch Acc: 0.0625\n","Batch 65/72 | Loss: 7.5033 | Batch Acc: 0.1250\n","Batch 66/72 | Loss: 6.8645 | Batch Acc: 0.1875\n","Batch 67/72 | Loss: 5.8251 | Batch Acc: 0.0625\n","Batch 68/72 | Loss: 6.7886 | Batch Acc: 0.0625\n","Batch 69/72 | Loss: 6.7802 | Batch Acc: 0.0000\n","Batch 70/72 | Loss: 6.8665 | Batch Acc: 0.0625\n","Batch 71/72 | Loss: 7.5631 | Batch Acc: 0.0000\n","Batch 72/72 | Loss: 7.6226 | Batch Acc: 0.0769\n","Epoch Train Loss: 480.0120, Accuracy: 0.0975\n","Validation Accuracy: 0.1250\n","✅ 儲存最佳模型（Acc: 0.1250）\n","\n","Epoch 9/50\n","Batch 1/72 | Loss: 5.7816 | Batch Acc: 0.1875\n","Batch 2/72 | Loss: 6.6058 | Batch Acc: 0.0625\n","Batch 3/72 | Loss: 6.5825 | Batch Acc: 0.0625\n","Batch 4/72 | Loss: 5.8339 | Batch Acc: 0.1250\n","Batch 5/72 | Loss: 7.1929 | Batch Acc: 0.1250\n","Batch 6/72 | Loss: 7.6171 | Batch Acc: 0.0625\n","Batch 7/72 | Loss: 5.8965 | Batch Acc: 0.1250\n","Batch 8/72 | Loss: 6.6360 | Batch Acc: 0.0625\n","Batch 9/72 | Loss: 7.0527 | Batch Acc: 0.0625\n","Batch 10/72 | Loss: 7.3084 | Batch Acc: 0.0000\n","Batch 11/72 | Loss: 6.5737 | Batch Acc: 0.1250\n","Batch 12/72 | Loss: 7.5434 | Batch Acc: 0.0000\n","Batch 13/72 | Loss: 7.4937 | Batch Acc: 0.0625\n","Batch 14/72 | Loss: 6.0252 | Batch Acc: 0.1875\n","Batch 15/72 | Loss: 6.2267 | Batch Acc: 0.1875\n","Batch 16/72 | Loss: 6.7894 | Batch Acc: 0.0625\n","Batch 17/72 | Loss: 7.0083 | Batch Acc: 0.1250\n","Batch 18/72 | Loss: 6.7831 | Batch Acc: 0.1250\n","Batch 19/72 | Loss: 6.6738 | Batch Acc: 0.1250\n","Batch 20/72 | Loss: 7.2459 | Batch Acc: 0.2500\n","Batch 21/72 | Loss: 5.8066 | Batch Acc: 0.1875\n","Batch 22/72 | Loss: 6.2937 | Batch Acc: 0.0625\n","Batch 23/72 | Loss: 6.7619 | Batch Acc: 0.0625\n","Batch 24/72 | Loss: 6.6459 | Batch Acc: 0.1250\n","Batch 25/72 | Loss: 6.5065 | Batch Acc: 0.0625\n","Batch 26/72 | Loss: 6.9192 | Batch Acc: 0.1250\n","Batch 27/72 | Loss: 7.3186 | Batch Acc: 0.0625\n","Batch 28/72 | Loss: 7.5757 | Batch Acc: 0.0625\n","Batch 29/72 | Loss: 7.6721 | Batch Acc: 0.0625\n","Batch 30/72 | Loss: 6.4081 | Batch Acc: 0.0000\n","Batch 31/72 | Loss: 6.7988 | Batch Acc: 0.1250\n","Batch 32/72 | Loss: 6.3885 | Batch Acc: 0.1250\n","Batch 33/72 | Loss: 6.6642 | Batch Acc: 0.1250\n","Batch 34/72 | Loss: 6.7770 | Batch Acc: 0.0625\n","Batch 35/72 | Loss: 6.9257 | Batch Acc: 0.0625\n","Batch 36/72 | Loss: 6.3940 | Batch Acc: 0.1250\n","Batch 37/72 | Loss: 5.9655 | Batch Acc: 0.1875\n","Batch 38/72 | Loss: 6.3489 | Batch Acc: 0.1250\n","Batch 39/72 | Loss: 7.0234 | Batch Acc: 0.0000\n","Batch 40/72 | Loss: 6.2484 | Batch Acc: 0.0625\n","Batch 41/72 | Loss: 6.6842 | Batch Acc: 0.0625\n","Batch 42/72 | Loss: 6.6638 | Batch Acc: 0.0625\n","Batch 43/72 | Loss: 6.7902 | Batch Acc: 0.3125\n","Batch 44/72 | Loss: 6.4203 | Batch Acc: 0.1250\n","Batch 45/72 | Loss: 7.3514 | Batch Acc: 0.0000\n","Batch 46/72 | Loss: 5.5498 | Batch Acc: 0.2500\n","Batch 47/72 | Loss: 6.6169 | Batch Acc: 0.1250\n","Batch 48/72 | Loss: 6.6242 | Batch Acc: 0.1875\n","Batch 49/72 | Loss: 6.8797 | Batch Acc: 0.1875\n","Batch 50/72 | Loss: 6.8816 | Batch Acc: 0.2500\n","Batch 51/72 | Loss: 6.5750 | Batch Acc: 0.1250\n","Batch 52/72 | Loss: 6.8613 | Batch Acc: 0.1250\n","Batch 53/72 | Loss: 5.9775 | Batch Acc: 0.1250\n","Batch 54/72 | Loss: 6.6810 | Batch Acc: 0.1875\n","Batch 55/72 | Loss: 7.1161 | Batch Acc: 0.0625\n","Batch 56/72 | Loss: 6.5857 | Batch Acc: 0.0625\n","Batch 57/72 | Loss: 6.5648 | Batch Acc: 0.1875\n","Batch 58/72 | Loss: 6.0903 | Batch Acc: 0.1250\n","Batch 59/72 | Loss: 6.1377 | Batch Acc: 0.2500\n","Batch 60/72 | Loss: 7.2114 | Batch Acc: 0.0625\n","Batch 61/72 | Loss: 6.6884 | Batch Acc: 0.0625\n","Batch 62/72 | Loss: 5.7417 | Batch Acc: 0.1250\n","Batch 63/72 | Loss: 6.0031 | Batch Acc: 0.0625\n","Batch 64/72 | Loss: 6.6136 | Batch Acc: 0.0000\n","Batch 65/72 | Loss: 6.7212 | Batch Acc: 0.0000\n","Batch 66/72 | Loss: 6.7040 | Batch Acc: 0.1250\n","Batch 67/72 | Loss: 6.7927 | Batch Acc: 0.0625\n","Batch 68/72 | Loss: 5.4763 | Batch Acc: 0.1250\n","Batch 69/72 | Loss: 6.6134 | Batch Acc: 0.0625\n","Batch 70/72 | Loss: 7.0609 | Batch Acc: 0.0625\n","Batch 71/72 | Loss: 6.3594 | Batch Acc: 0.0625\n","Batch 72/72 | Loss: 5.9237 | Batch Acc: 0.0769\n","Epoch Train Loss: 477.2749, Accuracy: 0.1062\n","Validation Accuracy: 0.1080\n","\n","Epoch 10/50\n","Batch 1/72 | Loss: 7.2734 | Batch Acc: 0.0625\n","Batch 2/72 | Loss: 5.9074 | Batch Acc: 0.0625\n","Batch 3/72 | Loss: 7.0019 | Batch Acc: 0.0625\n","Batch 4/72 | Loss: 6.4475 | Batch Acc: 0.1875\n","Batch 5/72 | Loss: 6.8566 | Batch Acc: 0.1250\n","Batch 6/72 | Loss: 6.7291 | Batch Acc: 0.0625\n","Batch 7/72 | Loss: 6.8521 | Batch Acc: 0.1875\n","Batch 8/72 | Loss: 6.7946 | Batch Acc: 0.1875\n","Batch 9/72 | Loss: 6.5408 | Batch Acc: 0.1250\n","Batch 10/72 | Loss: 6.2514 | Batch Acc: 0.1250\n","Batch 11/72 | Loss: 7.9694 | Batch Acc: 0.0625\n","Batch 12/72 | Loss: 6.5620 | Batch Acc: 0.0625\n","Batch 13/72 | Loss: 7.2774 | Batch Acc: 0.0625\n","Batch 14/72 | Loss: 6.8012 | Batch Acc: 0.1250\n","Batch 15/72 | Loss: 6.4891 | Batch Acc: 0.1250\n","Batch 16/72 | Loss: 5.8994 | Batch Acc: 0.1875\n","Batch 17/72 | Loss: 6.3232 | Batch Acc: 0.1875\n","Batch 18/72 | Loss: 6.7925 | Batch Acc: 0.1250\n","Batch 19/72 | Loss: 6.9396 | Batch Acc: 0.0625\n","Batch 20/72 | Loss: 7.0023 | Batch Acc: 0.0625\n","Batch 21/72 | Loss: 6.6446 | Batch Acc: 0.0625\n","Batch 22/72 | Loss: 6.0926 | Batch Acc: 0.2500\n","Batch 23/72 | Loss: 5.9252 | Batch Acc: 0.1250\n","Batch 24/72 | Loss: 6.8208 | Batch Acc: 0.0625\n","Batch 25/72 | Loss: 6.0841 | Batch Acc: 0.2500\n","Batch 26/72 | Loss: 6.7233 | Batch Acc: 0.0625\n","Batch 27/72 | Loss: 6.0594 | Batch Acc: 0.2500\n","Batch 28/72 | Loss: 6.7326 | Batch Acc: 0.0000\n","Batch 29/72 | Loss: 6.0850 | Batch Acc: 0.1875\n","Batch 30/72 | Loss: 6.6969 | Batch Acc: 0.2500\n","Batch 31/72 | Loss: 7.5127 | Batch Acc: 0.0000\n","Batch 32/72 | Loss: 6.2708 | Batch Acc: 0.1875\n","Batch 33/72 | Loss: 6.4309 | Batch Acc: 0.0625\n","Batch 34/72 | Loss: 6.7110 | Batch Acc: 0.0625\n","Batch 35/72 | Loss: 6.9906 | Batch Acc: 0.0000\n","Batch 36/72 | Loss: 6.5295 | Batch Acc: 0.0625\n","Batch 37/72 | Loss: 6.7566 | Batch Acc: 0.0625\n","Batch 38/72 | Loss: 5.1358 | Batch Acc: 0.1250\n","Batch 39/72 | Loss: 6.2523 | Batch Acc: 0.1250\n","Batch 40/72 | Loss: 6.7750 | Batch Acc: 0.0625\n","Batch 41/72 | Loss: 7.0166 | Batch Acc: 0.0625\n","Batch 42/72 | Loss: 6.9387 | Batch Acc: 0.1250\n","Batch 43/72 | Loss: 6.6999 | Batch Acc: 0.2500\n","Batch 44/72 | Loss: 7.1610 | Batch Acc: 0.2500\n","Batch 45/72 | Loss: 6.4527 | Batch Acc: 0.1250\n","Batch 46/72 | Loss: 7.3813 | Batch Acc: 0.0000\n","Batch 47/72 | Loss: 6.4613 | Batch Acc: 0.1250\n","Batch 48/72 | Loss: 6.2228 | Batch Acc: 0.1875\n","Batch 49/72 | Loss: 7.0276 | Batch Acc: 0.0000\n","Batch 50/72 | Loss: 5.9544 | Batch Acc: 0.0625\n","Batch 51/72 | Loss: 6.9694 | Batch Acc: 0.0000\n","Batch 52/72 | Loss: 5.9244 | Batch Acc: 0.1250\n","Batch 53/72 | Loss: 6.8904 | Batch Acc: 0.1875\n","Batch 54/72 | Loss: 6.5768 | Batch Acc: 0.0000\n","Batch 55/72 | Loss: 6.5885 | Batch Acc: 0.0000\n","Batch 56/72 | Loss: 7.8428 | Batch Acc: 0.0000\n","Batch 57/72 | Loss: 7.3368 | Batch Acc: 0.0625\n","Batch 58/72 | Loss: 6.9737 | Batch Acc: 0.1875\n","Batch 59/72 | Loss: 6.1759 | Batch Acc: 0.0625\n","Batch 60/72 | Loss: 6.1475 | Batch Acc: 0.1875\n","Batch 61/72 | Loss: 6.1125 | Batch Acc: 0.0625\n","Batch 62/72 | Loss: 6.3035 | Batch Acc: 0.1875\n","Batch 63/72 | Loss: 6.3231 | Batch Acc: 0.0625\n","Batch 64/72 | Loss: 6.6733 | Batch Acc: 0.1875\n","Batch 65/72 | Loss: 6.7371 | Batch Acc: 0.0625\n","Batch 66/72 | Loss: 6.6345 | Batch Acc: 0.1250\n","Batch 67/72 | Loss: 6.3496 | Batch Acc: 0.1250\n","Batch 68/72 | Loss: 6.3587 | Batch Acc: 0.1250\n","Batch 69/72 | Loss: 7.2276 | Batch Acc: 0.1250\n","Batch 70/72 | Loss: 6.6114 | Batch Acc: 0.0625\n","Batch 71/72 | Loss: 6.5784 | Batch Acc: 0.0625\n","Batch 72/72 | Loss: 6.6207 | Batch Acc: 0.0000\n","Epoch Train Loss: 477.2135, Accuracy: 0.1070\n","Validation Accuracy: 0.0909\n","\n","Epoch 11/50\n","Batch 1/72 | Loss: 6.9707 | Batch Acc: 0.1250\n","Batch 2/72 | Loss: 7.4448 | Batch Acc: 0.0000\n","Batch 3/72 | Loss: 7.0525 | Batch Acc: 0.0625\n","Batch 4/72 | Loss: 6.5602 | Batch Acc: 0.1250\n","Batch 5/72 | Loss: 5.8290 | Batch Acc: 0.2500\n","Batch 6/72 | Loss: 6.3134 | Batch Acc: 0.2500\n","Batch 7/72 | Loss: 7.8016 | Batch Acc: 0.0000\n","Batch 8/72 | Loss: 5.7273 | Batch Acc: 0.2500\n","Batch 9/72 | Loss: 6.4596 | Batch Acc: 0.0625\n","Batch 10/72 | Loss: 6.8007 | Batch Acc: 0.0000\n","Batch 11/72 | Loss: 6.2516 | Batch Acc: 0.1875\n","Batch 12/72 | Loss: 6.5353 | Batch Acc: 0.0625\n","Batch 13/72 | Loss: 6.1906 | Batch Acc: 0.1875\n","Batch 14/72 | Loss: 6.0210 | Batch Acc: 0.0625\n","Batch 15/72 | Loss: 6.4411 | Batch Acc: 0.0625\n","Batch 16/72 | Loss: 7.0576 | Batch Acc: 0.0625\n","Batch 17/72 | Loss: 6.3133 | Batch Acc: 0.0625\n","Batch 18/72 | Loss: 7.0342 | Batch Acc: 0.1250\n","Batch 19/72 | Loss: 6.0550 | Batch Acc: 0.0625\n","Batch 20/72 | Loss: 6.2939 | Batch Acc: 0.0625\n","Batch 21/72 | Loss: 7.5592 | Batch Acc: 0.1250\n","Batch 22/72 | Loss: 7.0442 | Batch Acc: 0.0625\n","Batch 23/72 | Loss: 5.4320 | Batch Acc: 0.1875\n","Batch 24/72 | Loss: 6.4686 | Batch Acc: 0.0625\n","Batch 25/72 | Loss: 7.2091 | Batch Acc: 0.1250\n","Batch 26/72 | Loss: 7.8521 | Batch Acc: 0.0000\n","Batch 27/72 | Loss: 6.1170 | Batch Acc: 0.0625\n","Batch 28/72 | Loss: 5.9086 | Batch Acc: 0.0625\n","Batch 29/72 | Loss: 6.5783 | Batch Acc: 0.0625\n","Batch 30/72 | Loss: 6.5069 | Batch Acc: 0.3125\n","Batch 31/72 | Loss: 6.6805 | Batch Acc: 0.1250\n","Batch 32/72 | Loss: 6.0151 | Batch Acc: 0.1875\n","Batch 33/72 | Loss: 6.8193 | Batch Acc: 0.0625\n","Batch 34/72 | Loss: 7.0120 | Batch Acc: 0.0000\n","Batch 35/72 | Loss: 5.9704 | Batch Acc: 0.1875\n","Batch 36/72 | Loss: 6.0252 | Batch Acc: 0.1250\n","Batch 37/72 | Loss: 5.9825 | Batch Acc: 0.2500\n","Batch 38/72 | Loss: 6.7822 | Batch Acc: 0.1250\n","Batch 39/72 | Loss: 6.7353 | Batch Acc: 0.0625\n","Batch 40/72 | Loss: 6.3731 | Batch Acc: 0.0625\n","Batch 41/72 | Loss: 7.4920 | Batch Acc: 0.1250\n","Batch 42/72 | Loss: 6.5875 | Batch Acc: 0.1250\n","Batch 43/72 | Loss: 6.2210 | Batch Acc: 0.1250\n","Batch 44/72 | Loss: 5.9349 | Batch Acc: 0.0625\n","Batch 45/72 | Loss: 7.2001 | Batch Acc: 0.0625\n","Batch 46/72 | Loss: 7.1577 | Batch Acc: 0.0000\n","Batch 47/72 | Loss: 6.5915 | Batch Acc: 0.1875\n","Batch 48/72 | Loss: 6.2533 | Batch Acc: 0.1250\n","Batch 49/72 | Loss: 5.8928 | Batch Acc: 0.0625\n","Batch 50/72 | Loss: 5.8001 | Batch Acc: 0.2500\n","Batch 51/72 | Loss: 6.8762 | Batch Acc: 0.1250\n","Batch 52/72 | Loss: 6.1197 | Batch Acc: 0.0625\n","Batch 53/72 | Loss: 7.1210 | Batch Acc: 0.0000\n","Batch 54/72 | Loss: 7.4862 | Batch Acc: 0.0000\n","Batch 55/72 | Loss: 7.0851 | Batch Acc: 0.1875\n","Batch 56/72 | Loss: 6.7922 | Batch Acc: 0.1875\n","Batch 57/72 | Loss: 6.4064 | Batch Acc: 0.0000\n","Batch 58/72 | Loss: 6.4834 | Batch Acc: 0.1250\n","Batch 59/72 | Loss: 7.3478 | Batch Acc: 0.0000\n","Batch 60/72 | Loss: 6.3716 | Batch Acc: 0.1875\n","Batch 61/72 | Loss: 6.4086 | Batch Acc: 0.1250\n","Batch 62/72 | Loss: 6.0686 | Batch Acc: 0.0625\n","Batch 63/72 | Loss: 5.8850 | Batch Acc: 0.3125\n","Batch 64/72 | Loss: 6.6069 | Batch Acc: 0.1250\n","Batch 65/72 | Loss: 6.8226 | Batch Acc: 0.0625\n","Batch 66/72 | Loss: 7.3115 | Batch Acc: 0.0000\n","Batch 67/72 | Loss: 7.6162 | Batch Acc: 0.0000\n","Batch 68/72 | Loss: 7.4528 | Batch Acc: 0.2500\n","Batch 69/72 | Loss: 5.9492 | Batch Acc: 0.0625\n","Batch 70/72 | Loss: 6.3429 | Batch Acc: 0.3750\n","Batch 71/72 | Loss: 6.5252 | Batch Acc: 0.1875\n","Batch 72/72 | Loss: 7.3462 | Batch Acc: 0.1538\n","Epoch Train Loss: 475.7809, Accuracy: 0.1114\n","Validation Accuracy: 0.0966\n","\n","Epoch 12/50\n","Batch 1/72 | Loss: 6.9882 | Batch Acc: 0.1250\n","Batch 2/72 | Loss: 5.7963 | Batch Acc: 0.1250\n","Batch 3/72 | Loss: 6.5469 | Batch Acc: 0.0625\n","Batch 4/72 | Loss: 6.4343 | Batch Acc: 0.0000\n","Batch 5/72 | Loss: 6.5083 | Batch Acc: 0.1250\n","Batch 6/72 | Loss: 6.5641 | Batch Acc: 0.0000\n","Batch 7/72 | Loss: 6.3414 | Batch Acc: 0.1250\n","Batch 8/72 | Loss: 6.4691 | Batch Acc: 0.1250\n","Batch 9/72 | Loss: 6.4319 | Batch Acc: 0.0625\n","Batch 10/72 | Loss: 7.4142 | Batch Acc: 0.1875\n","Batch 11/72 | Loss: 6.4697 | Batch Acc: 0.0625\n","Batch 12/72 | Loss: 7.7193 | Batch Acc: 0.1250\n","Batch 13/72 | Loss: 6.3829 | Batch Acc: 0.1875\n","Batch 14/72 | Loss: 6.8633 | Batch Acc: 0.0625\n","Batch 15/72 | Loss: 5.8603 | Batch Acc: 0.2500\n","Batch 16/72 | Loss: 6.1226 | Batch Acc: 0.1250\n","Batch 17/72 | Loss: 7.1025 | Batch Acc: 0.0625\n","Batch 18/72 | Loss: 5.5983 | Batch Acc: 0.1875\n","Batch 19/72 | Loss: 6.7487 | Batch Acc: 0.1250\n","Batch 20/72 | Loss: 6.6339 | Batch Acc: 0.0625\n","Batch 21/72 | Loss: 6.8773 | Batch Acc: 0.2500\n","Batch 22/72 | Loss: 7.2108 | Batch Acc: 0.1875\n","Batch 23/72 | Loss: 6.9044 | Batch Acc: 0.1875\n","Batch 24/72 | Loss: 6.4443 | Batch Acc: 0.1875\n","Batch 25/72 | Loss: 7.1846 | Batch Acc: 0.0625\n","Batch 26/72 | Loss: 7.2403 | Batch Acc: 0.0000\n","Batch 27/72 | Loss: 6.5585 | Batch Acc: 0.1250\n","Batch 28/72 | Loss: 6.6016 | Batch Acc: 0.0625\n","Batch 29/72 | Loss: 6.1957 | Batch Acc: 0.1250\n","Batch 30/72 | Loss: 6.4513 | Batch Acc: 0.0625\n","Batch 31/72 | Loss: 6.8036 | Batch Acc: 0.1250\n","Batch 32/72 | Loss: 6.3405 | Batch Acc: 0.1250\n","Batch 33/72 | Loss: 5.4746 | Batch Acc: 0.1875\n","Batch 34/72 | Loss: 7.5456 | Batch Acc: 0.1250\n","Batch 35/72 | Loss: 6.1608 | Batch Acc: 0.1250\n","Batch 36/72 | Loss: 6.5027 | Batch Acc: 0.0625\n","Batch 37/72 | Loss: 6.4236 | Batch Acc: 0.1875\n","Batch 38/72 | Loss: 6.2365 | Batch Acc: 0.1875\n","Batch 39/72 | Loss: 7.1457 | Batch Acc: 0.0000\n","Batch 40/72 | Loss: 6.6681 | Batch Acc: 0.1250\n","Batch 41/72 | Loss: 5.7824 | Batch Acc: 0.1875\n","Batch 42/72 | Loss: 6.6241 | Batch Acc: 0.1875\n","Batch 43/72 | Loss: 6.0986 | Batch Acc: 0.1250\n","Batch 44/72 | Loss: 6.7032 | Batch Acc: 0.1250\n","Batch 45/72 | Loss: 6.4601 | Batch Acc: 0.0000\n","Batch 46/72 | Loss: 5.9785 | Batch Acc: 0.2500\n","Batch 47/72 | Loss: 6.8400 | Batch Acc: 0.0000\n","Batch 48/72 | Loss: 6.0446 | Batch Acc: 0.1250\n","Batch 49/72 | Loss: 5.6207 | Batch Acc: 0.1875\n","Batch 50/72 | Loss: 8.4178 | Batch Acc: 0.0625\n","Batch 51/72 | Loss: 6.3572 | Batch Acc: 0.0625\n","Batch 52/72 | Loss: 6.1762 | Batch Acc: 0.1250\n","Batch 53/72 | Loss: 5.6628 | Batch Acc: 0.0625\n","Batch 54/72 | Loss: 6.5340 | Batch Acc: 0.0625\n","Batch 55/72 | Loss: 6.4214 | Batch Acc: 0.1875\n","Batch 56/72 | Loss: 6.0345 | Batch Acc: 0.1250\n","Batch 57/72 | Loss: 5.8382 | Batch Acc: 0.3125\n","Batch 58/72 | Loss: 6.2921 | Batch Acc: 0.0625\n","Batch 59/72 | Loss: 6.3942 | Batch Acc: 0.2500\n","Batch 60/72 | Loss: 6.5080 | Batch Acc: 0.1250\n","Batch 61/72 | Loss: 7.2824 | Batch Acc: 0.0000\n","Batch 62/72 | Loss: 6.9569 | Batch Acc: 0.0000\n","Batch 63/72 | Loss: 6.9918 | Batch Acc: 0.0625\n","Batch 64/72 | Loss: 6.6575 | Batch Acc: 0.0000\n","Batch 65/72 | Loss: 6.9822 | Batch Acc: 0.1250\n","Batch 66/72 | Loss: 6.8559 | Batch Acc: 0.0000\n","Batch 67/72 | Loss: 6.5381 | Batch Acc: 0.0625\n","Batch 68/72 | Loss: 5.9951 | Batch Acc: 0.0625\n","Batch 69/72 | Loss: 6.8417 | Batch Acc: 0.0625\n","Batch 70/72 | Loss: 7.4535 | Batch Acc: 0.0000\n","Batch 71/72 | Loss: 7.3046 | Batch Acc: 0.0625\n","Batch 72/72 | Loss: 6.3255 | Batch Acc: 0.0000\n","Epoch Train Loss: 472.9406, Accuracy: 0.1079\n","Validation Accuracy: 0.1023\n","⏹️ 觸發 Early Stopping\n","Validation Accuracy: 0.1136\n","\n","Final Test Accuracy: 0.1136\n"]}],"source":["# Step 7: 訓練流程\n","best_acc = 0.0\n","\n","for epoch in range(num_epochs):\n","    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n","    train_one_epoch_amp(model, train_loader)\n","    val_acc = evaluate(model, val_loader)\n","    # print(\"validation accuracy: \", val_acc)\n","    if val_acc > best_acc:\n","        best_acc = val_acc\n","        patience_counter = 0\n","        torch.save(model.state_dict(), \"best_model.pth\")\n","        print(f\"✅ 儲存最佳模型（Acc: {best_acc:.4f}）\")\n","    else:\n","        patience_counter += 1\n","        if patience_counter >= early_stop_patience:\n","            print(\"⏹️ 觸發 Early Stopping\")\n","            break\n","\n","test_acc = evaluate(model, test_loader)\n","print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RRZynCkG5A_c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749210128633,"user_tz":-480,"elapsed":30666,"user":{"displayName":"Chaha","userId":"17365212891346415618"}},"outputId":"fba3d911-5ef5-478a-f6c0-0760d8f5c0fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.1136\n","\n","Final Test Accuracy: 0.1136\n"]}],"source":["test_acc = evaluate(model, test_loader)\n","print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OcFN_o-nhoXj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749210128638,"user_tz":-480,"elapsed":5,"user":{"displayName":"Chaha","userId":"17365212891346415618"}},"outputId":"ed9ab8aa-e76a-470f-fd64-92b99a22a171"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["PureMLP(\n","  (unfold): Unfold(kernel_size=16, dilation=1, padding=0, stride=16)\n","  (proj): Sequential(\n","    (0): Linear(in_features=768, out_features=256, bias=True)\n","    (1): GELU(approximate='none')\n","    (2): Dropout(p=0.2, inplace=False)\n","  )\n","  (mlp_head): Sequential(\n","    (0): Linear(in_features=256, out_features=512, bias=True)\n","    (1): GELU(approximate='none')\n","    (2): Dropout(p=0.2, inplace=False)\n","    (3): Linear(in_features=512, out_features=256, bias=True)\n","    (4): GELU(approximate='none')\n","    (5): Dropout(p=0.2, inplace=False)\n","    (6): Linear(in_features=256, out_features=22, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":13}],"source":["# Step 8: 載入最佳模型（推論前）\n","model.load_state_dict(torch.load(\"best_model.pth\"))\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CSdGpwm95A_d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749210128801,"user_tz":-480,"elapsed":162,"user":{"displayName":"Chaha","userId":"17365212891346415618"}},"outputId":"e20e3895-cdd7-407e-a18a-3c05c501827b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Node 1: 0.0342\n","Node 2: 0.0413\n","Node 3: 0.0544\n","Node 4: 0.0614\n","Node 5: 0.0537\n","Node 6: 0.0879\n","Node 7: 0.0553\n","Node 8: 0.0328\n","Node 9: 0.0171\n","Node 10: 0.0212\n","Node 11: 0.0150\n","Node 12: 0.0171\n","Node 13: 0.0184\n","Node 14: 0.0511\n","Node 15: 0.0407\n","Node 16: 0.0243\n","Node 17: 0.0181\n","Node 18: 0.0331\n","Node 19: 0.0697\n","Node 20: 0.0582\n","Node 21: 0.0644\n","Node 22: 0.1306\n"]}],"source":["from PIL import Image\n","\n","# Load model\n","model.load_state_dict(torch.load(\"best_model.pth\"))\n","model.eval()\n","model.to(device)\n","\n","# Load and preprocess image\n","img_path = \"/content/drive/MyDrive/DLA_term_project_data/classified_data/test/node_15/IMG_4393.jpg\"\n","img = Image.open(img_path).convert(\"RGB\")\n","input_tensor = test_transform(img).unsqueeze(0).to(device)\n","\n","# Predict\n","with torch.no_grad():\n","    logits = model(input_tensor)\n","    probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n","\n","# Print probabilities\n","for idx, prob in enumerate(probs):\n","    print(f\"Node {idx+1}: {prob:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FMyafEEN5A_d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749210157883,"user_tz":-480,"elapsed":29077,"user":{"displayName":"Chaha","userId":"17365212891346415618"}},"outputId":"fd9d3d9b-4cf8-4f2e-d3dd-4142063a076e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sample 1: True Node=1, Predicted Node=22 ❌\n","Sample 2: True Node=1, Predicted Node=2 ❌\n","Sample 3: True Node=1, Predicted Node=21 ❌\n","Sample 4: True Node=1, Predicted Node=19 ❌\n","Sample 5: True Node=1, Predicted Node=2 ❌\n","Sample 6: True Node=1, Predicted Node=22 ❌\n","Sample 7: True Node=1, Predicted Node=22 ❌\n","Sample 8: True Node=1, Predicted Node=2 ❌\n","Sample 9: True Node=2, Predicted Node=22 ❌\n","Sample 10: True Node=2, Predicted Node=22 ❌\n","Sample 11: True Node=2, Predicted Node=22 ❌\n","Sample 12: True Node=2, Predicted Node=22 ❌\n","Sample 13: True Node=2, Predicted Node=2 ✅\n","Sample 14: True Node=2, Predicted Node=2 ✅\n","Sample 15: True Node=2, Predicted Node=22 ❌\n","Sample 16: True Node=2, Predicted Node=9 ❌\n","Sample 17: True Node=3, Predicted Node=22 ❌\n","Sample 18: True Node=3, Predicted Node=19 ❌\n","Sample 19: True Node=3, Predicted Node=19 ❌\n","Sample 20: True Node=3, Predicted Node=21 ❌\n","Sample 21: True Node=3, Predicted Node=2 ❌\n","Sample 22: True Node=3, Predicted Node=22 ❌\n","Sample 23: True Node=3, Predicted Node=22 ❌\n","Sample 24: True Node=3, Predicted Node=22 ❌\n","Sample 25: True Node=4, Predicted Node=2 ❌\n","Sample 26: True Node=4, Predicted Node=19 ❌\n","Sample 27: True Node=4, Predicted Node=22 ❌\n","Sample 28: True Node=4, Predicted Node=22 ❌\n","Sample 29: True Node=4, Predicted Node=19 ❌\n","Sample 30: True Node=4, Predicted Node=22 ❌\n","Sample 31: True Node=4, Predicted Node=2 ❌\n","Sample 32: True Node=4, Predicted Node=2 ❌\n","Sample 33: True Node=5, Predicted Node=22 ❌\n","Sample 34: True Node=5, Predicted Node=22 ❌\n","Sample 35: True Node=5, Predicted Node=22 ❌\n","Sample 36: True Node=5, Predicted Node=19 ❌\n","Sample 37: True Node=5, Predicted Node=22 ❌\n","Sample 38: True Node=5, Predicted Node=22 ❌\n","Sample 39: True Node=5, Predicted Node=2 ❌\n","Sample 40: True Node=5, Predicted Node=2 ❌\n","Sample 41: True Node=6, Predicted Node=19 ❌\n","Sample 42: True Node=6, Predicted Node=19 ❌\n","Sample 43: True Node=6, Predicted Node=21 ❌\n","Sample 44: True Node=6, Predicted Node=21 ❌\n","Sample 45: True Node=6, Predicted Node=22 ❌\n","Sample 46: True Node=6, Predicted Node=21 ❌\n","Sample 47: True Node=6, Predicted Node=22 ❌\n","Sample 48: True Node=6, Predicted Node=21 ❌\n","Sample 49: True Node=7, Predicted Node=22 ❌\n","Sample 50: True Node=7, Predicted Node=2 ❌\n","Sample 51: True Node=7, Predicted Node=2 ❌\n","Sample 52: True Node=7, Predicted Node=22 ❌\n","Sample 53: True Node=7, Predicted Node=22 ❌\n","Sample 54: True Node=7, Predicted Node=9 ❌\n","Sample 55: True Node=7, Predicted Node=19 ❌\n","Sample 56: True Node=7, Predicted Node=19 ❌\n","Sample 57: True Node=8, Predicted Node=21 ❌\n","Sample 58: True Node=8, Predicted Node=19 ❌\n","Sample 59: True Node=8, Predicted Node=19 ❌\n","Sample 60: True Node=8, Predicted Node=21 ❌\n","Sample 61: True Node=8, Predicted Node=22 ❌\n","Sample 62: True Node=8, Predicted Node=22 ❌\n","Sample 63: True Node=8, Predicted Node=22 ❌\n","Sample 64: True Node=8, Predicted Node=22 ❌\n","Sample 65: True Node=9, Predicted Node=2 ❌\n","Sample 66: True Node=9, Predicted Node=9 ✅\n","Sample 67: True Node=9, Predicted Node=21 ❌\n","Sample 68: True Node=9, Predicted Node=22 ❌\n","Sample 69: True Node=9, Predicted Node=2 ❌\n","Sample 70: True Node=9, Predicted Node=22 ❌\n","Sample 71: True Node=9, Predicted Node=9 ✅\n","Sample 72: True Node=9, Predicted Node=2 ❌\n","Sample 73: True Node=10, Predicted Node=2 ❌\n","Sample 74: True Node=10, Predicted Node=2 ❌\n","Sample 75: True Node=10, Predicted Node=22 ❌\n","Sample 76: True Node=10, Predicted Node=19 ❌\n","Sample 77: True Node=10, Predicted Node=2 ❌\n","Sample 78: True Node=10, Predicted Node=2 ❌\n","Sample 79: True Node=10, Predicted Node=22 ❌\n","Sample 80: True Node=10, Predicted Node=1 ❌\n","Sample 81: True Node=11, Predicted Node=2 ❌\n","Sample 82: True Node=11, Predicted Node=2 ❌\n","Sample 83: True Node=11, Predicted Node=2 ❌\n","Sample 84: True Node=11, Predicted Node=22 ❌\n","Sample 85: True Node=11, Predicted Node=22 ❌\n","Sample 86: True Node=11, Predicted Node=2 ❌\n","Sample 87: True Node=11, Predicted Node=2 ❌\n","Sample 88: True Node=11, Predicted Node=19 ❌\n","Sample 89: True Node=12, Predicted Node=19 ❌\n","Sample 90: True Node=12, Predicted Node=15 ❌\n","Sample 91: True Node=12, Predicted Node=19 ❌\n","Sample 92: True Node=12, Predicted Node=19 ❌\n","Sample 93: True Node=12, Predicted Node=22 ❌\n","Sample 94: True Node=12, Predicted Node=22 ❌\n","Sample 95: True Node=12, Predicted Node=22 ❌\n","Sample 96: True Node=12, Predicted Node=22 ❌\n","Sample 97: True Node=13, Predicted Node=2 ❌\n","Sample 98: True Node=13, Predicted Node=22 ❌\n","Sample 99: True Node=13, Predicted Node=22 ❌\n","Sample 100: True Node=13, Predicted Node=22 ❌\n","Sample 101: True Node=13, Predicted Node=22 ❌\n","Sample 102: True Node=13, Predicted Node=22 ❌\n","Sample 103: True Node=13, Predicted Node=22 ❌\n","Sample 104: True Node=13, Predicted Node=22 ❌\n","Sample 105: True Node=14, Predicted Node=2 ❌\n","Sample 106: True Node=14, Predicted Node=22 ❌\n","Sample 107: True Node=14, Predicted Node=22 ❌\n","Sample 108: True Node=14, Predicted Node=19 ❌\n","Sample 109: True Node=14, Predicted Node=22 ❌\n","Sample 110: True Node=14, Predicted Node=2 ❌\n","Sample 111: True Node=14, Predicted Node=22 ❌\n","Sample 112: True Node=14, Predicted Node=22 ❌\n","Sample 113: True Node=15, Predicted Node=19 ❌\n","Sample 114: True Node=15, Predicted Node=21 ❌\n","Sample 115: True Node=15, Predicted Node=19 ❌\n","Sample 116: True Node=15, Predicted Node=21 ❌\n","Sample 117: True Node=15, Predicted Node=22 ❌\n","Sample 118: True Node=15, Predicted Node=22 ❌\n","Sample 119: True Node=15, Predicted Node=22 ❌\n","Sample 120: True Node=15, Predicted Node=22 ❌\n","Sample 121: True Node=16, Predicted Node=19 ❌\n","Sample 122: True Node=16, Predicted Node=19 ❌\n","Sample 123: True Node=16, Predicted Node=19 ❌\n","Sample 124: True Node=16, Predicted Node=19 ❌\n","Sample 125: True Node=16, Predicted Node=19 ❌\n","Sample 126: True Node=16, Predicted Node=19 ❌\n","Sample 127: True Node=16, Predicted Node=19 ❌\n","Sample 128: True Node=16, Predicted Node=22 ❌\n","Sample 129: True Node=17, Predicted Node=19 ❌\n","Sample 130: True Node=17, Predicted Node=19 ❌\n","Sample 131: True Node=17, Predicted Node=19 ❌\n","Sample 132: True Node=17, Predicted Node=19 ❌\n","Sample 133: True Node=17, Predicted Node=22 ❌\n","Sample 134: True Node=17, Predicted Node=19 ❌\n","Sample 135: True Node=17, Predicted Node=19 ❌\n","Sample 136: True Node=17, Predicted Node=19 ❌\n","Sample 137: True Node=18, Predicted Node=19 ❌\n","Sample 138: True Node=18, Predicted Node=19 ❌\n","Sample 139: True Node=18, Predicted Node=19 ❌\n","Sample 140: True Node=18, Predicted Node=19 ❌\n","Sample 141: True Node=18, Predicted Node=19 ❌\n","Sample 142: True Node=18, Predicted Node=19 ❌\n","Sample 143: True Node=18, Predicted Node=19 ❌\n","Sample 144: True Node=18, Predicted Node=22 ❌\n","Sample 145: True Node=19, Predicted Node=19 ✅\n","Sample 146: True Node=19, Predicted Node=19 ✅\n","Sample 147: True Node=19, Predicted Node=19 ✅\n","Sample 148: True Node=19, Predicted Node=19 ✅\n","Sample 149: True Node=19, Predicted Node=19 ✅\n","Sample 150: True Node=19, Predicted Node=19 ✅\n","Sample 151: True Node=19, Predicted Node=19 ✅\n","Sample 152: True Node=19, Predicted Node=19 ✅\n","Sample 153: True Node=20, Predicted Node=19 ❌\n","Sample 154: True Node=20, Predicted Node=19 ❌\n","Sample 155: True Node=20, Predicted Node=19 ❌\n","Sample 156: True Node=20, Predicted Node=19 ❌\n","Sample 157: True Node=20, Predicted Node=19 ❌\n","Sample 158: True Node=20, Predicted Node=22 ❌\n","Sample 159: True Node=20, Predicted Node=19 ❌\n","Sample 160: True Node=20, Predicted Node=22 ❌\n","Sample 161: True Node=21, Predicted Node=19 ❌\n","Sample 162: True Node=21, Predicted Node=19 ❌\n","Sample 163: True Node=21, Predicted Node=19 ❌\n","Sample 164: True Node=21, Predicted Node=19 ❌\n","Sample 165: True Node=21, Predicted Node=22 ❌\n","Sample 166: True Node=21, Predicted Node=21 ✅\n","Sample 167: True Node=21, Predicted Node=22 ❌\n","Sample 168: True Node=21, Predicted Node=21 ✅\n","Sample 169: True Node=22, Predicted Node=22 ✅\n","Sample 170: True Node=22, Predicted Node=2 ❌\n","Sample 171: True Node=22, Predicted Node=19 ❌\n","Sample 172: True Node=22, Predicted Node=22 ✅\n","Sample 173: True Node=22, Predicted Node=19 ❌\n","Sample 174: True Node=22, Predicted Node=2 ❌\n","Sample 175: True Node=22, Predicted Node=2 ❌\n","Sample 176: True Node=22, Predicted Node=22 ✅\n","\n","Misclassification count per node (True label):\n","Node 1: 8\n","Node 2: 6\n","Node 3: 8\n","Node 4: 8\n","Node 5: 8\n","Node 6: 8\n","Node 7: 8\n","Node 8: 8\n","Node 9: 6\n","Node 10: 8\n","Node 11: 8\n","Node 12: 8\n","Node 13: 8\n","Node 14: 8\n","Node 15: 8\n","Node 16: 8\n","Node 17: 8\n","Node 18: 8\n","Node 19: 0\n","Node 20: 8\n","Node 21: 6\n","Node 22: 5\n","\n","Node most often misclassified: Node 1 (8 times)\n"]}],"source":["import numpy as np\n","from collections import Counter\n","\n","model.eval()\n","all_preds = []\n","all_labels = []\n","\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        probs = torch.softmax(outputs, dim=1)\n","        preds = torch.argmax(probs, dim=1)\n","        all_preds.extend(preds.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","# Print predictions and ground truths\n","for i, (pred, label) in enumerate(zip(all_preds, all_labels)):\n","    correct = \"✅\" if pred == label else \"❌\"\n","    print(f\"Sample {i+1}: True Node={label+1}, Predicted Node={pred+1} {correct}\")\n","\n","# Count misclassifications per node\n","misclass_counts = Counter()\n","for pred, label in zip(all_preds, all_labels):\n","    if pred != label:\n","        misclass_counts[label+1] += 1  # +1 for 1-based node index\n","\n","print(\"\\nMisclassification count per node (True label):\")\n","for node in range(1, len(test_loader.dataset.classes)+1):\n","    print(f\"Node {node}: {misclass_counts[node]}\")\n","\n","# Find the most misclassified node(s)\n","if misclass_counts:\n","    most_misclassified = misclass_counts.most_common(1)[0]\n","    print(f\"\\nNode most often misclassified: Node {most_misclassified[0]} ({most_misclassified[1]} times)\")\n","else:\n","    print(\"\\nNo misclassifications!\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.3"}},"nbformat":4,"nbformat_minor":0}